[
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-drupal_dblog/master/lib/logstash/inputs/drupal_dblog.rb",
    "name": "drupal_dblog",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "By default, the event only contains the current user id as a field.\nIf you whish to add the username as an additional field, set this to true.",
        "base": false,
        "name": "add_usernames",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The amount of log messages that should be fetched with each query.\nBulk fetching is done to prevent querying huge data sets when lots of\nmessages are in the database.",
        "base": false,
        "name": "bulksize",
        "validate": "number",
        "default": "5000"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Specify all drupal databases that you whish to import from.\nThis can be as many as you whish.\nThe format is a hash, with a unique site name as the key, and a databse\nurl as the value.\n\nExample:\n[\n  \"site1\", \"mysql://user1:password@host1.com/databasename\",\n  \"other_site\", \"mysql://user2:password@otherhost.com/databasename\",\n  ...\n]",
        "base": false,
        "name": "databases",
        "validate": "hash"
      },
      {
        "comments": "Time between checks in minutes.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Label this input with a type.\nTypes are used mainly for filter activation.\n\n\nIf you create an input with type \"foobar\", then only filters\nwhich also have type \"foobar\" will act on them.\n\nThe type is also stored as part of the event itself, so you\ncan also use the type to search for in the web interface.",
        "base": false,
        "name": "type",
        "validate": "string",
        "default": "watchdog"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-file/master/lib/logstash/inputs/file.rb",
    "name": "file",
    "type": "input",
    "params": [
      {
        "comments": "The path(s) to the file(s) to use as an input.\nYou can use filename patterns here, such as `/var/log/*.log`.\nIf you use a pattern like `/var/log/**/*.log`, a recursive search\nof `/var/log` will be done for all `*.log` files.\nPaths must be absolute and cannot be relative.\n\nYou may also configure multiple paths. See an example\non the <<array,Logstash configuration page>>.",
        "base": false,
        "name": "path",
        "validate": "array",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The file input closes any files that were last read the specified\ntimespan in seconds ago.\nThis has different implications depending on if a file is being tailed or\nread. If tailing, and there is a large time gap in incoming data the file\ncan be closed (allowing other files to be opened) but will be queued for\nreopening when new data is detected. If reading, the file will be closed\nafter closed_older seconds from when the last bytes were read.\nThe default is 1 hour",
        "base": false,
        "name": "close_older",
        "validate": "number",
        "default": "1 * 60 * 60"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "set the new line delimiter, defaults to \"\\n\"",
        "base": false,
        "name": "delimiter",
        "validate": "string",
        "default": "\\n"
      },
      {
        "comments": "How often (in seconds) we expand the filename patterns in the\n`path` option to discover new files to watch.",
        "base": false,
        "name": "discover_interval",
        "validate": "number",
        "default": "15"
      },
      {
        "comments": "Exclusions (matched against the filename, not full path). Filename\npatterns are valid here, too. For example, if you have\n[source,ruby]\n    path => \"/var/log/*\"\n\nYou might want to exclude gzipped files:\n[source,ruby]\n    exclude => \"*.gz\"",
        "base": false,
        "name": "exclude",
        "validate": "array"
      },
      {
        "comments": "When the file input discovers a file that was last modified\nbefore the specified timespan in seconds, the file is ignored.\nAfter it's discovery, if an ignored file is modified it is no\nlonger ignored and any new data is read. By default, this option is\ndisabled. Note this unit is in seconds.",
        "base": false,
        "name": "ignore_older",
        "validate": "number"
      },
      {
        "comments": "What is the maximum number of file_handles that this input consumes\nat any one time. Use close_older to close some files if you need to\nprocess more files than this number. This should not be set to the\nmaximum the OS can do because file handles are needed for other\nLS plugins and OS processes.\nThe default of 4095 is set in filewatch.",
        "base": false,
        "name": "max_open_files",
        "validate": "number"
      },
      {
        "comments": "Path of the sincedb database file (keeps track of the current\nposition of monitored log files) that will be written to disk.\nThe default will write sincedb files to `<path.data>/plugins/inputs/file`\nNOTE: it must be a file path and not a directory path",
        "base": false,
        "name": "sincedb_path",
        "validate": "string"
      },
      {
        "comments": "How often (in seconds) to write a since database with the current position of\nmonitored log files.",
        "base": false,
        "name": "sincedb_write_interval",
        "validate": "number",
        "default": "15"
      },
      {
        "comments": "Choose where Logstash starts initially reading files: at the beginning or\nat the end. The default behavior treats files like live streams and thus\nstarts at the end. If you have old data you want to import, set this\nto 'beginning'.\n\nThis option only modifies \"first contact\" situations where a file\nis new and not seen before, i.e. files that don't have a current\nposition recorded in a sincedb file read by Logstash. If a file\nhas already been seen before, this option has no effect and the\nposition recorded in the sincedb file will be used.",
        "base": false,
        "name": "start_position",
        "validate": [
          "beginning",
          "end"
        ],
        "default": "end"
      },
      {
        "comments": "How often (in seconds) we stat files to see if they have been modified.\nIncreasing this interval will decrease the number of system calls we make,\nbut increase the time to detect new log lines.",
        "base": false,
        "name": "stat_interval",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-elasticsearch/master/lib/logstash/inputs/elasticsearch.rb",
    "name": "elasticsearch",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "SSL Certificate Authority file in PEM encoded format, must also include any chain certificates as necessary",
        "base": false,
        "name": "ca_file",
        "validate": "path"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "If set, include Elasticsearch document information such as index, type, and\nthe id in the event.\n\nIt might be important to note, with regards to metadata, that if you're\ningesting documents with the intent to re-index them (or just update them)\nthat the `action` option in the elasticsearch output wants to know how to\nhandle those things. It can be dynamically assigned with a field\nadded to the metadata.\n\nExample\n[source, ruby]\n    input {\n      elasticsearch {\n        hosts => \"es.production.mysite.org\"\n        index => \"mydata-2018.09.*\"\n        query => \"*\"\n        size => 500\n        scroll => \"5m\"\n        docinfo => true\n      }\n    }\n    output {\n      elasticsearch {\n        index => \"copy-of-production.%{[@metadata][_index]}\"\n        index_type => \"%{[@metadata][_type]}\"\n        document_id => \"%{[@metadata][_id]}\"\n      }\n    }\n",
        "base": false,
        "name": "docinfo",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "List of document metadata to move to the `docinfo_target` field\nTo learn more about Elasticsearch metadata fields read\nhttp://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_document_metadata.html",
        "base": false,
        "name": "docinfo_fields",
        "validate": "array",
        "default": "[\"_index\",\"_type\",\"_id\"]"
      },
      {
        "comments": "Where to move the Elasticsearch document information by default we use the @metadata field.",
        "base": false,
        "name": "docinfo_target",
        "validate=> string": null,
        "default": "LogStashEventMETADATA"
      },
      {
        "comments": "List of elasticsearch hosts to use for querying.\neach host can be either IP, HOST, IP:port or HOST:port\nport defaults to 9200",
        "base": false,
        "name": "hosts",
        "validate": "array"
      },
      {
        "comments": "The index or alias to search.",
        "base": false,
        "name": "index",
        "validate": "string",
        "default": "logstash-*"
      },
      {
        "comments": "Basic Auth - password",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "The query to be executed. Read the Elasticsearch query DSL documentation\nfor more info\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html",
        "base": false,
        "name": "query",
        "validate": "string",
        "default": "\"{ \"sort\" [ \"_doc\" ] }\""
      },
      {
        "comments": "This parameter controls the keepalive time in seconds of the scrolling\nrequest and initiates the scrolling process. The timeout applies per\nround trip (i.e. between the previous scroll request, to the next).",
        "base": false,
        "name": "scroll",
        "validate": "string",
        "default": "1m"
      },
      {
        "comments": "This allows you to set the maximum number of hits returned per scroll.",
        "base": false,
        "name": "size",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "SSL",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Basic Auth - username",
        "base": false,
        "name": "user",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-ganglia/master/lib/logstash/inputs/ganglia.rb",
    "name": "ganglia",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The address to listen on",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The port to listen on. Remember that ports less than 1024 (privileged\nports) may require root to use.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8649"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-gelf/master/lib/logstash/inputs/gelf.rb",
    "name": "gelf",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The IP address or hostname to listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The port to listen on. Remember that ports less than 1024 (privileged\nports) may require root to use.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "12201"
      },
      {
        "comments": "Whether or not to remap the GELF message fields to Logstash event fields or\nleave them intact.\n\nRemapping converts the following GELF fields to Logstash equivalents:\n\n* `full\\_message` becomes `event.get(\"message\")`.\n* if there is no `full\\_message`, `short\\_message` becomes `event.get(\"message\")`.",
        "base": false,
        "name": "remap",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Whether or not to remove the leading `\\_` in GELF fields or leave them\nin place. (Logstash < 1.2 did not remove them by default.). Note that\nGELF version 1.1 format now requires all non-standard fields to be added\nas an \"additional\" field, beginning with an underscore.\n\ne.g. `\\_foo` becomes `foo`\n",
        "base": false,
        "name": "strip_leading_underscore",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-imap/master/lib/logstash/inputs/imap.rb",
    "name": "imap",
    "type": "input",
    "params": [
      {
        "comments": "",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "password",
        "validate": "password",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "user",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "",
        "base": false,
        "name": "check_interval",
        "validate": "number",
        "default": "300"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "For multipart messages, use the first part that has this\ncontent-type as the event message.",
        "base": false,
        "name": "content_type",
        "validate": "string",
        "default": "text/plain"
      },
      {
        "comments": "",
        "base": false,
        "name": "delete",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "",
        "base": false,
        "name": "expunge",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "",
        "base": false,
        "name": "fetch_count",
        "validate": "number",
        "default": "50"
      },
      {
        "comments": "",
        "base": false,
        "name": "folder",
        "validate": "string",
        "default": "INBOX"
      },
      {
        "comments": "",
        "base": false,
        "name": "lowercase_headers",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "",
        "base": false,
        "name": "port",
        "validate": "number"
      },
      {
        "comments": "",
        "base": false,
        "name": "secure",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "",
        "base": false,
        "name": "strip_attachments",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "",
        "base": false,
        "name": "verify_cert",
        "validate": "boolean",
        "default": "true"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-heroku/master/lib/logstash/inputs/heroku.rb",
    "name": "heroku",
    "type": "input",
    "params": [
      {
        "comments": "The name of your heroku application. This is usually the first part of the\nthe domain name `my-app-name.herokuapp.com`",
        "base": false,
        "name": "app",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-pipe/master/lib/logstash/inputs/pipe.rb",
    "name": "pipe",
    "type": "input",
    "params": [
      {
        "comments": "Command to run and read events from, one line at a time.\n\nExample:\n[source,ruby]\n   command => \"echo hello world\"",
        "base": false,
        "name": "command",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-irc/master/lib/logstash/inputs/irc.rb",
    "name": "irc",
    "type": "input",
    "params": [
      {
        "comments": "Channels to join and read messages from.\n\nThese should be full channel names including the '#' symbol, such as\n\"#logstash\".\n\nFor passworded channels, add a space and the channel password, such as\n\"#logstash password\".\n",
        "base": false,
        "name": "channels",
        "validate": "array",
        "required": true
      },
      {
        "comments": "Host of the IRC Server to connect to.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Catch all IRC channel/user events not just channel messages",
        "base": false,
        "name": "catch_all",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Gather and send user counts for channels - this requires catch_all and will force it",
        "base": false,
        "name": "get_stats",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "IRC Nickname",
        "base": false,
        "name": "nick",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "IRC Server password",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "Port for the IRC Server",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "6667"
      },
      {
        "comments": "IRC Real name",
        "base": false,
        "name": "real",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Set this to true to enable SSL.",
        "base": false,
        "name": "secure",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "How often in minutes to get the user count stats",
        "base": false,
        "name": "stats_interval",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "IRC Username",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": "logstash"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-jmx/master/lib/logstash/inputs/jmx.rb",
    "name": "jmx",
    "type": "input",
    "params": [
      {
        "comments": "Path where json conf files are stored",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Indicate number of thread launched to retrieve metrics",
        "base": false,
        "name": "nb_thread",
        "validate": "number",
        "default": "4"
      },
      {
        "comments": "Indicate interval between two jmx metrics retrieval\n(in s)",
        "base": false,
        "name": "polling_frequency",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-redis/master/lib/logstash/inputs/redis.rb",
    "name": "redis",
    "type": "input",
    "params": [
      {
        "comments": "Specify either list or channel.  If `redis\\_type` is `list`, then we will BLPOP the\nkey.  If `redis\\_type` is `channel`, then we will SUBSCRIBE to the key.\nIf `redis\\_type` is `pattern_channel`, then we will PSUBSCRIBE to the key.",
        "base": false,
        "name": "data_type",
        "validate": [
          "list",
          "channel",
          "pattern_channel"
        ],
        "required": true
      },
      {
        "comments": "The name of a Redis list or channel.",
        "base": false,
        "name": "key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The number of events to return from Redis using EVAL.",
        "base": false,
        "name": "batch_count",
        "validate": "number",
        "default": "125"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The Redis database number.",
        "base": false,
        "name": "db",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The hostname of your Redis server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "127.0.0.1"
      },
      {
        "comments": "Password to authenticate with. There is no authentication by default.",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "The port to connect on.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "6379"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Initial connection timeout in seconds.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-lumberjack/master/lib/logstash/inputs/lumberjack.rb",
    "name": "lumberjack",
    "type": "input",
    "params": [
      {
        "comments": "The port to listen on.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "SSL certificate to use.",
        "base": false,
        "name": "ssl_certificate",
        "validate": "path",
        "required": true
      },
      {
        "comments": "SSL key to use.",
        "base": false,
        "name": "ssl_key",
        "validate": "path",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of seconds before we raise a timeout,\nthis option is useful to control how much time to wait if something is blocking the pipeline.",
        "base": false,
        "name": "congestion_threshold",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The IP address to listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "SSL key passphrase to use.",
        "base": false,
        "name": "ssl_key_passphrase",
        "validate": "password"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-rabbitmq/master/lib/logstash/inputs/rabbitmq.rb",
    "name": "rabbitmq",
    "type": "input",
    "params": [
      {
        "comments": "Amount of time in seconds to wait after a failed subscription request\nbefore retrying. Subscribes can fail if the server goes away and then comes back.",
        "base": false,
        "name": "subscription_retry_interval_seconds",
        "validate": "number",
        "required": true,
        "default": "5"
      },
      {
        "comments": "Enable message acknowledgements. With acknowledgements\nmessages fetched by Logstash but not yet sent into the\nLogstash pipeline will be requeued by the server if Logstash\nshuts down. Acknowledgements will however hurt the message\nthroughput.\n\nThis will only send an ack back every `prefetch_count` messages.\nWorking in batches provides a performance boost here.",
        "base": false,
        "name": "ack",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Extra queue arguments as an array.\nTo make a RabbitMQ queue mirrored, use: `{\"x-ha-policy\" => \"all\"}`",
        "base": false,
        "name": "arguments",
        "validate": "array",
        "default": "{}"
      },
      {
        "comments": "Should the queue be deleted on the broker when the last consumer\ndisconnects? Set this option to `false` if you want the queue to remain\non the broker, queueing up messages until a consumer comes along to\nconsume them.",
        "base": false,
        "name": "auto_delete",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Is this queue durable? (aka; Should it survive a broker restart?)",
        "base": false,
        "name": "durable",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The name of the exchange to bind the queue to. Specify `exchange_type`\nas well to declare the exchange if it does not exist",
        "base": false,
        "name": "exchange",
        "validate": "string"
      },
      {
        "comments": "The type of the exchange to bind to. Specifying this will cause this plugin\nto declare the exchange if it does not exist.",
        "base": false,
        "name": "exchange_type",
        "validate": "string"
      },
      {
        "comments": "Is the queue exclusive? Exclusive queues can only be used by the connection\nthat declared them and will be deleted when it is closed (e.g. due to a Logstash\nrestart).",
        "base": false,
        "name": "exclusive",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The routing key to use when binding a queue to the exchange.\nThis is only relevant for direct or topic exchanges.\n\n* Routing keys are ignored on fanout exchanges.\n* Wildcards are not valid on direct exchanges.",
        "base": false,
        "name": "key",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Enable the storage of message headers and properties in `@metadata`. This may impact performance",
        "base": false,
        "name": "metadata_enabled",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If true the queue will be passively declared, meaning it must\nalready exist on the server. To have Logstash create the queue\nif necessary leave this option as false. If actively declaring\na queue that already exists, the queue options for this plugin\n(durable etc) must match those of the existing queue.",
        "base": false,
        "name": "passive",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Prefetch count. If acknowledgements are enabled with the `ack`\noption, specifies the number of outstanding unacknowledged\nmessages allowed.",
        "base": false,
        "name": "prefetch_count",
        "validate": "number",
        "default": "256"
      },
      {
        "comments": "The name of the queue Logstash will consume events from. If\nleft empty, a transient queue with an randomly chosen name\nwill be created.",
        "base": false,
        "name": "queue",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-log4j/master/lib/logstash/inputs/log4j.rb",
    "name": "log4j",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "When mode is `server`, the address to listen on.\nWhen mode is `client`, the address to connect to.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "Mode to operate in. `server` listens for client connections,\n`client` connects to a server.",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "server"
      },
      {
        "comments": "When mode is `server`, the port to listen on.\nWhen mode is `client`, the port to connect to.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "4560"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-s3/master/lib/logstash/inputs/s3.rb",
    "name": "s3",
    "type": "input",
    "params": [
      {
        "comments": "The name of the S3 bucket.",
        "base": false,
        "name": "bucket",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Append a prefix to the key (full path including file name in s3) after processing.\nIf backing up to another (or the same) bucket, this effectively lets you\nchoose a new 'folder' to place the files in",
        "base": false,
        "name": "backup_add_prefix",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Name of a S3 bucket to backup processed files to.",
        "base": false,
        "name": "backup_to_bucket",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Path of a local directory to backup processed files to.",
        "base": false,
        "name": "backup_to_dir",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Whether to delete processed files from the original bucket.",
        "base": false,
        "name": "delete",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Ruby style regexp of keys to exclude from the bucket",
        "base": false,
        "name": "exclude_pattern",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Interval to wait between to check the file list again after a run is finished.\nValue is in seconds.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "If specified, the prefix of filenames in the bucket must match (not a regexp)",
        "base": false,
        "name": "prefix",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Where to write the since database (keeps track of the date\nthe last handled file was added to S3). The default will write\nsincedb files to some path matching \"$HOME/.sincedb*\"\nShould be a path with filename not just a directory.",
        "base": false,
        "name": "sincedb_path",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Set the directory where logstash will store the tmp files before processing them.\ndefault to the current OS temporary directory in linux /tmp/logstash",
        "base": false,
        "name": "temporary_directory",
        "validate": "string",
        "default": "File.join(Dir.tmpdir, \"logstash\")"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-stdin/master/lib/logstash/inputs/stdin.rb",
    "name": "stdin",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-sqs/master/lib/logstash/inputs/sqs.rb",
    "name": "sqs",
    "type": "input",
    "params": [
      {
        "comments": "Name of the SQS Queue name to pull messages from. Note that this is just the name of the queue, not the URL or ARN.",
        "base": false,
        "name": "queue",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Name of the event field in which to store the SQS message ID",
        "base": false,
        "name": "id_field",
        "validate": "string"
      },
      {
        "comments": "Name of the event field in which to store the SQS message MD5 checksum",
        "base": false,
        "name": "md5_field",
        "validate": "string"
      },
      {
        "comments": "Polling frequency, default is 20 seconds",
        "base": false,
        "name": "polling_frequency",
        "validate": "number",
        "default": "DEFAULT_POLLING_FREQUENCY"
      },
      {
        "comments": "Name of the event field in which to store the SQS message Sent Timestamp",
        "base": false,
        "name": "sent_timestamp_field",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-stomp/master/lib/logstash/inputs/stomp.rb",
    "name": "stomp",
    "type": "input",
    "params": [
      {
        "comments": "The destination to read events from.\n\nExample: `/topic/logstash`",
        "base": false,
        "name": "destination",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The address of the STOMP server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Enable debugging output?",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The password to authenticate with.",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": ""
      },
      {
        "comments": "The port to connet to on your STOMP server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "61613"
      },
      {
        "comments": "Auto reconnect",
        "base": false,
        "name": "reconnect",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "uto reconnect interval in seconds",
        "base": false,
        "name": "reconnect_interval",
        "validate": "number",
        "default": "30"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "The username to authenticate with.",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The vhost to use",
        "base": false,
        "name": "vhost",
        "validate": "string",
        "default": "nil"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-unix/master/lib/logstash/inputs/unix.rb",
    "name": "unix",
    "type": "input",
    "params": [
      {
        "comments": "When mode is `server`, the path to listen on.\nWhen mode is `client`, the path to connect to.",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Amount of time in seconds to wait if the socket file is not present, before retrying.\nOnly positive values are allowed.\n\nThis setting is only used if `mode` is `client`.",
        "base": false,
        "name": "socket_not_present_retry_interval_seconds",
        "validate": "number",
        "required": true,
        "default": "5"
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The 'read' timeout in seconds. If a particular connection is idle for\nmore than this timeout period, we will assume it is dead and close it.\n\nIf you never want to timeout, use -1.",
        "base": false,
        "name": "data_timeout",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "Remove socket file in case of EADDRINUSE failure",
        "base": false,
        "name": "force_unlink",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Mode to operate in. `server` listens for client connections,\n`client` connects to a server.",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "server"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-relp/master/lib/logstash/inputs/relp.rb",
    "name": "relp",
    "type": "input",
    "params": [
      {
        "comments": "The port to listen on.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The address to listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The SSL CA certificate, chainfile or CA path. The system CA path is automatically included.",
        "base": false,
        "name": "ssl_cacert",
        "validate": "path"
      },
      {
        "comments": "SSL certificate path",
        "base": false,
        "name": "ssl_cert",
        "validate": "path"
      },
      {
        "comments": "Enable SSL (must be set for other `ssl_` options to take effect).",
        "base": false,
        "name": "ssl_enable",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "SSL key path",
        "base": false,
        "name": "ssl_key",
        "validate": "path"
      },
      {
        "comments": "SSL key passphrase",
        "base": false,
        "name": "ssl_key_passphrase",
        "validate": "password",
        "default": "nil"
      },
      {
        "comments": "Verify the identity of the other end of the SSL connection against the CA.\nFor input, sets the field `sslsubject` to that of the client certificate.",
        "base": false,
        "name": "ssl_verify",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-sqlite/master/lib/logstash/inputs/sqlite.rb",
    "name": "sqlite",
    "type": "input",
    "params": [
      {
        "comments": "The path to the sqlite database file.",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "How many rows to fetch at a time from each `SELECT` call.",
        "base": false,
        "name": "batch",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Any tables to exclude by name.\nBy default all tables are followed.",
        "base": false,
        "name": "exclude_tables",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-syslog/master/lib/logstash/inputs/syslog.rb",
    "name": "syslog",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Labels for facility levels. These are defined in RFC3164.",
        "base": false,
        "name": "facility_labels",
        "validate": "array",
        "default": "[\"kernel\",\"user-level\",\"mail\",\"system\",\"security/authorization\",\"syslogd\",\"line printer\",\"network news\",\"UUCP\",\"clock\",\"security/authorization\",\"FTP\",\"NTP\",\"log audit\",\"log alert\",\"clock\",\"local0\",\"local1\",\"local2\",\"local3\",\"local4\",\"local5\",\"local6\",\"local7\"]"
      },
      {
        "comments": "The address to listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "Specify a locale to be used for date parsing using either IETF-BCP47 or POSIX language tag.\nSimple examples are `en`,`en-US` for BCP47 or `en_US` for POSIX.\nIf not specified, the platform default will be used.\n\nThe locale is mostly necessary to be set for parsing month names (pattern with MMM) and\nweekday names (pattern with EEE).\n",
        "base": false,
        "name": "locale",
        "validate": "string"
      },
      {
        "comments": "The port to listen on. Remember that ports less than 1024 (privileged\nports) may require root to use.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "514"
      },
      {
        "comments": "Proxy protocol support, only v1 is supported at this time\nhttp://www.haproxy.org/download/1.5/doc/proxy-protocol.txt",
        "base": false,
        "name": "proxy_protocol",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Labels for severity levels. These are defined in RFC3164.",
        "base": false,
        "name": "severity_labels",
        "validate": "array",
        "default": "[\"Emergency\",\"Alert\",\"Critical\",\"Error\",\"Warning\",\"Notice\",\"Informational\",\"Debug\"]"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Specify a time zone canonical ID to be used for date parsing.\nThe valid IDs are listed on the [Joda.org available time zones page](http://joda-time.sourceforge.net/timezones.html).\nThis is useful in case the time zone cannot be extracted from the value,\nand is not the platform default.\nIf this is not specified the platform default will be used.\nCanonical ID is good as it takes care of daylight saving time for you\nFor example, `America/Los_Angeles` or `Europe/France` are valid IDs.",
        "base": false,
        "name": "timezone",
        "validate": "string"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Use label parsing for severity and facility levels.",
        "base": false,
        "name": "use_labels",
        "validate": "boolean",
        "default": "true"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-zenoss/master/lib/logstash/inputs/zenoss.rb",
    "name": "zenoss",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The name of the exchange to bind the queue. This is analogous to the 'rabbitmq\noutput' [config 'name'](../outputs/rabbitmq)",
        "base": false,
        "name": "exchange",
        "validate": "string",
        "default": "zenoss.zenevents"
      },
      {
        "comments": "Your rabbitmq server address",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The routing key to use. This is only valid for direct or fanout exchanges\n\n* Routing keys are ignored on topic exchanges.\n* Wildcards are not valid on direct exchanges.",
        "base": false,
        "name": "key",
        "validate": "string",
        "default": "zenoss.zenevent.#"
      },
      {
        "comments": "Your rabbitmq password",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": "zenoss"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Your rabbitmq username",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": "zenoss"
      },
      {
        "comments": "The vhost to use. If you don't know what this is, leave the default.",
        "base": false,
        "name": "vhost",
        "validate": "string",
        "default": "/zenoss"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-varnishlog/master/lib/logstash/inputs/varnishlog.rb",
    "name": "varnishlog",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-twitter/master/lib/logstash/inputs/twitter.rb",
    "name": "twitter",
    "type": "input",
    "params": [
      {
        "comments": "Your Twitter App's consumer key\n\nDon't know what this is? You need to create an \"application\"\non Twitter, see this url: <https://dev.twitter.com/apps/new>",
        "base": false,
        "name": "consumer_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your Twitter App's consumer secret\n\nIf you don't have one of these, you can create one by\nregistering a new application with Twitter:\n<https://dev.twitter.com/apps/new>",
        "base": false,
        "name": "consumer_secret",
        "validate": "password",
        "required": true
      },
      {
        "comments": "Your oauth token.\n\nTo get this, login to Twitter with whatever account you want,\nthen visit <https://dev.twitter.com/apps>\n\nClick on your app (used with the consumer_key and consumer_secret settings)\nThen at the bottom of the page, click 'Create my access token' which\nwill create an oauth token and secret bound to your account and that\napplication.",
        "base": false,
        "name": "oauth_token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your oauth token secret.\n\nTo get this, login to Twitter with whatever account you want,\nthen visit <https://dev.twitter.com/apps>\n\nClick on your app (used with the consumer_key and consumer_secret settings)\nThen at the bottom of the page, click 'Create my access token' which\nwill create an oauth token and secret bound to your account and that\napplication.",
        "base": false,
        "name": "oauth_token_secret",
        "validate": "password",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "A comma separated list of user IDs, indicating the users to\nreturn statuses for in the Twitter stream.\nSee https://dev.twitter.com/streaming/overview/request-parameters#follow\nfor more details.",
        "base": false,
        "name": "follows",
        "validate": "array"
      },
      {
        "comments": "Record full tweet object as given to us by the Twitter Streaming API.",
        "base": false,
        "name": "full_tweet",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Lets you ingore the retweets coming out of the Twitter API. Default => false",
        "base": false,
        "name": "ignore_retweets",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Any keywords to track in the Twitter stream. For multiple keywords, use\nthe syntax [\"foo\", \"bar\"]. There's a logical OR between each keyword\nstring listed and a logical AND between words separated by spaces per\nkeyword string.\nSee https://dev.twitter.com/streaming/overview/request-parameters#track\nfor more details.\n\nThe wildcard \"*\" option is not supported. To ingest a sample stream of\nall tweets, the use_samples option is recommended.",
        "base": false,
        "name": "keywords",
        "validate": "array"
      },
      {
        "comments": "A list of BCP 47 language identifiers corresponding to any of the languages listed\non Twitters advanced search page will only return tweets that have been detected\nas being written in the specified languages.",
        "base": false,
        "name": "languages",
        "validate": "array"
      },
      {
        "comments": "A comma-separated list of longitude, latitude pairs specifying a set\nof bounding boxes to filter tweets by.\nSee https://dev.twitter.com/streaming/overview/request-parameters#locations\nfor more details.",
        "base": false,
        "name": "locations",
        "validate": "string"
      },
      {
        "comments": "Location of the proxy, by default the same machine as the one running this LS instance",
        "base": false,
        "name": "proxy_address",
        "validate": "string",
        "default": "127.0.0.1"
      },
      {
        "comments": "Port where the proxy is listening, by default 3128 (squid)",
        "base": false,
        "name": "proxy_port",
        "validate": "number",
        "default": "3128"
      },
      {
        "comments": "Duration in seconds to wait before retrying a connection when twitter responds with a 429 TooManyRequests\nIn some cases the 'x-rate-limit-reset' header is not set in the response and <error>.rate_limit.reset_in\nis nil. If this occurs then we use the integer specified here. The default is 5 minutes.",
        "base": false,
        "name": "rate_limit_reset_in",
        "validate": "number",
        "default": "300"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "When to use a proxy to handle the connections",
        "base": false,
        "name": "use_proxy",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Returns a small random sample of all public statuses. The tweets returned\nby the default access level are the same, so if two different clients connect\nto this endpoint, they will see the same tweets. If set to true, the keywords,\nfollows, locations, and languages options will be ignored. Default => false",
        "base": false,
        "name": "use_samples",
        "validate": "boolean",
        "default": "false"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-wmi/master/lib/logstash/inputs/wmi.rb",
    "name": "wmi",
    "type": "input",
    "params": [
      {
        "comments": "WMI query",
        "base": false,
        "name": "query",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Host to connect to ( Defaults to localhost )",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "Polling interval",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "Namespace when doing remote connections",
        "base": false,
        "name": "namespace",
        "validate": "string",
        "default": "\"root\\cimv2\""
      },
      {
        "comments": "Password when doing remote connections",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Username when doing remote connections",
        "base": false,
        "name": "user",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-email/master/lib/logstash/outputs/email.rb",
    "name": "email",
    "type": "output",
    "params": [
      {
        "comments": "The fully-qualified email address to send the email to.\n\nThis field also accepts a comma-separated string of addresses, for example:\n`\"me@host.com, you@host.com\"`\n\nYou can also use dynamic fields from the event with the `%{fieldname}` syntax.",
        "base": false,
        "name": "to",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The address used to connect to the mail server",
        "base": false,
        "name": "address",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "Attachments - specify the name(s) and location(s) of the files.",
        "base": false,
        "name": "attachments",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Authentication method used when identifying with the server",
        "base": false,
        "name": "authentication",
        "validate": "string"
      },
      {
        "comments": "Body for the email - plain text only.",
        "base": false,
        "name": "body",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The fully-qualified email address(es) to include as cc: address(es).\n\nThis field also accepts a comma-separated string of addresses, for example:\n`\"me@host.com, you@host.com\"`",
        "base": false,
        "name": "cc",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "contenttype : for multipart messages, set the content-type and/or charset of the HTML part.\nNOTE: this may not be functional (KH)",
        "base": false,
        "name": "contenttype",
        "validate": "string",
        "default": "\"text/html",
        "charset=UTF-8\"": null
      },
      {
        "comments": "Run the mail relay in debug mode",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Domain used to send the email messages",
        "base": false,
        "name": "domain",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The fully-qualified email address for the From: field in the email.",
        "base": false,
        "name": "from",
        "validate": "string",
        "default": "logstash.alert@nowhere.com"
      },
      {
        "comments": "HTML Body for the email, which may contain HTML markup.",
        "base": false,
        "name": "htmlbody",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Password to authenticate with the server",
        "base": false,
        "name": "password",
        "validate": "string"
      },
      {
        "comments": "Port used to communicate with the mail server",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "25"
      },
      {
        "comments": "The fully qualified email address for the Reply-To: field.",
        "base": false,
        "name": "replyto",
        "validate": "string"
      },
      {
        "comments": "Subject: for the email.",
        "base": false,
        "name": "subject",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Enables TLS when communicating with the server",
        "base": false,
        "name": "use_tls",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Username to authenticate with the server",
        "base": false,
        "name": "username",
        "validate": "string"
      },
      {
        "comments": "How Logstash should send the email, either via SMTP or by invoking sendmail.",
        "base": false,
        "name": "via",
        "validate": "string",
        "default": "smtp"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-exec/master/lib/logstash/outputs/exec.rb",
    "name": "exec",
    "type": "output",
    "params": [
      {
        "comments": "Command line to execute via subprocess. Use `dtach` or `screen` to\nmake it non blocking. This value can include `%{name}` and other\ndynamic strings.",
        "base": false,
        "name": "command",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "display the result of the command to the terminal",
        "base": false,
        "name": "quiet",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-websocket/master/lib/logstash/inputs/websocket.rb",
    "name": "websocket",
    "type": "input",
    "params": [
      {
        "comments": "The URL to connect to.",
        "base": false,
        "name": "url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Select the plugin's mode of operation. Right now only client mode\nis supported, i.e. this plugin connects to a websocket server and\nreceives events from the server as websocket messages.",
        "base": false,
        "name": "mode",
        "validate": [
          "client"
        ],
        "default": "client"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-graphite/master/lib/logstash/outputs/graphite.rb",
    "name": "graphite",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Exclude regex matched metric names, by default exclude unresolved %{field} strings.",
        "base": false,
        "name": "exclude_metrics",
        "validate": "array",
        "default": "[ \"%\\{[^}]+\\}\" ]"
      },
      {
        "comments": "An array indicating that these event fields should be treated as metrics\nand will be sent verbatim to Graphite. You may use either `fields_are_metrics`\nor `metrics`, but not both.",
        "base": false,
        "name": "fields_are_metrics",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The hostname or IP address of the Graphite server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "Include only regex matched metric names.",
        "base": false,
        "name": "include_metrics",
        "validate": "array",
        "default": "[\".*\"]"
      },
      {
        "comments": "The metric(s) to use. This supports dynamic strings like %{host}\nfor metric names and also for values. This is a hash field with key\nbeing the metric name, value being the metric value. Example:\n[source,ruby]\n    metrics => { \"%{host}/uptime\" => \"%{uptime_1m}\" }\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will be set to zero (0). You may use either `metrics` or `fields_are_metrics`,\nbut not both.",
        "base": false,
        "name": "metrics",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Defines the format of the metric string. The placeholder '*' will be\nreplaced with the name of the actual metric.\n[source,ruby]\n    metrics_format => \"foo.bar.*.sum\"\n\nNOTE: If no metrics_format is defined, the name of the metric will be used as fallback.",
        "base": false,
        "name": "metrics_format",
        "validate": "string",
        "default": "DEFAULT_METRICS_FORMAT"
      },
      {
        "comments": "When hashes are passed in as values they are broken out into a dotted notation\nFor instance if you configure this plugin with\n# [source,ruby]\n    metrics => \"mymetrics\"\n\nand \"mymetrics\" is a nested hash of '{a => 1, b => { c => 2 }}'\nthis plugin will generate two metrics: a => 1, and b.c => 2 .\nIf you've specified a 'metrics_format' it will respect that,\nbut you still may want control over the separator within these nested key names.\nThis config setting changes the separator from the '.' default.",
        "base": false,
        "name": "nested_object_separator",
        "validate": "string",
        "default": "."
      },
      {
        "comments": "The port to connect to on the Graphite server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "2003"
      },
      {
        "comments": "Interval between reconnect attempts to Carbon.",
        "base": false,
        "name": "reconnect_interval",
        "validate": "number",
        "default": "2"
      },
      {
        "comments": "Should metrics be resent on failure?",
        "base": false,
        "name": "resend_on_failure",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Use this field for the timestamp instead of '@timestamp' which is the\ndefault. Useful when backfilling or just getting more accurate data into\ngraphite since you probably have a cache layer infront of Logstash.",
        "base": false,
        "name": "timestamp_field",
        "validate": "string",
        "default": "@timestamp"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-cloudwatch/master/lib/logstash/outputs/cloudwatch.rb",
    "name": "cloudwatch",
    "type": "output",
    "params": [
      {
        "comments": "How many data points can be given in one call to the CloudWatch API",
        "base": false,
        "name": "batch_size",
        "validate": "number",
        "default": "20"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The default dimensions [ name, value, ... ] to use for events which do not have a `CW_dimensions` field",
        "base": false,
        "name": "dimensions",
        "validate": "hash"
      },
      {
        "comments": "The name of the field used to set the dimensions on an event metric\nThe field named here, if present in an event, must have an array of\none or more key & value pairs, for example...\n    `add_field => [ \"CW_dimensions\", \"Environment\", \"CW_dimensions\", \"prod\" ]`\nor, equivalently...\n    `add_field => [ \"CW_dimensions\", \"Environment\" ]`\n    `add_field => [ \"CW_dimensions\", \"prod\" ]`",
        "base": false,
        "name": "field_dimensions",
        "validate": "string",
        "default": "CW_dimensions"
      },
      {
        "comments": "The name of the field used to set the metric name on an event\nThe author of this plugin recommends adding this field to events in inputs &\nfilters rather than using the per-output default setting so that one output\nplugin on your logstash indexer can serve all events (which of course had\nfields set on your logstash shippers.)",
        "base": false,
        "name": "field_metricname",
        "validate": "string",
        "default": "CW_metricname"
      },
      {
        "comments": "The name of the field used to set a different namespace per event\nNote: Only one namespace can be sent to CloudWatch per API call\nso setting different namespaces will increase the number of API calls\nand those cost money.",
        "base": false,
        "name": "field_namespace",
        "validate": "string",
        "default": "CW_namespace"
      },
      {
        "comments": "The name of the field used to set the unit on an event metric",
        "base": false,
        "name": "field_unit",
        "validate": "string",
        "default": "CW_unit"
      },
      {
        "comments": "The name of the field used to set the value (float) on an event metric",
        "base": false,
        "name": "field_value",
        "validate": "string",
        "default": "CW_value"
      },
      {
        "comments": "The default metric name to use for events which do not have a `CW_metricname` field.\nBeware: If this is provided then all events which pass through this output will be aggregated and\nsent to CloudWatch, so use this carefully.  Furthermore, when providing this option, you\nwill probably want to also restrict events from passing through this output using event\ntype, tag, and field matching",
        "base": false,
        "name": "metricname",
        "validate": "string"
      },
      {
        "comments": "The default namespace to use for events which do not have a `CW_namespace` field",
        "base": false,
        "name": "namespace",
        "validate": "string",
        "default": "Logstash"
      },
      {
        "comments": "How many events to queue before forcing a call to the CloudWatch API ahead of `timeframe` schedule\nSet this to the number of events-per-timeframe you will be sending to CloudWatch to avoid extra API calls",
        "base": false,
        "name": "queue_size",
        "validate": "number",
        "default": "10000"
      },
      {
        "comments": "How often to send data to CloudWatch\nThis does not affect the event timestamps, events will always have their\nactual timestamp (to-the-minute) sent to CloudWatch.\n\nWe only call the API if there is data to send.\n\nSee the Rufus Scheduler docs for an https://github.com/jmettraux/rufus-scheduler#the-time-strings-understood-by-rufus-scheduler[explanation of allowed values]",
        "base": false,
        "name": "timeframe",
        "validate": "string",
        "default": "1m"
      },
      {
        "comments": "The default unit to use for events which do not have a `CW_unit` field\nIf you set this option you should probably set the \"value\" option along with it",
        "base": false,
        "name": "unit",
        "validate": "VALID_UNITS",
        "default": "COUNT_UNIT"
      },
      {
        "comments": "The default value to use for events which do not have a `CW_value` field\nIf provided, this must be a string which can be converted to a float, for example...\n    \"1\", \"2.34\", \".5\", and \"0.67\"\nIf you set this option you should probably set the `unit` option along with it",
        "base": false,
        "name": "value",
        "validate": "string",
        "default": "1"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-tcp/master/lib/logstash/inputs/tcp.rb",
    "name": "tcp",
    "type": "input",
    "params": [
      {
        "comments": "When mode is `server`, the port to listen on.\nWhen mode is `client`, the port to connect to.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "",
        "base": false,
        "name": "data_timeout",
        "validate": "number",
        "default": -1,
        "deprecated": "This setting is not used by this plugin. It will be removed soon."
      },
      {
        "comments": "When mode is `server`, the address to listen on.\nWhen mode is `client`, the address to connect to.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "Mode to operate in. `server` listens for client connections,\n`client` connects to a server.",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "server"
      },
      {
        "comments": "Proxy protocol support, only v1 is supported at this time\nhttp://www.haproxy.org/download/1.5/doc/proxy-protocol.txt",
        "base": false,
        "name": "proxy_protocol",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The SSL CA certificate, chainfile or CA path. The system CA path is automatically included.",
        "base": false,
        "name": "ssl_cacert",
        "validate": "path",
        "deprecated": "This setting is deprecated in favor of ssl_extra_chain_certs as it sets a more clear expectation to add more X509 certificates to the store"
      },
      {
        "comments": "SSL certificate path",
        "base": false,
        "name": "ssl_cert",
        "validate": "path"
      },
      {
        "comments": "Enable SSL (must be set for other `ssl_` options to take effect).",
        "base": false,
        "name": "ssl_enable",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "An Array of extra X509 certificates to be added to the certificate chain.\nUseful when the CA chain is not necessary in the system store.",
        "base": false,
        "name": "ssl_extra_chain_certs",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "SSL key path",
        "base": false,
        "name": "ssl_key",
        "validate": "path"
      },
      {
        "comments": "SSL key passphrase",
        "base": false,
        "name": "ssl_key_passphrase",
        "validate": "password",
        "default": "nil"
      },
      {
        "comments": "Verify the identity of the other end of the SSL connection against the CA.\nFor input, sets the field `sslsubject` to that of the client certificate.",
        "base": false,
        "name": "ssl_verify",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-http/master/lib/logstash/outputs/http.rb",
    "name": "http",
    "type": "output",
    "params": [
      {
        "comments": "The HTTP Verb. One of \"put\", \"post\", \"patch\", \"delete\", \"get\", \"head\"",
        "base": false,
        "name": "http_method",
        "validate": "VALID_METHODS",
        "required": true
      },
      {
        "comments": "URL to use",
        "base": false,
        "name": "url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Content type\n\nIf not specified, this defaults to the following:\n\n* if format is \"json\", \"application/json\"\n* if format is \"form\", \"application/x-www-form-urlencoded\"",
        "base": false,
        "name": "content_type",
        "validate": "string"
      },
      {
        "comments": "Set the format of the http body.\n\nIf form, then the body will be the mapping (or whole event) converted\ninto a query parameter string, e.g. `foo=bar&baz=fizz...`\n\nIf message, then the body will be the result of formatting the event according to message\n\nOtherwise, the event is sent as json.",
        "base": false,
        "name": "format",
        "validate": [
          "json",
          "form",
          "message"
        ],
        "default": "json"
      },
      {
        "comments": "Custom headers to use\nformat is `headers => [\"X-My-Header\", \"%{host}\"]`",
        "base": false,
        "name": "headers",
        "validate": "hash"
      },
      {
        "comments": "If you would like to consider some non-2xx codes to be successes\nenumerate them here. Responses returning these codes will be considered successes",
        "base": false,
        "name": "ignorable_codes",
        "validate": "number",
        "list": true
      },
      {
        "comments": "This lets you choose the structure and parts of the event that are sent.\n\n\nFor example:\n[source,ruby]\n   mapping => {\"foo\" => \"%{host}\"\n              \"bar\" => \"%{type}\"}",
        "base": false,
        "name": "mapping",
        "validate": "hash"
      },
      {
        "comments": "",
        "base": false,
        "name": "message",
        "validate": "string"
      },
      {
        "comments": "Set this to false if you don't want this output to retry failed requests",
        "base": false,
        "name": "retry_failed",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "If encountered as response codes this plugin will retry these requests",
        "base": false,
        "name": "retryable_codes",
        "validate": "number",
        "list": true,
        "default": "[429,500,502,503,504]"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-file/master/lib/logstash/outputs/file.rb",
    "name": "file",
    "type": "output",
    "params": [
      {
        "comments": "The path to the file to write. Event fields can be used here,\nlike `/var/log/logstash/%{host}/%{application}`\nOne may also utilize the path option for date-based log\nrotation via the joda time format. This will use the event\ntimestamp.\nE.g.: `path => \"./test-%{+YYYY-MM-dd}.txt\"` to create\n`./test-2013-05-29.txt`\n\nIf you use an absolute path you cannot start with a dynamic string.\nE.g: `/%{myfield}/`, `/test-%{myfield}/` are not valid paths",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "If the configured file is deleted, but an event is handled by the plugin,\nthe plugin will recreate the file. Default => true",
        "base": false,
        "name": "create_if_deleted",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Dir access mode to use. Note that due to the bug in jruby system umask\nis ignored on linux: https://github.com/jruby/jruby/issues/3426\nSetting it to -1 uses default OS value.\nExample: `\"dir_mode\" => 0750`",
        "base": false,
        "name": "dir_mode",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "File access mode to use. Note that due to the bug in jruby system umask\nis ignored on linux: https://github.com/jruby/jruby/issues/3426\nSetting it to -1 uses default OS value.\nExample: `\"file_mode\" => 0640`",
        "base": false,
        "name": "file_mode",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "If the generated path is invalid, the events will be saved\ninto this file and inside the defined path.",
        "base": false,
        "name": "filename_failure",
        "validate": "string",
        "default": "_filepath_failures"
      },
      {
        "comments": "Flush interval (in seconds) for flushing writes to log files.\n0 will flush on every message.",
        "base": false,
        "name": "flush_interval",
        "validate": "number",
        "default": "2"
      },
      {
        "comments": "Gzip the output stream before writing to disk.",
        "base": false,
        "name": "gzip",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-lumberjack/master/lib/logstash/outputs/lumberjack.rb",
    "name": "lumberjack",
    "type": "output",
    "params": [
      {
        "comments": "list of addresses lumberjack can send to",
        "base": false,
        "name": "hosts",
        "validate": "array",
        "required": true
      },
      {
        "comments": "the port to connect to",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "ssl certificate to use",
        "base": false,
        "name": "ssl_certificate",
        "validate": "path",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "To make efficient calls to the lumberjack output we are buffering events locally.\nif the number of events exceed the number the declared `flush_size` we will\nsend them to the logstash server.",
        "base": false,
        "name": "flush_size",
        "validate": "number",
        "default": "1024"
      },
      {
        "comments": "The amount of time since last flush before a flush is forced.\n\nThis setting helps ensure slow event rates don't get stuck in Logstash.\nFor example, if your `flush_size` is 100, and you have received 10 events,\nand it has been more than `idle_flush_time` seconds since the last flush,\nLogstash will flush those 10 events automatically.\n\nThis helps keep both fast and slow log streams moving along in\nnear-real-time.",
        "base": false,
        "name": "idle_flush_time",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-juggernaut/master/lib/logstash/outputs/juggernaut.rb",
    "name": "juggernaut",
    "type": "output",
    "params": [
      {
        "comments": "List of channels to which to publish. Dynamic names are\nvalid here, for example `logstash-%{type}`.",
        "base": false,
        "name": "channels",
        "validate": "array",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The redis database number.",
        "base": false,
        "name": "db",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The hostname of the redis server to which juggernaut is listening.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "127.0.0.1"
      },
      {
        "comments": "How should the message be formatted before pushing to the websocket.",
        "base": false,
        "name": "message_format",
        "validate": "string"
      },
      {
        "comments": "Password to authenticate with.  There is no authentication by default.",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "The port to connect on.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "6379"
      },
      {
        "comments": "Redis initial connection timeout in seconds.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-ganglia/master/lib/logstash/outputs/ganglia.rb",
    "name": "ganglia",
    "type": "output",
    "params": [
      {
        "comments": "The metric to use. This supports dynamic strings like `%{host}`",
        "base": false,
        "name": "metric",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The value to use. This supports dynamic strings like `%{bytes}`\nIt will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "value",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Metric group",
        "base": false,
        "name": "group",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The address of the ganglia server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "Lifetime in seconds of this metric",
        "base": false,
        "name": "lifetime",
        "validate": "number",
        "default": "300"
      },
      {
        "comments": "Maximum time in seconds between gmetric calls for this metric.",
        "base": false,
        "name": "max_interval",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The type of value for this metric.",
        "base": false,
        "name": "metric_type",
        "validate": "%w{string int8 uint8 int16 uint16 int32 uint32 float double},"
      },
      {
        "comments": "The port to connect on your ganglia server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8649"
      },
      {
        "comments": "Metric slope, represents metric behavior",
        "base": false,
        "name": "slope",
        "validate": "%w{zero positive negative both unspecified}",
        "default": "both"
      },
      {
        "comments": "Gmetric units for metric, such as \"kb/sec\" or \"ms\" or whatever unit\nthis metric uses.",
        "base": false,
        "name": "units",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-opentsdb/master/lib/logstash/outputs/opentsdb.rb",
    "name": "opentsdb",
    "type": "output",
    "params": [
      {
        "comments": "The metric(s) to use. This supports dynamic strings like %{source_host}\nfor metric names and also for values. This is an array field with key\nof the metric name, value of the metric value, and multiple tag,values . Example:\n[source,ruby]\n    [\n      \"%{host}/uptime\",\n      %{uptime_1m} \" ,\n      \"hostname\" ,\n      \"%{host}\n      \"anotherhostname\" ,\n      \"%{host}\n    ]\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "metrics",
        "validate": "array",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The address of the opentsdb server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The port to connect on your graphite server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "4242"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-nagios/master/lib/logstash/outputs/nagios.rb",
    "name": "nagios",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The full path to your Nagios command file.",
        "base": false,
        "name": "commandfile",
        "default": "/var/lib/nagios3/rw/nagios.cmd"
      },
      {
        "comments": "The Nagios check level. Should be one of 0=OK, 1=WARNING, 2=CRITICAL,\n3=UNKNOWN. Defaults to 2 - CRITICAL.",
        "base": false,
        "name": "nagios_level",
        "validate": [
          "0",
          "1",
          "2",
          "3"
        ],
        "default": "2"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-sqs/master/lib/logstash/outputs/sqs.rb",
    "name": "sqs",
    "type": "output",
    "params": [
      {
        "comments": "The name of the target SQS queue. Note that this is just the name of the\nqueue, not the URL or ARN.",
        "base": false,
        "name": "queue",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Set to `true` to send messages to SQS in batches (with the\n`SendMessageBatch` API) or `false` to send messages to SQS individually\n(with the `SendMessage` API). The size of the batch is configurable via\n`batch_events`.",
        "base": false,
        "name": "batch",
        "validate": "boolean",
        "default": true,
        "deprecated": true
      },
      {
        "comments": "The number of events to be sent in each batch. Set this to `1` to disable\nthe batch sending of messages.",
        "base": false,
        "name": "batch_events",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "",
        "base": false,
        "name": "batch_timeout",
        "validate": "number",
        "deprecated": "This setting no longer has any effect."
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The maximum number of bytes for any message sent to SQS. Messages exceeding\nthis size will be dropped. See\nhttp://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/limits-messages.html.",
        "base": false,
        "name": "message_max_size",
        "validate": "bytes",
        "default": "256KiB"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-sns/master/lib/logstash/outputs/sns.rb",
    "name": "sns",
    "type": "output",
    "params": [
      {
        "comments": "Optional ARN to send messages to. If you do not set this you must\ninclude the `sns` field in your events to set the ARN on a per-message basis!",
        "base": false,
        "name": "arn",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "When an ARN for an SNS topic is specified here, the message\n\"Logstash successfully booted\" will be sent to it when this plugin\nis registered.\n\nExample: arn:aws:sns:us-east-1:770975001275:logstash-testing\n",
        "base": false,
        "name": "publish_boot_message_arn",
        "validate": "string"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-udp/master/lib/logstash/outputs/udp.rb",
    "name": "udp",
    "type": "output",
    "params": [
      {
        "comments": "The address to send messages to",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The port to send messages on",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-statsd/master/lib/logstash/outputs/statsd.rb",
    "name": "statsd",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "A count metric. `metric_name => count` as hash. `%{fieldname}` substitutions are\nallowed in the metric names.",
        "base": false,
        "name": "count",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "A decrement metric. Metric names as array. `%{fieldname}` substitutions are\nallowed in the metric names.",
        "base": false,
        "name": "decrement",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "A gauge metric. `metric_name => gauge` as hash. `%{fieldname}` substitutions are\nallowed in the metric names.",
        "base": false,
        "name": "gauge",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The hostname or IP address of the statsd server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "An increment metric. Metric names as array. `%{fieldname}` substitutions are\nallowed in the metric names.",
        "base": false,
        "name": "increment",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The statsd namespace to use for this metric. `%{fieldname}` substitutions are\nallowed.",
        "base": false,
        "name": "namespace",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The port to connect to on your statsd server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8125"
      },
      {
        "comments": "The sample rate for the metric.",
        "base": false,
        "name": "sample_rate",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The name of the sender. Dots will be replaced with underscores. `%{fieldname}`\nsubstitutions are allowed.",
        "base": false,
        "name": "sender",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "A set metric. `metric_name => \"string\"` to append as hash. `%{fieldname}`\nsubstitutions are allowed in the metric names.",
        "base": false,
        "name": "set",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "A timing metric. `metric_name => duration` as hash. `%{fieldname}` substitutions\nare allowed in the metric names.",
        "base": false,
        "name": "timing",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-stdout/master/lib/logstash/outputs/stdout.rb",
    "name": "stdout",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-beats/master/lib/logstash/outputs/beats.rb",
    "name": "beats",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-dissect/master/lib/logstash/filters/dissect.rb",
    "name": "dissect",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "With this setting `int` and `float` datatype conversions can be specified. +\nThese will be done after all `mapping` dissections have taken place. +\nFeel free to use this setting on its own without a `mapping` section. +\n\nFor example\n[source, ruby]\nfilter {\n  dissect {\n    convert_datatype => {\n      cpu => \"float\"\n      code => \"int\"\n    }\n  }\n}",
        "base": false,
        "name": "convert_datatype",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "",
        "base": false,
        "name": "mapping",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Append values to the `tags` field when dissection fails",
        "base": false,
        "name": "tag_on_failure",
        "validate": "array",
        "default": "[\"_dissectfailure\"]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-gemfire/master/lib/logstash/inputs/gemfire.rb",
    "name": "gemfire",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Your client cache name",
        "base": false,
        "name": "cache_name",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The path to a GemFire client cache XML file.\n\nExample:\n\n     <client-cache>\n       <pool name=\"client-pool\" subscription-enabled=\"true\" subscription-redundancy=\"1\">\n           <locator host=\"localhost\" port=\"31331\"/>\n       </pool>\n       <region name=\"Logstash\">\n           <region-attributes refid=\"CACHING_PROXY\" pool-name=\"client-pool\" >\n           </region-attributes>\n       </region>\n     </client-cache>\n",
        "base": false,
        "name": "cache_xml_file",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "A regexp to use when registering interest for cache events.\nIgnored if a :query is specified.",
        "base": false,
        "name": "interest_regexp",
        "validate": "string",
        "default": ".*"
      },
      {
        "comments": "A query to run as a GemFire \"continuous query\"; if specified it takes\nprecedence over :interest_regexp which will be ignore.\n\nImportant: use of continuous queries requires subscriptions to be enabled on the client pool.",
        "base": false,
        "name": "query",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "The region name",
        "base": false,
        "name": "region_name",
        "validate": "string",
        "default": "Logstash"
      },
      {
        "comments": "How the message is serialized in the cache. Can be one of \"json\" or \"plain\"; default is plain",
        "base": false,
        "name": "serialization",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-generator/master/lib/logstash/inputs/generator.rb",
    "name": "generator",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Set how many messages should be generated.\n\nThe default, `0`, means generate an unlimited number of events.",
        "base": false,
        "name": "count",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The lines to emit, in order. This option cannot be used with the 'message'\nsetting.\n\nExample:\n[source,ruby]\n    input {\n      generator {\n        lines => [\n          \"line 1\",\n          \"line 2\",\n          \"line 3\"\n        ]\n        # Emit all lines 3 times.\n        count => 3\n      }\n    }\n\nThe above will emit `line 1` then `line 2` then `line`, then `line 1`, etc...",
        "base": false,
        "name": "lines",
        "validate": "array"
      },
      {
        "comments": "The message string to use in the event.\n\nIf you set this to `stdin` then this plugin will read a single line from\nstdin and use that as the message string for every event.\n\nOtherwise, this value will be used verbatim as the event message.",
        "base": false,
        "name": "message",
        "validate": "string",
        "default": "Hello world!"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-graphite/master/lib/logstash/inputs/graphite.rb",
    "name": "graphite",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-snmptrap/master/lib/logstash/inputs/snmptrap.rb",
    "name": "snmptrap",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "SNMP Community String to listen for.",
        "base": false,
        "name": "community",
        "validate": "array",
        "default": "public"
      },
      {
        "comments": "The address to listen on",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The port to listen on. Remember that ports less than 1024 (privileged\nports) may require root to use. hence the default of 1062.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "1062"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "directory of YAML MIB maps  (same format ruby-snmp uses)",
        "base": false,
        "name": "yamlmibdir",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-puppet_facter/master/lib/logstash/inputs/puppet_facter.rb",
    "name": "puppet_facter",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "uppet environment",
        "base": false,
        "name": "environment",
        "validate": "string",
        "default": "production"
      },
      {
        "comments": "emote IP Address to connect to",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "oll Interval in seconds",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "600"
      },
      {
        "comments": "emote port to connect to",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8140"
      },
      {
        "comments": "SL Private Key",
        "base": false,
        "name": "private_key",
        "validate": "path"
      },
      {
        "comments": "SL Public Key",
        "base": false,
        "name": "public_key",
        "validate": "path"
      },
      {
        "comments": "SL Enabled?",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-udp/master/lib/logstash/inputs/udp.rb",
    "name": "udp",
    "type": "input",
    "params": [
      {
        "comments": "The port which logstash will listen on. Remember that ports less\nthan 1024 (privileged ports) may require root or elevated privileges to use.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The maximum packet size to read from the network",
        "base": false,
        "name": "buffer_size",
        "validate": "number",
        "default": "65536"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The address which logstash will listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "This is the number of unprocessed UDP packets you can hold in memory\nbefore packets will start dropping.",
        "base": false,
        "name": "queue_size",
        "validate": "number",
        "default": "2000"
      },
      {
        "comments": "The socket receive buffer size in bytes.\nIf option is not set, the operating system default is used.\nThe operating system will use the max allowed value if receive_buffer_bytes is larger than allowed.\nConsult your operating system documentation if you need to increase this max allowed value.",
        "base": false,
        "name": "receive_buffer_bytes",
        "validate": "number"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Number of threads processing packets",
        "base": false,
        "name": "workers",
        "validate": "number",
        "default": "2"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-rackspace/master/lib/logstash/inputs/rackspace.rb",
    "name": "rackspace",
    "type": "input",
    "params": [
      {
        "comments": "Rackspace Cloud API Key",
        "base": false,
        "name": "api_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Rackspace Cloud Username",
        "base": false,
        "name": "username",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "number of messages to claim\nMin: 1, Max: 10",
        "base": false,
        "name": "claim",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Rackspace Queue Name",
        "base": false,
        "name": "queue",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Rackspace region\n`ord, dfw, lon, syd,` etc",
        "base": false,
        "name": "region",
        "validate": "string",
        "default": "dfw"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "length of time to hold claim\nMin: 60",
        "base": false,
        "name": "ttl",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-s3/master/lib/logstash/outputs/s3.rb",
    "name": "s3",
    "type": "output",
    "params": [
      {
        "comments": "S3 bucket",
        "base": false,
        "name": "bucket",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The S3 canned ACL to use when putting the file. Defaults to \"private\".",
        "base": false,
        "name": "canned_acl",
        "validate": "[\"private\", \"public_read\", \"public_read_write\", \"authenticated_read\"],"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Specify the content encoding. Supports (\"gzip\"). Defaults to \"none\"",
        "base": false,
        "name": "encoding",
        "validate": [
          "none",
          "gzip"
        ],
        "default": "none"
      },
      {
        "comments": "Specify a prefix to the uploaded filename, this can simulate directories on S3.  Prefix does not require leading slash.\nThis option support string interpolation, be warned this can created a lot of temporary local files.",
        "base": false,
        "name": "prefix",
        "validate": "string",
        "default": ""
      },
      {
        "comments": " IMPORTANT: if you use multiple instance of s3, you should specify on one of them the \"restore=> true\" and on the others \"restore => false\".\n This is hack for not destroy the new files after restoring the initial files.\n If you do not specify \"restore => true\" when logstash crashes or is restarted, the files are not sent into the bucket,\n for example if you have single Instance.",
        "base": false,
        "name": "restore",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Define the strategy to use to decide when we need to rotate the file and push it to S3,\nThe default strategy is to check for both size and time, the first one to match will rotate the file.",
        "base": false,
        "name": "rotation_strategy",
        "validate": [
          "size_and_time",
          "size",
          "time"
        ],
        "default": "size_and_time"
      },
      {
        "comments": "Specifies wether or not to use S3's server side encryption. Defaults to no encryption.",
        "base": false,
        "name": "server_side_encryption",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Specifies what type of encryption to use when SSE is enabled.",
        "base": false,
        "name": "server_side_encryption_algorithm",
        "validate": [
          "AES256",
          "awskms"
        ],
        "default": "AES256"
      },
      {
        "comments": "The version of the S3 signature hash to use. Normally uses the internal client default, can be explicitly\nspecified here",
        "base": false,
        "name": "signature_version",
        "validate": [
          "v2",
          "v4"
        ]
      },
      {
        "comments": "Set the size of file in bytes, this means that files on bucket when have dimension > file_size, they are stored in two or more file.\nIf you have tags then it will generate a specific size file for every tags\nNOTE: define size of file is the better thing, because generate a local temporary file on disk and then put it in bucket.",
        "base": false,
        "name": "size_file",
        "validate": "number",
        "default": "1024 * 1024 * 5"
      },
      {
        "comments": "The key to use when specified along with server_side_encryption => aws:kms.\nIf server_side_encryption => aws:kms is set but this is not default KMS key is used.\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html",
        "base": false,
        "name": "ssekms_key_id",
        "validate": "string"
      },
      {
        "comments": "Specifies what S3 storage class to use when uploading the file.\nMore information about the different storage classes can be found:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html\nDefaults to STANDARD.",
        "base": false,
        "name": "storage_class",
        "validate": [
          "STANDARD",
          "REDUCED_REDUNDANCY",
          "STANDARD_IA"
        ],
        "default": "STANDARD"
      },
      {
        "comments": "Define tags to be appended to the file on the S3 bucket.\n\nExample:\ntags => [\"elasticsearch\", \"logstash\", \"kibana\"]\n\nWill generate this file:\n\"ls.s3.logstash.local.2015-01-01T00.00.tag_elasticsearch.logstash.kibana.part0.txt\"\n",
        "base": false,
        "name": "tags",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Set the directory where logstash will store the tmp files before sending it to S3\ndefault to the current OS temporary directory in linux /tmp/logstash",
        "base": false,
        "name": "temporary_directory",
        "validate": "string",
        "default": "File.join(Dir.tmpdir, \"logstash\")"
      },
      {
        "comments": "Set the time, in MINUTES, to close the current sub_time_section of bucket.\nIf you define file_size you have a number of files in consideration of the section and the current tag.\n0 stay all time on listerner, beware if you specific 0 and size_file 0, because you will not put the file on bucket,\nfor now the only thing this plugin can do is to put the file when logstash restart.",
        "base": false,
        "name": "time_file",
        "validate": "number",
        "default": "15"
      },
      {
        "comments": "Number of items we can keep in the local queue before uploading them",
        "base": false,
        "name": "upload_queue_size",
        "validate": "number",
        "default": "2 * (Concurrent.processor_count * 0.25).ceil"
      },
      {
        "comments": "Specify how many workers to use to upload the files to S3",
        "base": false,
        "name": "upload_workers_count",
        "validate": "number",
        "default": "(Concurrent.processor_count * 0.5).ceil"
      },
      {
        "comments": "The common use case is to define permission on the root bucket and give Logstash full access to write its logs.\nIn some circonstances you need finer grained permission on subfolder, this allow you to disable the check at startup.",
        "base": false,
        "name": "validate_credentials_on_root_bucket",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-csv/master/lib/logstash/outputs/csv.rb",
    "name": "csv",
    "type": "output",
    "params": [
      {
        "comments": "The field names from the event that should be written to the CSV file.\nFields are written to the CSV in the same order as the array.\nIf a field does not exist on the event, an empty string will be written.\nSupports field reference syntax eg: `fields => [\"field1\", \"[nested][field]\"]`.",
        "base": false,
        "name": "fields",
        "validate": "array",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Options for CSV output. This is passed directly to the Ruby stdlib to_csv function.\nFull documentation is available on the http://ruby-doc.org/stdlib-2.0.0/libdoc/csv/rdoc/index.html[Ruby CSV documentation page].\nA typical use case would be to use alternative column or row seperators eg: `csv_options => {\"col_sep\" => \"\\t\" \"row_sep\" => \"\\r\\n\"}` gives tab seperated data with windows line endings",
        "base": false,
        "name": "csv_options",
        "validate": "hash",
        "required": false,
        "default": "Hash.new"
      },
      {
        "comments": "Option to not escape/munge string values. Please note turning off this option\nmay not make the values safe in your spreadsheet application",
        "base": false,
        "name": "spreadsheet_safe",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-zeromq/master/lib/logstash/inputs/zeromq.rb",
    "name": "zeromq",
    "type": "input",
    "params": [
      {
        "comments": "0mq topology\nThe default logstash topologies work as follows:\n\n* pushpull - inputs are pull, outputs are push\n* pubsub - inputs are subscribers, outputs are publishers\n* pair - inputs are clients, inputs are servers\n\nIf the predefined topology flows don't work for you,\nyou can change the `mode` setting\nTODO (lusis) add req/rep MAYBE\nTODO (lusis) add router/dealer",
        "base": false,
        "name": "topology",
        "validate": [
          "pushpull",
          "pubsub",
          "pair"
        ],
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "0mq socket address to connect or bind\nPlease note that `inproc://` will not work with logstash\nas each we use a context per thread.\nBy default, inputs bind/listen\nand outputs connect",
        "base": false,
        "name": "address",
        "validate": "array",
        "default": "[\"tcp//*2120\"]"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "mode\nserver mode binds/listens\nclient mode connects",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "server"
      },
      {
        "comments": "sender\noverrides the sender to\nset the source of the event\ndefault is `zmq+topology://type/`",
        "base": false,
        "name": "sender",
        "validate": "string"
      },
      {
        "comments": "0mq socket options\nThis exposes `zmq_setsockopt`\nfor advanced tuning\nsee http://api.zeromq.org/2-1:zmq-setsockopt for details\n\nThis is where you would set values like:\n\n * `ZMQ::HWM` - high water mark\n * `ZMQ::IDENTITY` - named queues\n * `ZMQ::SWAP_SIZE` - space for disk overflow\n\nExample:\n[source,ruby]\n    sockopt => {\n       \"ZMQ::HWM\" => 50\n       \"ZMQ::IDENTITY\"  => \"my_named_queue\"\n    }\n\ndefaults to: `sockopt => { \"ZMQ::RCVTIMEO\" => \"1000\" }`, which has the effect of \"interrupting\"\nthe recv operation at least once every second to allow for properly shutdown handling.",
        "base": false,
        "name": "sockopt",
        "validate": "hash",
        "default": "{ \"ZMQRCVTIMEO\" => \"1000\" }"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "0mq topic\nThis is used for the `pubsub` topology only\nOn inputs, this allows you to filter messages by topic\nOn outputs, this allows you to tag a message for routing\nNOTE: ZeroMQ does subscriber side filtering.\nNOTE: All topics have an implicit wildcard at the end\nYou can specify multiple topics here",
        "base": false,
        "name": "topic",
        "validate": "array"
      },
      {
        "comments": "Event topic field\nThis is used for the `pubsub` topology only\nWhen a message is received on a topic, the topic name on which\nthe message was received will saved in this field.",
        "base": false,
        "name": "topic_field",
        "validate": "string",
        "default": "topic"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-xmpp/master/lib/logstash/inputs/xmpp.rb",
    "name": "xmpp",
    "type": "input",
    "params": [
      {
        "comments": "The xmpp password for the user/identity.",
        "base": false,
        "name": "password",
        "validate": "password",
        "required": true
      },
      {
        "comments": "The user or resource ID, like `foo@example.com`.",
        "base": false,
        "name": "user",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The xmpp server to connect to. This is optional. If you omit this setting,\nthe host on the user/identity is used. (`foo.com` for `user@foo.com`)",
        "base": false,
        "name": "host",
        "validate": "string"
      },
      {
        "comments": "if muc/multi-user-chat required, give the name of the room that\nyou want to join: `room@conference.domain/nick`",
        "base": false,
        "name": "rooms",
        "validate": "array"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-hipchat/master/lib/logstash/outputs/hipchat.rb",
    "name": "hipchat",
    "type": "output",
    "params": [
      {
        "comments": "The ID or name of the room, support fieldref",
        "base": false,
        "name": "room_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The HipChat authentication token.",
        "base": false,
        "name": "token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Background color for message.\nHipChat currently supports one of \"yellow\", \"red\", \"green\", \"purple\",\n\"gray\", or \"random\". (default: yellow), support fieldref",
        "base": false,
        "name": "color",
        "validate": "string",
        "default": "yellow"
      },
      {
        "comments": "Message format to send, event tokens are usable here.",
        "base": false,
        "name": "format",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "The name the message will appear be sent from, you can use fieldref",
        "base": false,
        "name": "from",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "HipChat host to use",
        "base": false,
        "name": "host",
        "validate": "string"
      },
      {
        "comments": "Specify `Message Format`",
        "base": false,
        "name": "message_format",
        "validate": [
          "html",
          "text"
        ],
        "default": "html"
      },
      {
        "comments": "Whether or not this message should trigger a notification for people in the room.",
        "base": false,
        "name": "trigger_notify",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-nagios_nsca/master/lib/logstash/outputs/nagios_nsca.rb",
    "name": "nagios_nsca",
    "type": "output",
    "params": [
      {
        "comments": "The status to send to nagios. Should be 0 = OK, 1 = WARNING, 2 = CRITICAL, 3 = UNKNOWN",
        "base": false,
        "name": "nagios_status",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The nagios host or IP to send logs to. It should have a NSCA daemon running.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The format to use when writing events to nagios. This value\nsupports any string and can include `%{name}` and other dynamic\nstrings.",
        "base": false,
        "name": "message_format",
        "validate": "string",
        "default": "%{@timestamp} %{host} %{message}"
      },
      {
        "comments": "The nagios 'host' you want to submit a passive check result to. This\nparameter accepts interpolation, e.g. you can use `@source_host` or other\nlogstash internal variables.",
        "base": false,
        "name": "nagios_host",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "The nagios 'service' you want to submit a passive check result to. This\nparameter accepts interpolation, e.g. you can use `@source_host` or other\nlogstash internal variables.",
        "base": false,
        "name": "nagios_service",
        "validate": "string",
        "default": "LOGSTASH"
      },
      {
        "comments": "The port where the NSCA daemon on the nagios host listens.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "5667"
      },
      {
        "comments": "The path to the 'send_nsca' binary on the local host.",
        "base": false,
        "name": "send_nsca_bin",
        "validate": "string",
        "default": "/usr/sbin/send_nsca"
      },
      {
        "comments": "The path to the send_nsca config file on the local host.\nLeave blank if you don't want to provide a config file.",
        "base": false,
        "name": "send_nsca_config",
        "validate": "path"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-pipe/master/lib/logstash/outputs/pipe.rb",
    "name": "pipe",
    "type": "output",
    "params": [
      {
        "comments": "Command line to launch and pipe to",
        "base": false,
        "name": "command",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The format to use when writing events to the pipe. This value\nsupports any string and can include `%{name}` and other dynamic\nstrings.\n\nIf this setting is omitted, the full json representation of the\nevent will be written as a single line.",
        "base": false,
        "name": "message_format",
        "validate": "string"
      },
      {
        "comments": "Close pipe that hasn't been used for TTL seconds. -1 or 0 means never close.",
        "base": false,
        "name": "ttl",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-irc/master/lib/logstash/outputs/irc.rb",
    "name": "irc",
    "type": "output",
    "params": [
      {
        "comments": "Channels to broadcast to.\n\nThese should be full channel names including the '#' symbol, such as\n\"#logstash\".",
        "base": false,
        "name": "channels",
        "validate": "array",
        "required": true
      },
      {
        "comments": "Address of the host to connect to",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Message format to send, event tokens are usable here",
        "base": false,
        "name": "format",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "Limit the rate of messages sent to IRC in messages per second.",
        "base": false,
        "name": "messages_per_second",
        "validate": "number",
        "default": "0.5"
      },
      {
        "comments": "IRC Nickname",
        "base": false,
        "name": "nick",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "IRC server password",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "Port on host to connect to.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "6667"
      },
      {
        "comments": "Static string after event",
        "base": false,
        "name": "post_string",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Static string before event",
        "base": false,
        "name": "pre_string",
        "validate": "string",
        "required": false
      },
      {
        "comments": "IRC Real name",
        "base": false,
        "name": "real",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Set this to true to enable SSL.",
        "base": false,
        "name": "secure",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "IRC Username",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-redis/master/lib/logstash/outputs/redis.rb",
    "name": "redis",
    "type": "output",
    "params": [
      {
        "comments": "Set to true if you want Redis to batch up values and send 1 RPUSH command\ninstead of one command per value to push on the list.  Note that this only\nworks with `data_type=\"list\"` mode right now.\n\nIf true, we send an RPUSH every \"batch_events\" events or\n\"batch_timeout\" seconds (whichever comes first).\nOnly supported for `data_type` is \"list\".",
        "base": false,
        "name": "batch",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If batch is set to true, the number of events we queue up for an RPUSH.",
        "base": false,
        "name": "batch_events",
        "validate": "number",
        "default": "50"
      },
      {
        "comments": "If batch is set to true, the maximum amount of time between RPUSH commands\nwhen there are pending events to flush.",
        "base": false,
        "name": "batch_timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "How often to check for congestion. Default is one second.\nZero means to check on every event.",
        "base": false,
        "name": "congestion_interval",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "In case Redis `data_type` is `list` and has more than `@congestion_threshold` items,\nblock until someone consumes them and reduces congestion, otherwise if there are\nno consumers Redis will run out of memory, unless it was configured with OOM protection.\nBut even with OOM protection, a single Redis list can block all other users of Redis,\nuntil Redis CPU consumption reaches the max allowed RAM size.\nA default value of 0 means that this limit is disabled.\nOnly supported for `list` Redis `data_type`.",
        "base": false,
        "name": "congestion_threshold",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "Either list or channel.  If `redis_type` is list, then we will set\nRPUSH to key. If `redis_type` is channel, then we will PUBLISH to `key`.",
        "base": false,
        "name": "data_type",
        "validate": [
          "list",
          "channel"
        ],
        "required": false
      },
      {
        "comments": "The Redis database number.",
        "base": false,
        "name": "db",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The hostname(s) of your Redis server(s). Ports may be specified on any\nhostname, which will override the global port config.\nIf the hosts list is an array, Logstash will pick one random host to connect to,\nif that host is disconnected it will then pick another.\n\nFor example:\n[source,ruby]\n    \"127.0.0.1\"\n    [\"127.0.0.1\", \"127.0.0.2\"]\n    [\"127.0.0.1:6380\", \"127.0.0.1\"]",
        "base": false,
        "name": "host",
        "validate": "array",
        "default": "[\"127.0.0.1\"]"
      },
      {
        "comments": "The name of a Redis list or channel. Dynamic names are\nvalid here, for example `logstash-%{type}`.",
        "base": false,
        "name": "key",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Name is used for logging in case there are multiple instances.",
        "base": false,
        "name": "name",
        "validate": "string",
        "default": "\"default\","
      },
      {
        "comments": "Password to authenticate with.  There is no authentication by default.",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "The default port to connect on. Can be overridden on any hostname.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "6379"
      },
      {
        "comments": "The name of the Redis queue (we'll use RPUSH on this). Dynamic names are\nvalid here, for example `logstash-%{type}`",
        "base": false,
        "name": "queue",
        "validate": "string",
        "deprecated": true
      },
      {
        "comments": "Interval for reconnecting to failed Redis connections",
        "base": false,
        "name": "reconnect_interval",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Shuffle the host list during Logstash startup.",
        "base": false,
        "name": "shuffle_hosts",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Redis initial connection timeout in seconds.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-null/master/lib/logstash/outputs/null.rb",
    "name": "null",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-gelf/master/lib/logstash/outputs/gelf.rb",
    "name": "gelf",
    "type": "output",
    "params": [
      {
        "comments": "Graylog2 server IP address or hostname.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The GELF chunksize. You usually don't need to change this.",
        "base": false,
        "name": "chunksize",
        "validate": "number",
        "default": "1420"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The GELF custom field mappings. GELF supports arbitrary attributes as custom\nfields. This exposes that. Exclude the `_` portion of the field name\ne.g. `custom_fields => ['foo_field', 'some_value']`\nsets `_foo_field` = `some_value`.",
        "base": false,
        "name": "custom_fields",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The GELF full message. Dynamic values like `%{foo}` are permitted here.",
        "base": false,
        "name": "full_message",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "Ignore these fields when `ship_metadata` is set. Typically this lists the\nfields used in dynamic values for GELF fields.",
        "base": false,
        "name": "ignore_metadata",
        "validate": "array",
        "default": "[\"@timestamp\",\"@version\",\"severity\",\"host\",\"source_host\",\"source_path\",\"short_message\"]"
      },
      {
        "comments": "The GELF message level. Dynamic values like `%{level}` are permitted here;\nuseful if you want to parse the 'log level' from an event and use that\nas the GELF level/severity.\n\nValues here can be integers [0..7] inclusive or any of\n\"debug\", \"info\", \"warn\", \"error\", \"fatal\" (case insensitive).\nSingle-character versions of these are also valid, \"d\", \"i\", \"w\", \"e\", \"f\",\n\"u\"\nThe following additional severity\\_labels from Logstash's  syslog\\_pri filter\nare accepted: \"emergency\", \"alert\", \"critical\",  \"warning\", \"notice\", and\n\"informational\".",
        "base": false,
        "name": "level",
        "validate": "array",
        "default": "[\"%{severity}\",\"INFO\"]"
      },
      {
        "comments": "Graylog2 server port number.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "12201"
      },
      {
        "comments": "Allow overriding of the GELF `sender` field. This is useful if you\nwant to use something other than the event's source host as the\n\"sender\" of an event. A common case for this is using the application name\ninstead of the hostname.",
        "base": false,
        "name": "sender",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "Should Logstash ship metadata within event object? This will cause Logstash\nto ship any fields in the event (such as those created by grok) in the GELF\nmessages. These will be sent as underscored \"additional fields\".",
        "base": false,
        "name": "ship_metadata",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Ship tags within events. This will cause Logstash to ship the tags of an\nevent as the field `\\_tags`.",
        "base": false,
        "name": "ship_tags",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The GELF short message field name. If the field does not exist or is empty,\nthe event message is taken instead.",
        "base": false,
        "name": "short_message",
        "validate": "string",
        "default": "short_message"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-pagerduty/master/lib/logstash/outputs/pagerduty.rb",
    "name": "pagerduty",
    "type": "output",
    "params": [
      {
        "comments": "The PagerDuty Service API Key",
        "base": false,
        "name": "service_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Custom description",
        "base": false,
        "name": "description",
        "validate": "string",
        "default": "Logstash event for %{host}"
      },
      {
        "comments": "The event details. These might be data from the Logstash event fields you wish to include.\nTags are automatically included if detected so there is no need to explicitly add them here.",
        "base": false,
        "name": "details",
        "validate": "hash",
        "default": "{\"timestamp\" => \"%{@timestamp}\", \"message\" => \"%{message}\"}"
      },
      {
        "comments": "Event type",
        "base": false,
        "name": "event_type",
        "validate": [
          "trigger",
          "acknowledge",
          "resolve"
        ],
        "default": "trigger"
      },
      {
        "comments": "The service key to use. You'll need to set this up in PagerDuty beforehand.",
        "base": false,
        "name": "incident_key",
        "validate": "string",
        "default": "logstash/%{host}/%{type}"
      },
      {
        "comments": "PagerDuty API URL. You shouldn't need to change this, but is included to allow for flexibility\nshould PagerDuty iterate the API and Logstash hasn't been updated yet.",
        "base": false,
        "name": "pdurl",
        "validate": "string",
        "default": "https//events.pagerduty.com/generic/2010-04-15/create_event.json"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-rabbitmq/master/lib/logstash/outputs/rabbitmq.rb",
    "name": "rabbitmq",
    "type": "output",
    "params": [
      {
        "comments": "The name of the exchange",
        "base": false,
        "name": "exchange",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The exchange type (fanout, topic, direct)",
        "base": false,
        "name": "exchange_type",
        "validate": "EXCHANGE_TYPES",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Is this exchange durable? (aka; Should it survive a broker restart?)",
        "base": false,
        "name": "durable",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Key to route to by default. Defaults to 'logstash'\n\n* Routing keys are ignored on fanout exchanges.",
        "base": false,
        "name": "key",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Should RabbitMQ persist messages to disk?",
        "base": false,
        "name": "persistent",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-tcp/master/lib/logstash/outputs/tcp.rb",
    "name": "tcp",
    "type": "output",
    "params": [
      {
        "comments": "When mode is `server`, the address to listen on.\nWhen mode is `client`, the address to connect to.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "When mode is `server`, the port to listen on.\nWhen mode is `client`, the port to connect to.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The format to use when writing events to the file. This value\nsupports any string and can include `%{name}` and other dynamic\nstrings.\n\nIf this setting is omitted, the full json representation of the\nevent will be written as a single line.",
        "base": false,
        "name": "message_format",
        "validate": "string",
        "deprecated": true
      },
      {
        "comments": "Mode to operate in. `server` listens for client connections,\n`client` connects to a server.",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "client"
      },
      {
        "comments": "When connect failed,retry interval in sec.",
        "base": false,
        "name": "reconnect_interval",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "The SSL CA certificate, chainfile or CA path. The system CA path is automatically included.",
        "base": false,
        "name": "ssl_cacert",
        "validate": "path"
      },
      {
        "comments": "SSL certificate path",
        "base": false,
        "name": "ssl_cert",
        "validate": "path"
      },
      {
        "comments": "Enable SSL (must be set for other `ssl_` options to take effect).",
        "base": false,
        "name": "ssl_enable",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "SSL key path",
        "base": false,
        "name": "ssl_key",
        "validate": "path"
      },
      {
        "comments": "SSL key passphrase",
        "base": false,
        "name": "ssl_key_passphrase",
        "validate": "password",
        "default": "nil"
      },
      {
        "comments": "Verify the identity of the other end of the SSL connection against the CA.\nFor input, sets the field `sslsubject` to that of the client certificate.",
        "base": false,
        "name": "ssl_verify",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-zookeeper/master/lib/logstash/outputs/zookeeper.rb",
    "name": "zookeeper",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Znode we created is permanent or ephemeral.",
        "base": false,
        "name": "ephemeral",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "This is the zookeeper ip list.\nThe format is `host1:port1,host2:port2`",
        "base": false,
        "name": "ip_list",
        "validate": "string",
        "default": "localhost2181"
      },
      {
        "comments": "The znode path you want to write.\nIf the path not exist, we will create it.",
        "base": false,
        "name": "path",
        "validate": "string",
        "default": "/logstash"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-language/master/lib/logstash/filters/language.rb",
    "name": "language",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "",
        "base": false,
        "name": "amount_of_chars",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "",
        "base": false,
        "name": "concat_fields",
        "validate": "boolean"
      },
      {
        "comments": "",
        "base": false,
        "name": "concat_prefix",
        "validate": "string",
        "default": "language"
      },
      {
        "comments": "Replace the message with this value.",
        "base": false,
        "name": "fields",
        "validate": "array",
        "default": "message"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-example/master/lib/logstash/outputs/example.rb",
    "name": "example",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-elasticsearch/master/lib/logstash/outputs/elasticsearch.rb",
    "name": "elasticsearch",
    "type": "output",
    "params": [
      {
        "comments": "The Elasticsearch action to perform. Valid actions are:\n\n- index: indexes a document (an event from Logstash).\n- delete: deletes a document by id (An id is required for this action)\n- create: indexes a document, fails if a document by that id already exists in the index.\n- update: updates a document by id. Update has a special case where you can upsert -- update a\n  document if not already present. See the `upsert` option. NOTE: This does not work and is not supported\n  in Elasticsearch 1.x. Please upgrade to ES 2.x or greater to use this feature with Logstash!\n- A sprintf style string to change the action based on the content of the event. The value `%{[foo]}`\n  would use the foo field for the action\n\nFor more details on actions, check out the http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html[Elasticsearch bulk API documentation]",
        "base": false,
        "name": "action",
        "validate": "string",
        "default": "index"
      },
      {
        "comments": "HTTP Path to perform the _bulk requests to\nthis defaults to a concatenation of the path parameter and \"_bulk\"",
        "base": false,
        "name": "bulk_path",
        "validate": "string"
      },
      {
        "comments": "The .cer or .pem file to validate the server's certificate",
        "base": false,
        "name": "cacert",
        "validate": "path"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Enable `doc_as_upsert` for update mode.\nCreate a new document with source if `document_id` doesn't exist in Elasticsearch",
        "base": true,
        "name": "doc_as_upsert",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The document ID for the index. Useful for overwriting existing entries in\nElasticsearch with the same ID.",
        "base": true,
        "name": "document_id",
        "validate": "string"
      },
      {
        "comments": "The document type to write events to. Generally you should try to write only\nsimilar events to the same 'type'. String expansion `%{foo}` works here.\nUnless you set 'document_type', the event 'type' will be used if it exists\notherwise the document type will be assigned the value of 'logs'",
        "base": true,
        "name": "document_type",
        "validate": "string"
      },
      {
        "comments": "Set the Elasticsearch errors in the whitelist that you don't want to log.\nA useful example is when you want to skip all 409 errors\nwhich are `document_already_exists_exception`.",
        "base": false,
        "name": "failure_type_logging_whitelist",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "This plugin uses the bulk index API for improved indexing performance.\nThis setting defines the maximum sized bulk request Logstash will make.\nYou may want to increase this to be in line with your pipeline's batch size.\nIf you specify a number larger than the batch size of your pipeline it will have no effect,\nsave for the case where a filter increases the size of an inflight batch by outputting\nevents.",
        "base": true,
        "name": "flush_size",
        "validate": "number",
        "deprecate": "\"This setting is no longer necessary as we now try to restrict bulk requests to sane sizes. See the \"Batch Sizes\" section of the docs. If you think you still need to restrict payloads based on the number, not size, of events, please open a ticket.\""
      },
      {
        "comments": "HTTP Path where a HEAD request is sent when a backend is marked down\nthe request is sent in the background to see if it has come back again\nbefore it is once again eligible to service requests.\nIf you have custom firewall rules you may need to change this",
        "base": false,
        "name": "healthcheck_path",
        "validate": "string"
      },
      {
        "comments": "Sets the host(s) of the remote instance. If given an array it will load balance requests across the hosts specified in the `hosts` parameter.\nRemember the `http` protocol uses the http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-http.html#modules-http[http] address (eg. 9200, not 9300).\n    `\"127.0.0.1\"`\n    `[\"127.0.0.1:9200\",\"127.0.0.2:9200\"]`\n    `[\"http://127.0.0.1\"]`\n    `[\"https://127.0.0.1:9200\"]`\n    `[\"https://127.0.0.1:9200/mypath\"]` (If using a proxy on a subpath)\nIt is important to exclude http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html[dedicated master nodes] from the `hosts` list\nto prevent LS from sending bulk requests to the master nodes.  So this parameter should only reference either data or client nodes in Elasticsearch.\n\nAny special characters present in the URLs here MUST be URL escaped! This means `#` should be put in as `%23` for instance.",
        "base": true,
        "name": "hosts",
        "validate": "uri",
        "default": "[LogStashUtilSafeURI.new(\"//127.0.0.1\")]",
        "list": true
      },
      {
        "comments": "Enable gzip compression on requests. Note that response compression is on by default for Elasticsearch v5.0 and beyond",
        "base": false,
        "name": "http_compression",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The amount of time since last flush before a flush is forced.\n\nThis setting helps ensure slow event rates don't get stuck in Logstash.\nFor example, if your `flush_size` is 100, and you have received 10 events,\nand it has been more than `idle_flush_time` seconds since the last flush,\nLogstash will flush those 10 events automatically.\n\nThis helps keep both fast and slow log streams moving along in\nnear-real-time.",
        "base": true,
        "name": "idle_flush_time",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The index to write events to. This can be dynamic using the `%{foo}` syntax.\nThe default value will partition your indices by day so you can more easily\ndelete old data or only search specific date ranges.\nIndexes may not contain uppercase characters.\nFor weekly indexes ISO 8601 format is recommended, eg. logstash-%{+xxxx.ww}.\nLS uses Joda to format the index pattern from event timestamp.\nJoda formats are defined http://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html[here].",
        "base": true,
        "name": "index",
        "validate": "string",
        "default": "logstash-%{+YYYY.MM.dd}"
      },
      {
        "comments": "The keystore used to present a certificate to the server.\nIt can be either .jks or .p12",
        "base": false,
        "name": "keystore",
        "validate": "path"
      },
      {
        "comments": "Set the keystore password",
        "base": false,
        "name": "keystore_password",
        "validate": "password"
      },
      {
        "comments": "From Logstash 1.3 onwards, a template is applied to Elasticsearch during\nLogstash's startup if one with the name `template_name` does not already exist.\nBy default, the contents of this template is the default template for\n`logstash-%{+YYYY.MM.dd}` which always matches indices based on the pattern\n`logstash-*`.  Should you require support for other index names, or would like\nto change the mappings in the template in general, a custom template can be\nspecified by setting `template` to the path of a template file.\n\nSetting `manage_template` to false disables this feature.  If you require more\ncontrol over template creation, (e.g. creating indices dynamically based on\nfield names) you should set `manage_template` to false and use the REST\nAPI to apply your templates manually.",
        "base": true,
        "name": "manage_template",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Pass a set of key value pairs as the URL query string. This query string is added\nto every host listed in the 'hosts' configuration. If the 'hosts' list contains\nurls that already have query strings, the one specified here will be appended.",
        "base": false,
        "name": "parameters",
        "validate": "hash"
      },
      {
        "comments": "For child documents, ID of the associated parent.\nThis can be dynamic using the `%{foo}` syntax.",
        "base": true,
        "name": "parent",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Password to authenticate to a secure Elasticsearch cluster",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "HTTP Path at which the Elasticsearch server lives. Use this if you must run Elasticsearch behind a proxy that remaps\nthe root path for the Elasticsearch HTTP API lives.\nNote that if you use paths as components of URLs in the 'hosts' field you may\nnot also set this field. That will raise an error at startup",
        "base": false,
        "name": "path",
        "validate": "string"
      },
      {
        "comments": "Set which ingest pipeline you wish to execute for an event. You can also use event dependent configuration\nhere like `pipeline => \"%{INGEST_PIPELINE}\"`",
        "base": true,
        "name": "pipeline",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "While the output tries to reuse connections efficiently we have a maximum.\nThis sets the maximum number of open connections the output will create.\nSetting this too low may mean frequently closing / opening connections\nwhich is bad.",
        "base": false,
        "name": "pool_max",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "While the output tries to reuse connections efficiently we have a maximum per endpoint.\nThis sets the maximum number of open connections per endpoint the output will create.\nSetting this too low may mean frequently closing / opening connections\nwhich is bad.",
        "base": false,
        "name": "pool_max_per_route",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "Set the address of a forward HTTP proxy.\nThis used to accept hashes as arguments but now only accepts\narguments of the URI type to prevent leaking credentials.",
        "base": false,
        "name": "proxy",
        "validate": "uri"
      },
      {
        "comments": "How frequently, in seconds, to wait between resurrection attempts.\nResurrection is the process by which backend endpoints marked 'down' are checked\nto see if they have come back to life",
        "base": false,
        "name": "resurrect_delay",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Set initial interval in seconds between bulk retries. Doubled on each retry up to `retry_max_interval`",
        "base": true,
        "name": "retry_initial_interval",
        "validate": "number",
        "default": "2"
      },
      {
        "comments": "Set max interval in seconds between bulk retries.",
        "base": true,
        "name": "retry_max_interval",
        "validate": "number",
        "default": "64"
      },
      {
        "comments": "The number of times Elasticsearch should internally retry an update/upserted document\nSee the https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-updates.html[partial updates]\nfor more info",
        "base": true,
        "name": "retry_on_conflict",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "A routing override to be applied to all processed events.\nThis can be dynamic using the `%{foo}` syntax.",
        "base": true,
        "name": "routing",
        "validate": "string"
      },
      {
        "comments": "Set script name for scripted update mode",
        "base": true,
        "name": "script",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Set the language of the used script. If not set, this defaults to painless in ES 5.0",
        "base": true,
        "name": "script_lang",
        "validate": "string",
        "default": "painless"
      },
      {
        "comments": "Define the type of script referenced by \"script\" variable\n inline : \"script\" contains inline script\n indexed : \"script\" contains the name of script directly indexed in elasticsearch\n file    : \"script\" contains the name of script stored in elasticseach's config directory",
        "base": true,
        "name": "script_type",
        "validate": [
          "inline",
          "indexed",
          "file"
        ],
        "default": "[\"inline\"]"
      },
      {
        "comments": "Set variable name passed to script (scripted update)",
        "base": true,
        "name": "script_var_name",
        "validate": "string",
        "default": "event"
      },
      {
        "comments": "if enabled, script is in charge of creating non-existent document (scripted update)",
        "base": true,
        "name": "scripted_upsert",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "This setting asks Elasticsearch for the list of all cluster nodes and adds them to the hosts list.\nNote: This will return ALL nodes with HTTP enabled (including master nodes!). If you use\nthis with master nodes, you probably want to disable HTTP on them by setting\n`http.enabled` to false in their elasticsearch.yml. You can either use the `sniffing` option or\nmanually enter multiple Elasticsearch hosts using the `hosts` parameter.",
        "base": false,
        "name": "sniffing",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "How long to wait, in seconds, between sniffing attempts",
        "base": false,
        "name": "sniffing_delay",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "HTTP Path to be used for the sniffing requests\nthe default value is computed by concatenating the path value and \"_nodes/http\"\nif sniffing_path is set it will be used as an absolute path\ndo not use full URL here, only paths, e.g. \"/sniff/_nodes/http\"",
        "base": false,
        "name": "sniffing_path",
        "validate": "string"
      },
      {
        "comments": "Enable SSL/TLS secured communication to Elasticsearch cluster. Leaving this unspecified will use whatever scheme\nis specified in the URLs listed in 'hosts'. If no explicit protocol is specified plain HTTP will be used.\nIf SSL is explicitly disabled here the plugin will refuse to start if an HTTPS URL is given in 'hosts'",
        "base": false,
        "name": "ssl",
        "validate": "boolean"
      },
      {
        "comments": "Option to validate the server's certificate. Disabling this severely compromises security.\nFor more information on disabling certificate verification please read\nhttps://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf",
        "base": false,
        "name": "ssl_certificate_verification",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "You can set the path to your own template here, if you so desire.\nIf not set, the included template will be used.",
        "base": true,
        "name": "template",
        "validate": "path"
      },
      {
        "comments": "This configuration option defines how the template is named inside Elasticsearch.\nNote that if you have used the template management features and subsequently\nchange this, you will need to prune the old template manually, e.g.\n\n`curl -XDELETE <http://localhost:9200/_template/OldTemplateName?pretty>`\n\nwhere `OldTemplateName` is whatever the former setting was.",
        "base": true,
        "name": "template_name",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The template_overwrite option will always overwrite the indicated template\nin Elasticsearch with either the one indicated by template or the included one.\nThis option is set to false by default. If you always want to stay up to date\nwith the template provided by Logstash, this option could be very useful to you.\nLikewise, if you have your own template file managed by puppet, for example, and\nyou wanted to be able to update it regularly, this option could help there as well.\n\nPlease note that if you are using your own customized version of the Logstash\ntemplate (logstash), setting this to true will make Logstash to overwrite\nthe \"logstash\" template (i.e. removing all customized settings)",
        "base": true,
        "name": "template_overwrite",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Set the timeout, in seconds, for network operations and requests sent Elasticsearch. If\na timeout occurs, the request will be retried.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The JKS truststore to validate the server's certificate.\nUse either `:truststore` or `:cacert`",
        "base": false,
        "name": "truststore",
        "validate": "path"
      },
      {
        "comments": "Set the truststore password",
        "base": false,
        "name": "truststore_password",
        "validate": "password"
      },
      {
        "comments": "Set upsert content for update mode.s\nCreate a new document with this parameter as json string if `document_id` doesn't exists",
        "base": true,
        "name": "upsert",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Username to authenticate to a secure Elasticsearch cluster",
        "base": false,
        "name": "user",
        "validate": "string"
      },
      {
        "comments": "How long to wait before checking if the connection is stale before executing a request on a connection using keepalive.\nYou may want to set this lower, if you get connection errors regularly\nQuoting the Apache commons docs (this client is based Apache Commmons):\n'Defines period of inactivity in milliseconds after which persistent connections must\nbe re-validated prior to being leased to the consumer. Non-positive value passed to\nthis method disables connection validation. This check helps detect connections that\nhave become stale (half-closed) while kept inactive in the pool.'\nSee https://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/impl/conn/PoolingHttpClientConnectionManager.html#setValidateAfterInactivity(int)[these docs for more info]",
        "base": false,
        "name": "validate_after_inactivity",
        "validate": "number",
        "default": "10000"
      },
      {
        "comments": "The version to use for indexing. Use sprintf syntax like `%{my_version}` to use a field value here.\nSee https://www.elastic.co/blog/elasticsearch-versioning-support.",
        "base": true,
        "name": "version",
        "validate": "string"
      },
      {
        "comments": "The version_type to use for indexing.\nSee https://www.elastic.co/blog/elasticsearch-versioning-support.\nSee also https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types",
        "base": true,
        "name": "version_type",
        "validate": [
          "internal",
          "external",
          "external_gt",
          "external_gte",
          "force"
        ]
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-tld/master/lib/logstash/filters/tld.rb",
    "name": "tld",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The source field to parse",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The target field to place all the data",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "tld"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-jms/master/lib/logstash/inputs/jms.rb",
    "name": "jms",
    "type": "input",
    "params": [
      {
        "comments": "Name of the destination queue or topic to use.",
        "base": false,
        "name": "destination",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Url to use when connecting to the JMS provider",
        "base": false,
        "name": "broker_url",
        "validate": "string"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Name of JMS Provider Factory class",
        "base": false,
        "name": "factory",
        "validate": "string"
      },
      {
        "comments": "Include JMS Message Body in the event\nSupports TextMessage, MapMessage and ByteMessage\nIf the JMS Message is a TextMessage or ByteMessage, then the value will be in the \"message\" field of the event\nIf the JMS Message is a MapMessage, then all the key/value pairs will be added in the Hashmap of the event\nStreamMessage and ObjectMessage are not supported",
        "base": false,
        "name": "include_body",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "A JMS message has three parts :\n Message Headers (required)\n Message Properties (optional)\n Message Bodies (optional)\nYou can tell the input plugin which parts should be included in the event produced by Logstash\n\nInclude JMS Message Header Field values in the event",
        "base": false,
        "name": "include_header",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Include JMS Message Properties Field values in the event",
        "base": false,
        "name": "include_properties",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Polling interval in seconds.\nThis is the time sleeping between asks to a consumed Queue.\nThis parameter has non influence in the case of a subcribed Topic.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "Mandatory if jndi lookup is being used,\ncontains details on how to connect to JNDI server",
        "base": false,
        "name": "jndi_context",
        "validate": "hash"
      },
      {
        "comments": "Name of JNDI entry at which the Factory can be found",
        "base": false,
        "name": "jndi_name",
        "validate": "string"
      },
      {
        "comments": "Password to use when connecting to the JMS provider",
        "base": false,
        "name": "password",
        "validate": "string"
      },
      {
        "comments": "If pub-sub (topic) style should be used.",
        "base": false,
        "name": "pub_sub",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "An optional array of Jar file names to load for the specified\nJMS provider. By using this option it is not necessary\nto put all the JMS Provider specific jar files into the\njava CLASSPATH prior to starting Logstash.",
        "base": false,
        "name": "require_jars",
        "validate": "array"
      },
      {
        "comments": "Choose an implementation of the run block. Value can be either consumer, async or thread",
        "base": false,
        "name": "runner",
        "validate": [
          "consumer",
          "async",
          "thread"
        ],
        "default": "consumer"
      },
      {
        "comments": "Set the selector to use to get messages off the queue or topic",
        "base": false,
        "name": "selector",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Initial connection timeout in seconds.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Convert the JMSTimestamp header field to the @timestamp value of the event",
        "base": false,
        "name": "use_jms_timestamp",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Username to connect to JMS provider with",
        "base": false,
        "name": "username",
        "validate": "string"
      },
      {
        "comments": "Yaml config file",
        "base": false,
        "name": "yaml_file",
        "validate": "string"
      },
      {
        "comments": "Yaml config file section name\nFor some known examples, see: [Example jms.yml](https://github.com/reidmorrison/jruby-jms/blob/master/examples/jms.yml)",
        "base": false,
        "name": "yaml_section",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-example/master/lib/logstash/filters/example.rb",
    "name": "example",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Replace the message with this value.",
        "base": false,
        "name": "message",
        "validate": "string",
        "default": "Hello World!"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-example/master/lib/logstash/inputs/example.rb",
    "name": "example",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Set how frequently messages should be sent.\n\nThe default, `1`, means send a message every second.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The message string to use in the event.",
        "base": false,
        "name": "message",
        "validate": "string",
        "default": "Hello World!"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-oui/master/lib/logstash/filters/oui.rb",
    "name": "oui",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The source field to parse",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The target field to place all the data",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "oui"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-metricize/master/lib/logstash/filters/metricize.rb",
    "name": "metricize",
    "type": "filter",
    "params": [
      {
        "comments": "A new matrics event will be created for each metric field in this list.\nAll fields in this list will be removed from generated events.",
        "base": false,
        "name": "metrics",
        "validate": "array",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Flag indicating whether the original event should be dropped or not.",
        "base": false,
        "name": "drop_original_event",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Name of the field the metric name will be written to.",
        "base": false,
        "name": "metric_field_name",
        "validate": "string",
        "default": "metric"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Name of the field the metric value will be written to.",
        "base": false,
        "name": "value_field_name",
        "validate": "string",
        "default": "value"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-heartbeat/master/lib/logstash/inputs/heartbeat.rb",
    "name": "heartbeat",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "How many times to iterate.\nThis is typically used only for testing purposes.",
        "base": false,
        "name": "count",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "Set how frequently messages should be sent.\n\nThe default, `60`, means send a message every 60 seconds.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The message string to use in the event.\n\nIf you set this to `epoch` then this plugin will use the current\ntimestamp in unix timestamp (which is by definition, UTC).  It will\noutput this value into a field called `clock`\n\nIf you set this to `sequence` then this plugin will send a sequence of\nnumbers beginning at 0 and incrementing each interval.  It will\noutput this value into a field called `clock`\n\nOtherwise, this value will be used verbatim as the event message. It\nwill output this value into a field called `message`",
        "base": false,
        "name": "message",
        "validate": "string",
        "default": "ok"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-rackspace/master/lib/logstash/outputs/rackspace.rb",
    "name": "rackspace",
    "type": "output",
    "params": [
      {
        "comments": "Rackspace Cloud API Key",
        "base": false,
        "name": "api_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Rackspace Cloud Username",
        "base": false,
        "name": "username",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Rackspace Queue Name",
        "base": false,
        "name": "queue",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Rackspace region\nord, dfw, lon, syd, etc",
        "base": false,
        "name": "region",
        "validate": "string",
        "default": "dfw"
      },
      {
        "comments": "time for item to live in queue",
        "base": false,
        "name": "ttl",
        "validate": "number",
        "default": "360"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-jmx-pipe/master/lib/logstash/inputs/jmx.rb",
    "name": "jmx",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-mongodb/master/lib/logstash/inputs/mongodb.rb",
    "name": "mongodb",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-emoji/master/lib/logstash/filters/emoji.rb",
    "name": "emoji",
    "type": "filter",
    "params": [
      {
        "comments": "The name of the logstash event field containing the value to be compared for\na match by the emoji filter (e.g. `severity`).\n\nIf this field is an array, only the first value will be used.",
        "base": false,
        "name": "field",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "In case no match is found in the event, this will add a default emoji, which\nwill always populate `target`, if the match failed.\n\nFor example, if we have configured `fallback => \"``\"`, using this\ndictionary:\n[source,ruby]\n    foo: \n\nThen, if logstash received an event with the field `foo` set to , the\ntarget field would be set to . However, if logstash received an event with\n`foo` set to `nope`, then the target field would still be populated, but\nwith the value of .\nThis configuration can be dynamic and include parts of the event using the\n`%{field}` syntax.",
        "base": false,
        "name": "fallback",
        "validate": "string"
      },
      {
        "comments": "If the target field already exists, this configuration item specifies\nwhether the filter should skip being rewritten as an emoji (default) or\noverwrite the target field value with the emoji value.",
        "base": false,
        "name": "override",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "`sev_alert` selects the emoji/unicode character for Alert severity",
        "base": false,
        "name": "sev_alert",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "`sev_critical` selects the emoji/unicode character for Critical severity",
        "base": false,
        "name": "sev_critical",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "`sev_debug` selects the emoji/unicode character for Debug severity",
        "base": false,
        "name": "sev_debug",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "`sev_emergency` selects the emoji/unicode character for Emergency severity",
        "base": false,
        "name": "sev_emergency",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "`sev_error` selects the emoji/unicode character for Error severity",
        "base": false,
        "name": "sev_error",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "`sev_info` selects the emoji/unicode character for Informational severity",
        "base": false,
        "name": "sev_info",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "`sev_notice` selects the emoji/unicode character for Notice severity",
        "base": false,
        "name": "sev_notice",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "`sev_warning` selects the emoji/unicode character for Warning severity",
        "base": false,
        "name": "sev_warning",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The target field you wish to populate with the emoji. The default\nis a field named `emoji`. Set this to the same value as the source (`field`)\nif you want to do a substitution, in this case filter will allways succeed.\nThis will overwrite the old value of the source field!",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "emoji"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-netflow/master/lib/logstash/inputs/netflow.rb",
    "name": "netflow",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-math/master/lib/logstash/filters/math.rb",
    "name": "math",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-jms/master/lib/logstash/outputs/jms.rb",
    "name": "jms",
    "type": "output",
    "params": [
      {
        "comments": "Url to use when connecting to the JMS provider",
        "base": false,
        "name": "broker_url",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Name of delivery mode to use\nOptions are \"persistent\" and \"non_persistent\" if not defined nothing will be passed.",
        "base": false,
        "name": "delivery_mode",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Name of the destination queue or topic to use.\nMandatory",
        "base": false,
        "name": "destination",
        "validate": "string"
      },
      {
        "comments": "Name of JMS Provider Factory class",
        "base": false,
        "name": "factory",
        "validate": "string"
      },
      {
        "comments": "Mandatory if jndi lookup is being used,\ncontains details on how to connect to JNDI server",
        "base": false,
        "name": "jndi_context",
        "validate": "hash"
      },
      {
        "comments": "Name of JNDI entry at which the Factory can be found",
        "base": false,
        "name": "jndi_name",
        "validate": "string"
      },
      {
        "comments": "Password to use when connecting to the JMS provider",
        "base": false,
        "name": "password",
        "validate": "string"
      },
      {
        "comments": "If pub-sub (topic) style should be used or not.\nMandatory",
        "base": false,
        "name": "pub_sub",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "An optional array of Jar file names to load for the specified\nJMS provider. By using this option it is not necessary\nto put all the JMS Provider specific jar files into the\njava CLASSPATH prior to starting Logstash.",
        "base": false,
        "name": "require_jars",
        "validate": "array"
      },
      {
        "comments": "Username to connect to JMS provider with",
        "base": false,
        "name": "username",
        "validate": "string"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      },
      {
        "comments": "Yaml config file",
        "base": false,
        "name": "yaml_file",
        "validate": "string"
      },
      {
        "comments": "Yaml config file section name\nFor some known examples, see: [Example jms.yml](https://github.com/reidmorrison/jruby-jms/blob/master/examples/jms.yml)",
        "base": false,
        "name": "yaml_section",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-jdbc_static/master/lib/logstash/filters/jdbc_static.rb",
    "name": "jdbc_static",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-debug/master/lib/logstash/filters/debug.rb",
    "name": "debug",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-age/master/lib/logstash/filters/age.rb",
    "name": "age",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Define the target field for the event age, in seconds.",
        "base": false,
        "name": "target",
        "default": "[@metadata][age]",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-kinesis/master/lib/logstash/inputs/kinesis.rb",
    "name": "kinesis",
    "type": "input",
    "params": [
      {
        "comments": "The kinesis stream name.",
        "base": false,
        "name": "kinesis_stream_name",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The application name used for the dynamodb coordination table. Must be\nunique for this kinesis stream.",
        "base": false,
        "name": "application_name",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "How many seconds between worker checkpoints to dynamodb.",
        "base": false,
        "name": "checkpoint_interval_seconds",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Worker metric tracking. By default this is disabled, set it to \"cloudwatch\"\nto enable the cloudwatch integration in the Kinesis Client Library.",
        "base": false,
        "name": "metrics",
        "validate": "[nil, \"cloudwatch\"]",
        "default": "nil"
      },
      {
        "comments": "The AWS region for Kinesis, DynamoDB, and CloudWatch (if enabled)",
        "base": false,
        "name": "region",
        "validate": "string",
        "default": "us-east-1"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-jdbc_streaming/master/lib/logstash/filters/jdbc_streaming.rb",
    "name": "jdbc_streaming",
    "type": "filter",
    "params": [
      {
        "comments": "Statement to execute.\nTo use parameters, use named parameter syntax, for example \"SELECT * FROM MYTABLE WHERE ID = :id\"",
        "base": false,
        "name": "statement",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Define the target field to store the extracted result(s)\nField is overwritten if exists",
        "base": false,
        "name": "target",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The minimum number of seconds any entry should remain in the cache, defaults to 5 seconds\nA numeric value, you can use decimals for example `{ \"cache_expiration\" => 0.25 }`\nIf there are transient jdbc errors the cache will store empty results for a given\nparameter set and bypass the jbdc lookup, this merges the default_hash into the event, until\nthe cache entry expires, then the jdbc lookup will be tried again for the same parameters\nConversely, while the cache contains valid results any external problem that would cause\njdbc errors, will not be noticed for the cache_expiration period.",
        "base": false,
        "name": "cache_expiration",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The maximum number of cache entries are stored, defaults to 500 entries\nThe least recently used entry will be evicted",
        "base": false,
        "name": "cache_size",
        "validate": "number",
        "default": "500"
      },
      {
        "comments": "Define a default object to use when lookup fails to return a matching row.\nensure that the key names of this object match the columns from the statement",
        "base": false,
        "name": "default_hash",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Hash of query parameter, for example `{ \"id\" => \"id_field\" }`",
        "base": false,
        "name": "parameters",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Append values to the `tags` field if no record was found and default values were used",
        "base": false,
        "name": "tag_on_default_use",
        "validate": "array",
        "default": "[\"_jdbcstreamingdefaultsused\"]"
      },
      {
        "comments": "Append values to the `tags` field if sql error occured",
        "base": false,
        "name": "tag_on_failure",
        "validate": "array",
        "default": "[\"_jdbcstreamingfailure\"]"
      },
      {
        "comments": "Enable or disable caching, boolean true or false, defaults to true",
        "base": false,
        "name": "use_cache",
        "validate": "boolean",
        "default": "true"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-kubernetes_metadata/master/lib/logstash/filters/kubernetes_metadata.rb",
    "name": "kubernetes_metadata",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-google_pubsub/master/lib/logstash/inputs/google_pubsub.rb",
    "name": "google_pubsub",
    "type": "input",
    "params": [
      {
        "comments": "",
        "base": false,
        "name": "max_messages",
        "validate": "number",
        "required": true,
        "default": "5"
      },
      {
        "comments": "Google Cloud Project ID (name, not number)",
        "base": false,
        "name": "project_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "subscription",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Google Cloud Pub/Sub Topic and Subscription.\nNote that the topic must be created manually with Cloud Logging\npre-configured export to PubSub configured to use the defined topic.\nThe subscription will be created automatically by the plugin.",
        "base": false,
        "name": "topic",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "If logstash is running within Google Compute Engine, the plugin will use\nGCE's Application Default Credentials. Outside of GCE, you will need to\nspecify a Service Account JSON key file.",
        "base": false,
        "name": "json_key_file",
        "validate": "path",
        "required": false
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-newrelic/master/lib/logstash/outputs/newrelic.rb",
    "name": "newrelic",
    "type": "output",
    "params": [
      {
        "comments": "Your New Relic account ID. This is the 5 or 6-digit number found in the URL when you are logged into New Relic:\nhttps://rpm.newrelic.com/accounts/[account_id]/...",
        "base": false,
        "name": "account_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your Insights Insert Key. You will need to generate one if you haven't already, as described here:\nhttps://docs.newrelic.com/docs/insights/new-relic-insights/adding-querying-data/inserting-custom-events-insights-api#register",
        "base": false,
        "name": "insert_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Batch Processing - all optional\nThis plugin uses the New Relic Insights REST API to send data.\nTo make efficient REST API calls, we will buffer a certain number of events before flushing that out to Insights.",
        "base": false,
        "name": "batch",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "This setting controls how many events will be buffered before sending a batch of events.",
        "base": false,
        "name": "batch_events",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "This setting controls how long the output will wait before sending a batch of a events,\nshould the minimum specified in batch_events not be met yet.",
        "base": false,
        "name": "batch_timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The name for your event type. Use alphanumeric characters only.\nIf left out, your events will be stored under \"logstashEvent\".",
        "base": false,
        "name": "event_type",
        "validate": "string",
        "default": "LogstashEvent"
      },
      {
        "comments": "Should the log events be sent to Insights over https instead of plain http (typically yes).",
        "base": false,
        "name": "proto",
        "validate": "string",
        "default": "https"
      },
      {
        "comments": "Proxy info - all optional\nIf using a proxy, only proxy_host is required.",
        "base": false,
        "name": "proxy_host",
        "validate": "string"
      },
      {
        "comments": "Proxy_password should be left out if connecting to your proxy unauthenticated.",
        "base": false,
        "name": "proxy_password",
        "validate": "password",
        "default": ""
      },
      {
        "comments": "Proxy_port will default to port 80 if left out.",
        "base": false,
        "name": "proxy_port",
        "validate": "number",
        "default": "80"
      },
      {
        "comments": "Proxy_user should be left out if connecting to your proxy unauthenticated.",
        "base": false,
        "name": "proxy_user",
        "validate": "string"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-jdbc/master/lib/logstash/inputs/jdbc.rb",
    "name": "jdbc",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The character encoding of all columns, leave empty if the columns are already properly UTF-8\nencoded. Specific columns charsets using :columns_charset can override this setting.",
        "base": false,
        "name": "charset",
        "validate": "string"
      },
      {
        "comments": "Whether the previous run state should be preserved",
        "base": false,
        "name": "clean_run",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The character encoding for specific columns. This option will override the `:charset` option\nfor the specified columns.\n\nExample:\n[source,ruby]\n-------------------------------------------------------\ninput {\n  jdbc {\n    ...\n    columns_charset => { \"column0\" => \"ISO-8859-1\" }\n    ...\n  }\n}\n-------------------------------------------------------\nthis will only convert column0 that has ISO-8859-1 as an original encoding.",
        "base": false,
        "name": "columns_charset",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Path to file with last run time",
        "base": false,
        "name": "last_run_metadata_path",
        "validate": "string",
        "default": "\"#{ENV[\"HOME\"]}/.logstash_jdbc_last_run\""
      },
      {
        "comments": "Whether to force the lowercasing of identifier fields",
        "base": false,
        "name": "lowercase_column_names",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Hash of query parameter, for example `{ \"target_id\" => \"321\" }`",
        "base": false,
        "name": "parameters",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Whether to save state or not in last_run_metadata_path",
        "base": false,
        "name": "record_last_run",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Schedule of when to periodically run statement, in Cron format\nfor example: \"* * * * *\" (execute query every minute, on the minute)\n\nThere is no schedule by default. If no schedule is given, then the statement is run\nexactly once.",
        "base": false,
        "name": "schedule",
        "validate": "string"
      },
      {
        "comments": "Statement to execute\n\nTo use parameters, use named parameter syntax.\nFor example:\n\n[source, ruby]\n-----------------------------------------------\n\"SELECT * FROM MYTABLE WHERE id = :target_id\"\n-----------------------------------------------\n\nhere, \":target_id\" is a named parameter. You can configure named parameters\nwith the `parameters` setting.",
        "base": false,
        "name": "statement",
        "validate": "string"
      },
      {
        "comments": "Path of file containing statement to execute",
        "base": false,
        "name": "statement_filepath",
        "validate": "path"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "If tracking column value rather than timestamp, the column whose value is to be tracked",
        "base": false,
        "name": "tracking_column",
        "validate": "string"
      },
      {
        "comments": "Type of tracking column. Currently only \"numeric\" and \"timestamp\"",
        "base": false,
        "name": "tracking_column_type",
        "validate": [
          "numeric",
          "timestamp"
        ],
        "default": "numeric"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Use an incremental column value rather than a timestamp",
        "base": false,
        "name": "use_column_value",
        "validate": "boolean",
        "default": "false"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-slack/master/lib/logstash/outputs/slack.rb",
    "name": "slack",
    "type": "output",
    "params": [
      {
        "comments": "The incoming webhook URI needed to post a message",
        "base": false,
        "name": "url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Attachments array as described https://api.slack.com/docs/attachments",
        "base": false,
        "name": "attachments",
        "validate": "array"
      },
      {
        "comments": "The channel to post to",
        "base": false,
        "name": "channel",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The text to post in slack",
        "base": false,
        "name": "format",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "Emoji icon to use",
        "base": false,
        "name": "icon_emoji",
        "validate": "string"
      },
      {
        "comments": "Icon URL to use",
        "base": false,
        "name": "icon_url",
        "validate": "string"
      },
      {
        "comments": "The username to use for posting",
        "base": false,
        "name": "username",
        "validate": "string"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-log4j2/master/lib/logstash/inputs/log4j2.rb",
    "name": "log4j2",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-logentries/master/lib/logstash/outputs/logentries.rb",
    "name": "logentries",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-zeromq/master/lib/logstash/filters/zeromq.rb",
    "name": "zeromq",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "tag to add if zeromq timeout expires before getting back an answer.\nIf set to \"\" then no tag will be added.",
        "base": false,
        "name": "add_tag_on_timeout",
        "validate": "string",
        "default": "zeromqtimeout"
      },
      {
        "comments": "0mq socket address to connect or bind\nPlease note that inproc:// will not work with logstash\nas we use a context per thread\nBy default, filters connect",
        "base": false,
        "name": "address",
        "validate": "string",
        "default": "tcp//127.0.0.12121"
      },
      {
        "comments": "The field to send off-site for processing\nIf this is unset, the whole event will be sent",
        "base": false,
        "name": "field",
        "validate": "string"
      },
      {
        "comments": "0mq mode\nserver mode binds/listens\nclient mode connects",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "client"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "number of retries, used for both sending and receiving messages.\nfor sending, retries should return instantly.\nfor receiving, the total blocking time is up to retries X timeout,\nwhich by default is 3 X 500 = 1500ms",
        "base": false,
        "name": "retries",
        "validate": "number",
        "default": "3"
      },
      {
        "comments": "A sentinel value to signal the filter to cancel the event\nIf the peer returns the sentinel value, the event will be cancelled",
        "base": false,
        "name": "sentinel",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "0mq socket options\nThis exposes zmq_setsockopt\nfor advanced tuning\nsee http://api.zeromq.org/2-1:zmq-setsockopt for details\n\nThis is where you would set values like:\nZMQ::HWM - high water mark\nZMQ::IDENTITY - named queues\nZMQ::SWAP_SIZE - space for disk overflow\nZMQ::SUBSCRIBE - topic filters for pubsub\n\nexample: sockopt => [\"ZMQ::HWM\", 50, \"ZMQ::IDENTITY\", \"my_named_queue\"]",
        "base": false,
        "name": "sockopt",
        "validate": "hash"
      },
      {
        "comments": "timeout in milliseconds on which to wait for a reply.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "500"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-perfmon/master/lib/logstash/inputs/perfmon.rb",
    "name": "perfmon",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Sets which perfmon counters to collect",
        "base": false,
        "name": "counters",
        "validate": "array",
        "required": false,
        "default": "["
      },
      {
        "comments": "Identifies the server being monitored. Defaults to hostname, but can be overriden.\n[source,ruby]\n    input {\n      perfmon {\n        interval => 10\n        counters => [\"\\Processor(_Total)\\% Privileged Time\"],\n        host => \"webserver1\"\n      }\n    }",
        "base": false,
        "name": "host",
        "required": false,
        "default": "Socket.gethostname"
      },
      {
        "comments": "Sets the frequency, in seconds, at which to collect perfmon metrics",
        "base": false,
        "name": "interval",
        "validate": "number",
        "required": false,
        "default": "10"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-neo4j/master/lib/logstash/inputs/neo4j.rb",
    "name": "neo4j",
    "type": "input",
    "params": [
      {
        "comments": "The path within your file system where the neo4j database is located",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Cypher query used to retrieve data from the neo4j database, this statement\nshould looks like something like this:\n\nMATCH (p:`Person`)-->(m:`Movie`) WHERE m.released = 2005 RETURN *\n",
        "base": false,
        "name": "query",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Schedule of when to periodically run statement, in Cron format\nfor example: \"* * * * *\" (execute query every minute, on the minute).\nIf this variable is not specified then this input will run only once",
        "base": false,
        "name": "schedule",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-unique/master/lib/logstash/filters/unique.rb",
    "name": "unique",
    "type": "filter",
    "params": [
      {
        "comments": "The fields on which to run the unique filter.",
        "base": false,
        "name": "fields",
        "validate": "array",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-http_poller/master/lib/logstash/inputs/http_poller.rb",
    "name": "http_poller",
    "type": "input",
    "params": [
      {
        "comments": "A Hash of urls in this format : `\"name\" => \"url\"`.\nThe name and the url will be passed in the outputed event",
        "base": false,
        "name": "urls",
        "validate": "hash",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "How often (in seconds) the urls will be called\nDEPRECATED. Use 'schedule' option instead.\nIf both interval and schedule options are specified, interval\noption takes higher precedence",
        "base": false,
        "name": "interval",
        "validate": "number",
        "deprecated": true
      },
      {
        "comments": "If you'd like to work with the request/response metadata.\nSet this value to the name of the field you'd like to store a nested\nhash of metadata.",
        "base": false,
        "name": "metadata_target",
        "validate": "string",
        "default": "@metadata"
      },
      {
        "comments": "Schedule of when to periodically poll from the urls\nFormat: A hash with\n  + key: \"cron\" | \"every\" | \"in\" | \"at\"\n  + value: string\nExamples:\n  a) { \"every\" => \"1h\" }\n  b) { \"cron\" => \"* * * * * UTC\" }\nSee: rufus/scheduler for details about different schedule options and value string format",
        "base": false,
        "name": "schedule",
        "validate": "hash"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Define the target field for placing the received data. If this setting is omitted, the data will be stored at the root (top level) of the event.",
        "base": false,
        "name": "target",
        "validate": "string"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-elasticsearch_java/master/lib/logstash/outputs/elasticsearch_java.rb",
    "name": "elasticsearch_java",
    "type": "output",
    "params": [
      {
        "comments": "The name/address of the host to bind to for Elasticsearch clustering. Equivalent to the Elasticsearch option 'network.host'\noption.\nThis MUST be set for either protocol to work (node or transport)! The internal Elasticsearch node\nwill bind to this ip. This ip MUST be reachable by all nodes in the Elasticsearch cluster",
        "base": false,
        "name": "network_host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The Elasticsearch action to perform. Valid actions are:\n\n- index: indexes a document (an event from Logstash).\n- delete: deletes a document by id (An id is required for this action)\n- create: indexes a document, fails if a document by that id already exists in the index.\n- update: updates a document by id. Update has a special case where you can upsert -- update a\n  document if not already present. See the `upsert` option\n- create_unless_exists: create the document unless it already exists, in which case do nothing.\n\nFor more details on actions, check out the http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html[Elasticsearch bulk API documentation]",
        "base": false,
        "name": "action",
        "validate": "%w(index delete create update create_unless_exists)",
        "default": "index"
      },
      {
        "comments": "The name of your cluster if you set it on the Elasticsearch side. Useful\nfor discovery when using `node` or `transport` protocols.\nBy default, it looks for a cluster named 'elasticsearch'.\nEquivalent to the Elasticsearch option 'cluster.name'",
        "base": false,
        "name": "cluster",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "This setting no longer does anything. It exists to keep config validation\nfrom failing. It will be removed in future versions.",
        "base": false,
        "name": "max_inflight_requests",
        "validate": "number",
        "default": 50,
        "deprecated": true
      },
      {
        "comments": "The node name Elasticsearch will use when joining a cluster.\n\nBy default, this is generated internally by the ES client.",
        "base": false,
        "name": "node_name",
        "validate": "string"
      },
      {
        "comments": "Choose the protocol used to talk to Elasticsearch.\n\nThe 'node' protocol (default) will connect to the cluster as a normal Elasticsearch\nnode (but will not store data). If you use the `node` protocol, you must permit\nbidirectional communication on the port 9300 (or whichever port you have\nconfigured).\n\nIf you do not specify the `host` parameter, it will use  multicast for http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html[Elasticsearch discovery].  While this may work in a test/dev environment where multicast is enabled in\nElasticsearch, we strongly recommend http://www.elastic.co/guide/en/elasticsearch/guide/current/important-configuration-changes.html#unicast[using unicast]\nin Elasticsearch.  To connect to an Elasticsearch cluster with unicast,\nyou must include the `host` parameter (see relevant section above).\n\nThe 'transport' protocol will connect to the host you specify and will\nnot show up as a 'node' in the Elasticsearch cluster. This is useful\nin situations where you cannot permit connections outbound from the\nElasticsearch cluster to this Logstash server.\n\nAll protocols will use bulk requests when talking to Elasticsearch.",
        "base": false,
        "name": "protocol",
        "validate": [
          "node",
          "transport"
        ],
        "default": "transport"
      },
      {
        "comments": "Enable cluster sniffing (transport only).\nAsks host for the list of all cluster nodes and adds them to the hosts list\nEquivalent to the Elasticsearch option 'client.transport.sniff'",
        "base": false,
        "name": "sniffing",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "This sets the local port to bind to. Equivalent to the Elasticsrearch option 'transport.tcp.port'",
        "base": false,
        "name": "transport_tcp_port",
        "validate": "number"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-salesforce/master/lib/logstash/inputs/salesforce.rb",
    "name": "salesforce",
    "type": "input",
    "params": [
      {
        "comments": "Consumer Key for authentication. You must set up a new SFDC\nconnected app with oath to use this output. More information\ncan be found here:\nhttps://help.salesforce.com/apex/HTViewHelpDoc?id=connected_app_create.htm",
        "base": false,
        "name": "client_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Consumer Secret from your oauth enabled connected app",
        "base": false,
        "name": "client_secret",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The password used to login to sfdc",
        "base": false,
        "name": "password",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The security token for this account. For more information about\ngenerting a security token, see:\nhttps://help.salesforce.com/apex/HTViewHelpDoc?id=user_security_token.htm",
        "base": false,
        "name": "security_token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The name of the salesforce object you are creating or updating",
        "base": false,
        "name": "sfdc_object_name",
        "validate": "string",
        "required": true
      },
      {
        "comments": "A valid salesforce user name, usually your email address.\nUsed for authentication and will be the user all objects\nare created or modified by",
        "base": false,
        "name": "username",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "By default, this uses the default Restforce API version.\nTo override this, set this to something like \"32.0\" for example",
        "base": false,
        "name": "api_version",
        "validate": "string",
        "required": false
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "These are the field names to return in the Salesforce query\nIf this is empty, all fields are returned.",
        "base": false,
        "name": "sfdc_fields",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "These options will be added to the WHERE clause in the\nSOQL statement. Additional fields can be filtered on by\nadding field1 = value1 AND field2 = value2 AND...",
        "base": false,
        "name": "sfdc_filters",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Setting this to true will convert SFDC's NamedFields__c to named_fields__c",
        "base": false,
        "name": "to_underscores",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Set this to true to connect to a sandbox sfdc instance\nlogging in through test.salesforce.com",
        "base": false,
        "name": "use_test_sandbox",
        "validate": "boolean",
        "default": "false"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-webhdfs/master/lib/logstash/outputs/webhdfs.rb",
    "name": "webhdfs",
    "type": "output",
    "params": [
      {
        "comments": "The server name for webhdfs/httpfs connections.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The path to the file to write to. Event fields can be used here,\nas well as date fields in the joda time format, e.g.:\n`/user/logstash/dt=%{+YYYY-MM-dd}/%{@source_host}-%{+HH}.log`",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The Username for webhdfs.",
        "base": false,
        "name": "user",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Compress output. One of ['none', 'snappy', 'gzip']",
        "base": false,
        "name": "compression",
        "validate": [
          "none",
          "snappy",
          "gzip"
        ],
        "default": "none"
      },
      {
        "comments": "Sending data to webhdfs if event count is above, even if `store_interval_in_secs` is not reached.",
        "base": false,
        "name": "flush_size",
        "validate": "number",
        "default": "500"
      },
      {
        "comments": "Sending data to webhdfs in x seconds intervals.",
        "base": false,
        "name": "idle_flush_time",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Set kerberos keytab file. Note that the gssapi library needs to be available to use this.",
        "base": false,
        "name": "kerberos_keytab",
        "validate": "string"
      },
      {
        "comments": "WebHdfs open timeout, default 30s.",
        "base": false,
        "name": "open_timeout",
        "validate": "number",
        "default": "30"
      },
      {
        "comments": "The server port for webhdfs/httpfs connections.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "50070"
      },
      {
        "comments": "The WebHdfs read timeout, default 30s.",
        "base": false,
        "name": "read_timeout",
        "validate": "number",
        "default": "30"
      },
      {
        "comments": "How long should we wait between retries.",
        "base": false,
        "name": "retry_interval",
        "validate": "number",
        "default": "0.5"
      },
      {
        "comments": "Retry some known webhdfs errors. These may be caused by race conditions when appending to same file, etc.",
        "base": false,
        "name": "retry_known_errors",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "How many times should we retry. If retry_times is exceeded, an error will be logged and the event will be discarded.",
        "base": false,
        "name": "retry_times",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Avoid appending to same file in multiple threads.\nThis solves some problems with multiple logstash output threads and locked file leases in webhdfs.\nIf this option is set to true, %{[@metadata][thread_id]} needs to be used in path config settting.",
        "base": false,
        "name": "single_file_per_thread",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Set snappy chunksize. Only neccessary for stream format. Defaults to 32k. Max is 65536\n@see http://code.google.com/p/snappy/source/browse/trunk/framing_format.txt",
        "base": false,
        "name": "snappy_bufsize",
        "validate": "number",
        "default": "32768"
      },
      {
        "comments": "Set snappy format. One of \"stream\", \"file\". Set to stream to be hive compatible.",
        "base": false,
        "name": "snappy_format",
        "validate": [
          "stream",
          "file"
        ],
        "default": "stream"
      },
      {
        "comments": "Set ssl cert file.",
        "base": false,
        "name": "ssl_cert",
        "validate": "string"
      },
      {
        "comments": "Set ssl key file.",
        "base": false,
        "name": "ssl_key",
        "validate": "string"
      },
      {
        "comments": "Standby namenode for ha hdfs.",
        "base": false,
        "name": "standby_host",
        "validate": "string",
        "default": "false"
      },
      {
        "comments": "Standby namenode port for ha hdfs.",
        "base": false,
        "name": "standby_port",
        "validate": "number",
        "default": "50070"
      },
      {
        "comments": "Use httpfs mode if set to true, else webhdfs.",
        "base": false,
        "name": "use_httpfs",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Set kerberos authentication.",
        "base": false,
        "name": "use_kerberos_auth",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Set ssl authentication. Note that the openssl library needs to be available to use this.",
        "base": false,
        "name": "use_ssl_auth",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-fluentd/master/lib/logstash/inputs/fluentd.rb",
    "name": "fluentd",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-http/master/lib/logstash/inputs/http.rb",
    "name": "http",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Apply specific codecs for specific content types.\nThe default codec will be applied only after this list is checked\nand no codec for the request's content-type is found",
        "base": false,
        "name": "additional_codecs",
        "validate": "hash",
        "default": "{ \"application/json\" => \"json\" }"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The host or ip to bind",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The JKS keystore to validate the client's certificates",
        "base": false,
        "name": "keystore",
        "validate": "path"
      },
      {
        "comments": "Set the truststore password",
        "base": false,
        "name": "keystore_password",
        "validate": "password"
      },
      {
        "comments": "Password for basic authorization",
        "base": false,
        "name": "password",
        "validate": "password",
        "required": false
      },
      {
        "comments": "The TCP port to bind to",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8080"
      },
      {
        "comments": "specify a custom set of response headers",
        "base": false,
        "name": "response_headers",
        "validate": "hash",
        "default": "{ \"Content-Type\" => \"text/plain\" }"
      },
      {
        "comments": "SSL Configurations\n\nEnable SSL",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Maximum number of threads to use",
        "base": false,
        "name": "threads",
        "validate": "number",
        "default": "4"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Username for basic authorization",
        "base": false,
        "name": "user",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Set the client certificate verification method. Valid methods: none, peer, force_peer",
        "base": false,
        "name": "verify_mode",
        "validate": [
          "none",
          "peer",
          "force_peer"
        ],
        "default": "none"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-bytesize/master/lib/logstash/filters/bytesize.rb",
    "name": "bytesize",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-dynamodb/master/lib/logstash/inputs/dynamodb.rb",
    "name": "dynamodb",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-lookup/master/lib/logstash/filters/lookup.rb",
    "name": "lookup",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-de_dot/master/lib/logstash/filters/de_dot.rb",
    "name": "de_dot",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The `fields` array should contain a list of known fields to act on.\nIf undefined, all top-level fields will be checked.  Sub-fields must be\nmanually specified in the array.  For example: `[\"field.suffix\",\"[foo][bar.suffix]\"]`\nwill result in \"field_suffix\" and nested or sub field [\"foo\"][\"bar_suffix\"]\n\nWARNING: This is an expensive operation.\n",
        "base": false,
        "name": "fields",
        "validate": "array"
      },
      {
        "comments": "If `nested` is _true_, then create sub-fields instead of replacing dots with\na different separator.",
        "base": false,
        "name": "nested",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Replace dots with this value.",
        "base": false,
        "name": "separator",
        "validate": "string",
        "default": "_"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-beats/master/lib/logstash/inputs/beats.rb",
    "name": "beats",
    "type": "input",
    "params": [
      {
        "comments": "The port to listen on.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The list of ciphers suite to use, listed by priorities.",
        "base": false,
        "name": "cipher_suites",
        "validate": "array",
        "default": "org.logstash.netty.SslSimpleBuilderDEFAULT_CIPHERS"
      },
      {
        "comments": "Close Idle clients after X seconds of inactivity.",
        "base": false,
        "name": "client_inactivity_timeout",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of seconds before we raise a timeout.\nThis option is useful to control how much time to wait if something is blocking the pipeline.",
        "base": false,
        "name": "congestion_threshold",
        "validate": "number",
        "default": 5,
        "deprecated": "This option is now deprecated since congestion control is done automatically"
      },
      {
        "comments": "The IP address to listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "",
        "base": false,
        "name": "include_codec_tag",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Events are by default sent in plain text. You can\nenable encryption by setting `ssl` to true and configuring\nthe `ssl_certificate` and `ssl_key` options.",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "SSL certificate to use.",
        "base": false,
        "name": "ssl_certificate",
        "validate": "path"
      },
      {
        "comments": "Validate client certificates against these authorities.\nYou can define multiple files or paths. All the certificates will\nbe read and added to the trust store. You need to configure the `ssl_verify_mode`\nto `peer` or `force_peer` to enable the verification.\n",
        "base": false,
        "name": "ssl_certificate_authorities",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Time in milliseconds for an incomplete ssl handshake to timeout",
        "base": false,
        "name": "ssl_handshake_timeout",
        "validate": "number",
        "default": "10000"
      },
      {
        "comments": "SSL key to use.\nNOTE: This key need to be in the PKCS8 format, you can convert it with https://www.openssl.org/docs/man1.1.0/apps/pkcs8.html[OpenSSL]\nfor more information.",
        "base": false,
        "name": "ssl_key",
        "validate": "path"
      },
      {
        "comments": "SSL key passphrase to use.",
        "base": false,
        "name": "ssl_key_passphrase",
        "validate": "password"
      },
      {
        "comments": "By default the server doesn't do any client verification.\n\n`peer` will make the server ask the client to provide a certificate.\nIf the client provides a certificate, it will be validated.\n\n`force_peer` will make the server ask the client to provide a certificate.\nIf the client doesn't provide a certificate, the connection will be closed.\n\nThis option needs to be used with `ssl_certificate_authorities` and a defined list of CAs.",
        "base": false,
        "name": "ssl_verify_mode",
        "validate": [
          "none",
          "peer",
          "force_peer"
        ],
        "default": "none"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "This is the default field to which the specified codec will be applied.",
        "base": false,
        "name": "target_field_for_codec",
        "validate": "string",
        "default": "message",
        "deprecated": "This option is now deprecated, the plugin is now compatible with Filebeat and Logstash-Forwarder"
      },
      {
        "comments": "The maximum TLS version allowed for the encrypted connections. The value must be the one of the following:\n1.0 for TLS 1.0, 1.1 for TLS 1.1, 1.2 for TLS 1.2",
        "base": false,
        "name": "tls_max_version",
        "validate": "number",
        "default": "TLS.max.version"
      },
      {
        "comments": "The minimum TLS version allowed for the encrypted connections. The value must be one of the following:\n1.0 for TLS 1.0, 1.1 for TLS 1.1, 1.2 for TLS 1.2",
        "base": false,
        "name": "tls_min_version",
        "validate": "number",
        "default": "TLS.min.version"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-rados/master/lib/logstash/outputs/rados.rb",
    "name": "rados",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-firehose/master/lib/logstash/outputs/firehose.rb",
    "name": "firehose",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-aggregate/master/lib/logstash/filters/aggregate.rb",
    "name": "aggregate",
    "type": "filter",
    "params": [
      {
        "comments": "The code to execute to update map, using current event.\n\nOr on the contrary, the code to execute to update event, using current map.\n\nYou will have a 'map' variable and an 'event' variable available (that is the event itself).\n\nExample:\n[source,ruby]\n    filter {\n      aggregate {\n        code => \"map['sql_duration'] += event.get('duration')\"\n      }\n    }",
        "base": false,
        "name": "code",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The expression defining task ID to correlate logs.\n\nThis value must uniquely identify the task.\n\nExample:\n[source,ruby]\n    filter {\n      aggregate {\n        task_id => \"%{type}%{my_task_id}\"\n      }\n    }",
        "base": false,
        "name": "task_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The path to file where aggregate maps are stored when Logstash stops\nand are loaded from when Logstash starts.\n\nIf not defined, aggregate maps will not be stored at Logstash stop and will be lost.\nMust be defined in only one aggregate filter (as aggregate maps are global).\n\nExample:\n[source,ruby]\n    filter {\n      aggregate {\n        aggregate_maps_path => \"/path/to/.aggregate_maps\"\n      }\n    }",
        "base": false,
        "name": "aggregate_maps_path",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Tell the filter that task is ended, and therefore, to delete aggregate map after code execution.",
        "base": false,
        "name": "end_of_task",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Tell the filter what to do with aggregate map.\n\n`\"create\"`: create the map, and execute the code only if map wasn't created before\n\n`\"update\"`: doesn't create the map, and execute the code only if map was created before\n\n`\"create_or_update\"`: create the map if it wasn't created before, execute the code in all cases",
        "base": false,
        "name": "map_action",
        "validate": "string",
        "default": "create_or_update"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "When this option is enabled, each time a task timeout is detected, it pushes task aggregation map as a new Logstash event.\nThis enables to detect and process task timeouts in Logstash, but also to manage tasks that have no explicit end event.",
        "base": false,
        "name": "push_map_as_event_on_timeout",
        "validate": "boolean",
        "required": false,
        "default": "false"
      },
      {
        "comments": "When this option is enabled, each time aggregate plugin detects a new task id, it pushes previous aggregate map as a new Logstash event,\nand then creates a new empty map for the next task.\n\nWARNING: this option works fine only if tasks come one after the other. It means : all task1 events, then all task2 events, etc...",
        "base": false,
        "name": "push_previous_map_as_event",
        "validate": "boolean",
        "required": false,
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The amount of seconds after a task \"end event\" can be considered lost.\n\nWhen timeout occurs for a task, The task \"map\" is evicted.\n\nTimeout can be defined for each \"task_id\" pattern.\n\nIf no timeout is defined, default timeout will be applied : 1800 seconds.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "required": false
      },
      {
        "comments": "The code to execute to complete timeout generated event, when `'push_map_as_event_on_timeout'` or `'push_previous_map_as_event'` is set to true.\nThe code block will have access to the newly generated timeout event that is pre-populated with the aggregation map.\n\nIf `'timeout_task_id_field'` is set, the event is also populated with the task_id value\n\nExample:\n[source,ruby]\n    filter {\n      aggregate {\n        timeout_code => \"event.set('state', 'timeout')\"\n      }\n    }",
        "base": false,
        "name": "timeout_code",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Defines tags to add when a timeout event is generated and yield\n\nExample:\n[source,ruby]\n    filter {\n      aggregate {\n        timeout_tags => [\"aggregate_timeout']\n      }\n    }",
        "base": false,
        "name": "timeout_tags",
        "validate": "array",
        "required": false,
        "default": "[]"
      },
      {
        "comments": "This option indicates the timeout generated event's field for the \"task_id\" value.\nThe task id will then be set into the timeout event. This can help correlate which tasks have been timed out.\n\nFor example, with option `timeout_task_id_field => \"my_id\"` ,when timeout task id is `\"12345\"`, the generated timeout event will contain `'my_id' => '12345'`.\n\nBy default, if this option is not set, task id value won't be set into timeout generated event.",
        "base": false,
        "name": "timeout_task_id_field",
        "validate": "string",
        "required": false
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-google_cloud_storage/master/lib/logstash/outputs/google_cloud_storage.rb",
    "name": "google_cloud_storage",
    "type": "output",
    "params": [
      {
        "comments": "GCS bucket name, without \"gs://\" or any other prefix.",
        "base": false,
        "name": "bucket",
        "validate": "string",
        "required": true
      },
      {
        "comments": "GCS path to private key file.",
        "base": false,
        "name": "key_path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "GCS service account.",
        "base": false,
        "name": "service_account",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Time pattern for log file, defaults to hourly files.\nMust Time.strftime patterns: www.ruby-doc.org/core-2.0/Time.html#method-i-strftime",
        "base": false,
        "name": "date_pattern",
        "validate": "string",
        "default": "%Y-%m-%dT%H00"
      },
      {
        "comments": "Flush interval in seconds for flushing writes to log files. 0 will flush\non every message.",
        "base": false,
        "name": "flush_interval_secs",
        "validate": "number",
        "default": "2"
      },
      {
        "comments": "Gzip output stream when writing events to log files.",
        "base": false,
        "name": "gzip",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "GCS private key password.",
        "base": false,
        "name": "key_password",
        "validate": "string",
        "default": "notasecret"
      },
      {
        "comments": "Log file prefix. Log file will follow the format:\n<prefix>_hostname_date<.part?>.log",
        "base": false,
        "name": "log_file_prefix",
        "validate": "string",
        "default": "logstash_gcs"
      },
      {
        "comments": "Sets max file size in kbytes. 0 disable max file check.",
        "base": false,
        "name": "max_file_size_kbytes",
        "validate": "number",
        "default": "10000"
      },
      {
        "comments": "The event format you want to store in files. Defaults to plain text.",
        "base": false,
        "name": "output_format",
        "validate": [
          "json",
          "plain"
        ],
        "default": "plain"
      },
      {
        "comments": "Directory where temporary files are stored.\nDefaults to /tmp/logstash-gcs-<random-suffix>",
        "base": false,
        "name": "temp_directory",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Uploader interval when uploading new files to GCS. Adjust time based\non your time pattern (for example, for hourly files, this interval can be\naround one hour).",
        "base": false,
        "name": "uploader_interval_secs",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-meetup/master/lib/logstash/inputs/meetup.rb",
    "name": "meetup",
    "type": "input",
    "params": [
      {
        "comments": "Interval to run the command. Value is in minutes.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Meetup Key",
        "base": false,
        "name": "meetupkey",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Event Status'",
        "base": false,
        "name": "eventstatus",
        "validate": "string",
        "default": "upcoming,past"
      },
      {
        "comments": "The Group ID, multiple may be specified seperated by commas\nMust have one of `urlname`, `venueid`, `groupid`",
        "base": false,
        "name": "groupid",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "URLName - the URL name ie `ElasticSearch-Oklahoma-City`\nMust have one of urlname, venue_id, group_id",
        "base": false,
        "name": "urlname",
        "validate": "string"
      },
      {
        "comments": "The venue ID\nMust have one of `urlname`, `venue_id`, `group_id`",
        "base": false,
        "name": "venueid",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-monasca_log_api/master/lib/logstash/outputs/monasca_log_api.rb",
    "name": "monasca_log_api",
    "type": "output",
    "params": [
      {
        "comments": "",
        "base": false,
        "name": "keystone_api_insecure",
        "validate": "boolean",
        "required": "false,"
      },
      {
        "comments": "keystone configuration",
        "base": false,
        "name": "keystone_api_url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "monasca_log_api_insecure",
        "validate": "boolean",
        "required": "false,"
      },
      {
        "comments": "monasca-log-api configuration",
        "base": false,
        "name": "monasca_log_api_url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "password",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "project_domain_name",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "project_name",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "user_domain_name",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "username",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "",
        "base": false,
        "name": "delay",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "global dimensions",
        "base": false,
        "name": "dimensions",
        "validate": "array",
        "required": false
      },
      {
        "comments": "",
        "base": false,
        "name": "elapsed_time_sec",
        "validate": "number",
        "default": "30"
      },
      {
        "comments": "",
        "base": false,
        "name": "max_data_size_kb",
        "validate": "number",
        "default": "5120"
      },
      {
        "comments": "",
        "base": false,
        "name": "num_of_logs",
        "validate": "number",
        "default": "125"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-cloudfoundry/master/lib/logstash/filters/cloudfoundry.rb",
    "name": "cloudfoundry",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-dead_letter_queue/master/lib/logstash/inputs/dead_letter_queue.rb",
    "name": "dead_letter_queue",
    "type": "input",
    "params": [
      {
        "comments": "Path to the dead letter queue directory which was created by a Logstash instance.\nThis is the path from where \"dead\" events are read from and is typically configured\nin the original Logstash instance with the setting path.dead_letter_queue.",
        "base": false,
        "name": "path",
        "validate": "path",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Should this input commit offsets as it processes the events. `false` value is typically\nused when you want to iterate multiple times over the events in the dead letter queue, but don't want to\nsave state. This is when you are exploring the events in the dead letter queue.",
        "base": false,
        "name": "commit_offsets",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "ID of the pipeline whose events you want to read from.",
        "base": false,
        "name": "pipeline_id",
        "validate": "string",
        "default": "main"
      },
      {
        "comments": "Path of the sincedb database file (keeps track of the current position of dead letter queue) that\nwill be written to disk. The default will write sincedb files to `<path.data>/plugins/inputs/dead_letter_queue`\nNOTE: it must be a file path and not a directory path",
        "base": false,
        "name": "sincedb_path",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Timestamp in ISO8601 format from when you want to start processing the events from.\nFor example, 2017-04-04T23:40:37",
        "base": false,
        "name": "start_timestamp",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-stomp/master/lib/logstash/outputs/stomp.rb",
    "name": "stomp",
    "type": "output",
    "params": [
      {
        "comments": "The destination to read events from. Supports string expansion, meaning\n`%{foo}` values will expand to the field value.\n\nExample: \"/topic/logstash\"",
        "base": false,
        "name": "destination",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The address of the STOMP server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Enable debugging output?",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Custom headers to send with each message. Supports string expansion, meaning\n%{foo} values will expand to the field value.\n\nExample: headers => [\"amq-msg-type\", \"text\", \"host\", \"%{host}\"]",
        "base": false,
        "name": "headers",
        "validate": "hash"
      },
      {
        "comments": "The password to authenticate with.",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": ""
      },
      {
        "comments": "The port to connect to on your STOMP server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "61613"
      },
      {
        "comments": "The username to authenticate with.",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The vhost to use",
        "base": false,
        "name": "vhost",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-truncate/master/lib/logstash/filters/truncate.rb",
    "name": "truncate",
    "type": "filter",
    "params": [
      {
        "comments": "Fields over this length will be truncated to this length.\n\nTruncation happens from the end of the text (the start will be kept).\n\nAs an example, if you set `length_bytes => 10` and a field contains \"hello\nworld, how are you?\", then this field will be truncated and have this value:\n\"hello worl\"",
        "base": false,
        "name": "length_bytes",
        "validate": "number",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "A list of fieldrefs to truncate if they are too long.\n\nIf not specified, the default behavior will be to attempt truncation on all\nstrings in the event. This default behavior could be computationally\nexpensive, so if you know exactly which fields you wish to truncate, it is\nadvised that you be specific and configure the fields you want truncated.\n\nSpecial behaviors for non-string fields:\n\n* Numbers: No action\n* Array: this plugin will attempt truncation on all elements of that array.\n* Hash: truncate will try all values of the hash (recursively, if this hash\ncontains other hashes).",
        "base": false,
        "name": "fields",
        "validate": "string",
        "list": true
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-googleanalytics/master/lib/logstash/inputs/googleanalytics.rb",
    "name": "googleanalytics",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-hashid/master/lib/logstash/filters/hashid.rb",
    "name": "hashid",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Use the timestamp to generate an ID prefix",
        "base": false,
        "name": "add_timestamp_prefix",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "If full hash generated is not to be used, this parameter specifies how many bytes that should be used\nIf not specified, the full hash will be used",
        "base": false,
        "name": "hash_bytes_used",
        "validate": "number"
      },
      {
        "comments": "Encryption key to be used when generating cryptographic hashes",
        "base": false,
        "name": "key",
        "validate": "string",
        "default": "hashid"
      },
      {
        "comments": "Hash function to use",
        "base": false,
        "name": "method",
        "validate": [
          "SHA1",
          "SHA256",
          "SHA384",
          "SHA512",
          "MD5"
        ],
        "default": "MD5"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Source field(s) to base the hash calculation on",
        "base": false,
        "name": "source",
        "validate": "array",
        "default": "[\"message\"]"
      },
      {
        "comments": "Target field.\nWill overwrite current value of a field if it exists.",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "hashid"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-cloudwatch/master/lib/logstash/inputs/cloudwatch.rb",
    "name": "cloudwatch",
    "type": "input",
    "params": [
      {
        "comments": "Specify the filters to apply when fetching resources:\n\nThis needs to follow the AWS convention of specifiying filters.\nInstances: { 'instance-id' => 'i-12344321' }\nTags: { \"tag:Environment\" => \"Production\" }\nVolumes: { 'attachment.status' => 'attached' }\nEach namespace uniquely support certian dimensions. Please consult the documentation\nto ensure you're using valid filters.",
        "base": false,
        "name": "filters",
        "validate": "array",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Use this for namespaces that need to combine the dimensions like S3 and SNS.",
        "base": false,
        "name": "combined",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Set how frequently CloudWatch should be queried\n\nThe default, `900`, means check every 15 minutes. Setting this value too low\n(generally less than 300) results in no metrics being returned from CloudWatch.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "(60 * 15)"
      },
      {
        "comments": "Specify the metrics to fetch for the namespace. The defaults are AWS/EC2 specific. See http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/aws-namespaces.html\nfor the available metrics for other namespaces.",
        "base": false,
        "name": "metrics",
        "validate": "array",
        "default": "[\"CPUUtilization\",\"DiskReadOps\",\"DiskWriteOps\",\"NetworkIn\",\"NetworkOut\"]"
      },
      {
        "comments": "The service namespace of the metrics to fetch.\n\nThe default is for the EC2 service. See http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/aws-namespaces.html\nfor valid values.",
        "base": false,
        "name": "namespace",
        "validate": "string",
        "default": "AWS/EC2"
      },
      {
        "comments": "Set the granularity of the returned datapoints.\n\nMust be at least 60 seconds and in multiples of 60.",
        "base": false,
        "name": "period",
        "validate": "number",
        "default": "(60 * 5)"
      },
      {
        "comments": "Specify the statistics to fetch for each namespace",
        "base": false,
        "name": "statistics",
        "validate": "array",
        "default": "[\"SampleCount\",\"Average\",\"Minimum\",\"Maximum\",\"Sum\"]"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-neo4j/master/lib/logstash/outputs/neo4j.rb",
    "name": "neo4j",
    "type": "output",
    "params": [
      {
        "comments": "The path within your file system where the neo4j database is located",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-cidr/master/lib/logstash/filters/cidr.rb",
    "name": "cidr",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The IP address(es) to check with. Example:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"testnet\" ]\n        address => [ \"%{src_ip}\", \"%{dst_ip}\" ]\n        network => [ \"192.0.2.0/24\" ]\n      }\n    }",
        "base": false,
        "name": "address",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The IP network(s) to check against. Example:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"linklocal\" ]\n        address => [ \"%{clientip}\" ]\n        network => [ \"169.254.0.0/16\", \"fe80::/64\" ]\n      }\n    }",
        "base": false,
        "name": "network",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-punct/master/lib/logstash/filters/punct.rb",
    "name": "punct",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field reference to use for punctuation stripping",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The field to store the result.",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "punct"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-translate/master/lib/logstash/filters/translate.rb",
    "name": "translate",
    "type": "filter",
    "params": [
      {
        "comments": "The name of the logstash event field containing the value to be compared for a\nmatch by the translate filter (e.g. `message`, `host`, `response_code`).\n\nIf this field is an array, only the first value will be used.",
        "base": false,
        "name": "field",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The destination field you wish to populate with the translated code. The default\nis a field named `translation`. Set this to the same value as source if you want\nto do a substitution, in this case filter will allways succeed. This will clobber\nthe old value of the source field!",
        "base": false,
        "name": "destination",
        "validate": "string",
        "default": "translation"
      },
      {
        "comments": "The dictionary to use for translation, when specified in the logstash filter\nconfiguration item (i.e. do not use the `@dictionary_path` file).\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        dictionary => [ \"100\", \"Continue\",\n                        \"101\", \"Switching Protocols\",\n                        \"merci\", \"thank you\",\n                        \"old version\", \"new version\" ]\n      }\n    }\n\nNOTE: It is an error to specify both `dictionary` and `dictionary_path`.",
        "base": false,
        "name": "dictionary",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The full path of the external dictionary file. The format of the table\nshould be a standard YAML, JSON, or CSV. Make sure you specify any integer-based keys\nin quotes. For example, the YAML file should look something like this:\n[source,ruby]\n    \"100\": Continue\n    \"101\": Switching Protocols\n    merci: gracias\n    old version: new version\n\nNOTE: it is an error to specify both `dictionary` and `dictionary_path`.\n\nThe currently supported formats are YAML, JSON, and CSV. Format selection is\nbased on the file extension: `json` for JSON, `yaml` or `yml` for YAML, and\n`csv` for CSV. The JSON format only supports simple key/value, unnested\nobjects. The CSV format expects exactly two columns, with the first serving\nas the original text, and the second column as the replacement.",
        "base": false,
        "name": "dictionary_path",
        "validate": "path"
      },
      {
        "comments": "If `exact => false`, and logstash receives the same event, the destination field\nwill be also set to `bar`. However, if logstash receives an event with the `data` field\nset to `foofing`, the destination field will be set to `barfing`.\n\nSet both `exact => true` AND `regex => `true` if you would like to match using dictionary\nkeys as regular expressions. A large dictionary could be expensive to match in this case.",
        "base": false,
        "name": "exact",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "In case no translation occurs in the event (no matches), this will add a default\ntranslation string, which will always populate `field`, if the match failed.\n\nFor example, if we have configured `fallback => \"no match\"`, using this dictionary:\n[source,ruby]\n    foo: bar\n\nThen, if logstash received an event with the field `foo` set to `bar`, the destination\nfield would be set to `bar`. However, if logstash received an event with `foo` set to `nope`,\nthen the destination field would still be populated, but with the value of `no match`.\nThis configuration can be dynamic and include parts of the event using the `%{field}` syntax.",
        "base": false,
        "name": "fallback",
        "validate": "string"
      },
      {
        "comments": "If the destination (or target) field already exists, this configuration item specifies\nwhether the filter should skip translation (default) or overwrite the target field\nvalue with the new translation value.",
        "base": false,
        "name": "override",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "When using a dictionary file, this setting will indicate how frequently\n(in seconds) logstash will check the dictionary file for updates.",
        "base": false,
        "name": "refresh_interval",
        "validate": "number",
        "default": "300"
      },
      {
        "comments": "If you'd like to treat dictionary keys as regular expressions, set `exact => true`.\nNote: this is activated only when `exact => true`.",
        "base": false,
        "name": "regex",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-uuid/master/lib/logstash/filters/uuid.rb",
    "name": "uuid",
    "type": "filter",
    "params": [
      {
        "comments": "Select the name of the field where the generated UUID should be\nstored.\n\nExample:\n[source,ruby]\n    filter {\n      uuid {\n        target => \"uuid\"\n      }\n    }",
        "base": false,
        "name": "target",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If the value in the field currently (if any) should be overridden\nby the generated UUID. Defaults to `false` (i.e. if the field is\npresent, with ANY value, it won't be overridden)\n\nExample:\n[source,ruby]\n   filter {\n      uuid {\n        target    => \"uuid\"\n        overwrite => true\n      }\n   }",
        "base": false,
        "name": "overwrite",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-rss/master/lib/logstash/inputs/rss.rb",
    "name": "rss",
    "type": "input",
    "params": [
      {
        "comments": "Interval to run the command. Value is in seconds.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "required": true
      },
      {
        "comments": "RSS/Atom feed URL",
        "base": false,
        "name": "url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-clone/master/lib/logstash/filters/clone.rb",
    "name": "clone",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "A new clone will be created with the given type for each type in this list.",
        "base": false,
        "name": "clones",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-metrics/master/lib/logstash/filters/metrics.rb",
    "name": "metrics",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The clear interval, when all counter are reset.\n\nIf set to -1, the default value, the metrics will never be cleared.\nOtherwise, should be a multiple of 5s.",
        "base": false,
        "name": "clear_interval",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "The flush interval, when the metrics event is created. Must be a multiple of 5s.",
        "base": false,
        "name": "flush_interval",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Don't track events that have `@timestamp` older than some number of seconds.\n\nThis is useful if you want to only include events that are near real-time\nin your metrics.\n\nFor example, to only count events that are within 10 seconds of real-time, you\nwould do this:\n\n    filter {\n      metrics {\n        meter => [ \"hits\" ]\n        ignore_older_than => 10\n      }\n    }",
        "base": false,
        "name": "ignore_older_than",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "syntax: `meter => [ \"name of metric\", \"name of metric\" ]`",
        "base": false,
        "name": "meter",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The percentiles that should be measured and emitted for timer values.",
        "base": false,
        "name": "percentiles",
        "validate": "array",
        "default": "[1,5,10,90,95,99,100]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The rates that should be measured, in minutes.\nPossible values are 1, 5, and 15.",
        "base": false,
        "name": "rates",
        "validate": "array",
        "default": "[1,5,15]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "syntax: `timer => [ \"name of metric\", \"%{time_value}\" ]`",
        "base": false,
        "name": "timer",
        "validate": "hash",
        "default": "{}"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-eventlog/master/lib/logstash/inputs/eventlog.rb",
    "name": "eventlog",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "How frequently should tail check for new event logs in ms (default: 1 second)",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "Event Log Name\nSystem and Security may require that privileges are given to the user running logstash.\nsee more at: https://social.technet.microsoft.com/forums/windowsserver/en-US/d2f813db-6142-4b5b-8d86-253ebb740473/easy-way-to-read-security-log",
        "base": false,
        "name": "logfile",
        "validate": [
          "Application",
          "Security",
          "System"
        ],
        "default": "Application"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-geoip/master/lib/logstash/filters/geoip.rb",
    "name": "geoip",
    "type": "filter",
    "params": [
      {
        "comments": "The field containing the IP address or hostname to map via geoip. If\nthis field is an array, only the first value will be used.",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "GeoIP lookup is surprisingly expensive. This filter uses an cache to take advantage of the fact that\nIPs agents are often found adjacent to one another in log files and rarely have a random distribution.\nThe higher you set this the more likely an item is to be in the cache and the faster this filter will run.\nHowever, if you set this too high you can use more memory than desired.\nSince the Geoip API upgraded to v2, there is not any eviction policy so far, if cache is full, no more record can be added.\nExperiment with different values for this option to find the best performance for your dataset.\n\nThis MUST be set to a value > 0. There is really no reason to not want this behavior, the overhead is minimal\nand the speed gains are large.\n\nIt is important to note that this config value is global to the geoip_type. That is to say all instances of the geoip filter\nof the same geoip_type share the same cache. The last declared cache size will 'win'. The reason for this is that there would be no benefit\nto having multiple caches for different instances at different points in the pipeline, that would just increase the\nnumber of cache misses and waste memory.",
        "base": false,
        "name": "cache_size",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "The path to the GeoLite2 database file which Logstash should use. Only City database is supported by now.\n\nIf not specified, this will default to the GeoLite2 City database that ships\nwith Logstash.",
        "base": false,
        "name": "database",
        "validate": "path"
      },
      {
        "comments": "An array of geoip fields to be included in the event.\n\nPossible fields depend on the database type. By default, all geoip fields\nare included in the event.\n\nFor the built-in GeoLite2 City database, the following are available:\n`city_name`, `continent_code`, `country_code2`, `country_code3`, `country_name`,\n`dma_code`, `ip`, `latitude`, `longitude`, `postal_code`, `region_name` and `timezone`.",
        "base": false,
        "name": "fields",
        "validate": "array",
        "default": "[\"city_name\", \"continent_code\","
      },
      {
        "comments": "GeoIP lookup is surprisingly expensive. This filter uses an LRU cache to take advantage of the fact that\nIPs agents are often found adjacent to one another in log files and rarely have a random distribution.\nThe higher you set this the more likely an item is to be in the cache and the faster this filter will run.\nHowever, if you set this too high you can use more memory than desired.\n\nExperiment with different values for this option to find the best performance for your dataset.\n\nThis MUST be set to a value > 0. There is really no reason to not want this behavior, the overhead is minimal\nand the speed gains are large.\n\nIt is important to note that this config value is global to the geoip_type. That is to say all instances of the geoip filter\nof the same geoip_type share the same cache. The last declared cache size will 'win'. The reason for this is that there would be no benefit\nto having multiple caches for different instances at different points in the pipeline, that would just increase the\nnumber of cache misses and waste memory.",
        "base": false,
        "name": "lru_cache_size",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Tags the event on failure to look up geo information. This can be used in later analysis.",
        "base": false,
        "name": "tag_on_failure",
        "validate": "array",
        "default": "[\"_geoip_lookup_failure\"]"
      },
      {
        "comments": "Specify the field into which Logstash should store the geoip data.\nThis can be useful, for example, if you have `src_ip` and `dst_ip` fields and\nwould like the GeoIP information of both IPs.\n\nIf you save the data to a target field other than `geoip` and want to use the\n`geo_point` related functions in Elasticsearch, you need to alter the template\nprovided with the Elasticsearch output and configure the output to use the\nnew template.\n\nEven if you don't use the `geo_point` mapping, the `[target][location]` field\nis still valid GeoJSON.",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "geoip"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-i18n/master/lib/logstash/filters/i18n.rb",
    "name": "i18n",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Replaces non-ASCII characters with an ASCII approximation, or\nif none exists, a replacement character which defaults to `?`\n\nExample:\n[source,ruby]\n    filter {\n      i18n {\n         transliterate => [\"field1\", \"field2\"]\n      }\n    }",
        "base": false,
        "name": "transliterate",
        "validate": "array"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-ruby/master/lib/logstash/filters/ruby.rb",
    "name": "ruby",
    "type": "filter",
    "params": [
      {
        "comments": "The code to execute for every event.\nYou will have an `event` variable available that is the event itself. See the <<event-api,Event API>> for more information.",
        "base": false,
        "name": "code",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Any code to execute at logstash startup-time",
        "base": false,
        "name": "init",
        "validate": "string"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-range/master/lib/logstash/filters/range.rb",
    "name": "range",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Negate the range match logic, events should be outsize of the specified range to match.",
        "base": false,
        "name": "negate",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "An array of field, min, max, action tuples.\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        ranges => [ \"message\", 0, 10, \"tag:short\",\n                    \"message\", 11, 100, \"tag:medium\",\n                    \"message\", 101, 1000, \"tag:long\",\n                    \"message\", 1001, 1e1000, \"drop\",\n                    \"duration\", 0, 100, \"field:latency:fast\",\n                    \"duration\", 101, 200, \"field:latency:normal\",\n                    \"duration\", 201, 1000, \"field:latency:slow\",\n                    \"duration\", 1001, 1e1000, \"field:latency:outlier\",\n                    \"requests\", 0, 10, \"tag:too_few_%{host}_requests\" ]\n      }\n    }\n\nSupported actions are drop tag or field with specified value.\nAdded tag names and field names and field values can have `%{dynamic}` values.\n",
        "base": false,
        "name": "ranges",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-elasticsearch/master/lib/logstash/filters/elasticsearch.rb",
    "name": "elasticsearch",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "SSL Certificate Authority file",
        "base": false,
        "name": "ca_file",
        "validate": "path"
      },
      {
        "comments": "Whether results should be sorted or not",
        "base": false,
        "name": "enable_sort",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Array of fields to copy from old event (found via elasticsearch) into new event",
        "base": false,
        "name": "fields",
        "validate": "array",
        "default": "{}"
      },
      {
        "comments": "List of elasticsearch hosts to use for querying.",
        "base": false,
        "name": "hosts",
        "validate": "array",
        "default": "[\"localhost9200\"]"
      },
      {
        "comments": "Comma-delimited list of index names to search; use `_all` or empty string to perform the operation on all indices",
        "base": false,
        "name": "index",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Basic Auth - password",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Elasticsearch query string. Read the Elasticsearch query string documentation.\nfor more info at: https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl-query-string-query.html#query-string-syntax",
        "base": false,
        "name": "query",
        "validate": "string"
      },
      {
        "comments": "File path to elasticsearch query in DSL format. Read the Elasticsearch query documentation\nfor more info at: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html",
        "base": false,
        "name": "query_template",
        "validate": "string"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "How many results to return",
        "base": false,
        "name": "result_size",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Comma-delimited list of `<field>:<direction>` pairs that define the sort order",
        "base": false,
        "name": "sort",
        "validate": "string",
        "default": "@timestampdesc"
      },
      {
        "comments": "SSL",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Tags the event on failure to look up geo information. This can be used in later analysis.",
        "base": false,
        "name": "tag_on_failure",
        "validate": "array",
        "default": "[\"_elasticsearch_lookup_failure\"]"
      },
      {
        "comments": "Basic Auth - username",
        "base": false,
        "name": "user",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-xml/master/lib/logstash/filters/xml.rb",
    "name": "xml",
    "type": "filter",
    "params": [
      {
        "comments": "Config for xml to hash is:\n[source,ruby]\n    source => source_field\n\nFor example, if you have the whole XML document in your `message` field:\n[source,ruby]\n    filter {\n      xml {\n        source => \"message\"\n      }\n    }\n\nThe above would parse the XML from the `message` field.",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "By default the filter will force single elements to be arrays. Setting this to\nfalse will prevent storing single elements in arrays.",
        "base": false,
        "name": "force_array",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "By default the filter will expand attributes differently from content inside\nof tags. This option allows you to force text content and attributes to always\nparse to a hash value.",
        "base": false,
        "name": "force_content",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "By default only namespaces declarations on the root element are considered.\nThis allows to configure all namespace declarations to parse the XML document.\n\nExample:\n\n[source,ruby]\nfilter {\n  xml {\n    namespaces => {\n      \"xsl\" => \"http://www.w3.org/1999/XSL/Transform\"\n      \"xhtml\" => http://www.w3.org/1999/xhtml\"\n    }\n  }\n}\n",
        "base": false,
        "name": "namespaces",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Remove all namespaces from all nodes in the document.\nOf course, if the document had nodes with the same names but different namespaces, they will now be ambiguous.",
        "base": false,
        "name": "remove_namespaces",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "By default the filter will store the whole parsed XML in the destination\nfield as described above. Setting this to false will prevent that.",
        "base": false,
        "name": "store_xml",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "By default, output nothing if the element is empty.\nIf set to `false`, empty element will result in an empty hash object.",
        "base": false,
        "name": "suppress_empty",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Define target for placing the data\n\nFor example if you want the data to be put in the `doc` field:\n[source,ruby]\n    filter {\n      xml {\n        target => \"doc\"\n      }\n    }\n\nXML in the value of the source field will be expanded into a\ndatastructure in the `target` field.\nNote: if the `target` field already exists, it will be overridden.\nRequired if `store_xml` is true (which is the default).",
        "base": false,
        "name": "target",
        "validate": "string"
      },
      {
        "comments": "xpath will additionally select string values (non-strings will be\nconverted to strings with Ruby's `to_s` function) from parsed XML\n(using each source field defined using the method above) and place\nthose values in the destination fields. Configuration:\n[source,ruby]\nxpath => [ \"xpath-syntax\", \"destination-field\" ]\n\nValues returned by XPath parsing from `xpath-syntax` will be put in the\ndestination field. Multiple values returned will be pushed onto the\ndestination field as an array. As such, multiple matches across\nmultiple source fields will produce duplicate entries in the field.\n\nMore on XPath: http://www.w3schools.com/xml/xml_xpath.asp\n\nThe XPath functions are particularly powerful:\nhttp://www.w3schools.com/xsl/xsl_functions.asp\n",
        "base": false,
        "name": "xpath",
        "validate": "hash",
        "default": "{}"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-throttle/master/lib/logstash/filters/throttle.rb",
    "name": "throttle",
    "type": "filter",
    "params": [
      {
        "comments": "The key used to identify events.  Events with the same key are grouped together.\nField substitutions are allowed, so you can combine multiple fields.",
        "base": false,
        "name": "key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Events greater than this count will be throttled.  Setting this value to -1, the\ndefault, will cause no events to be throttled based on the upper bound.",
        "base": false,
        "name": "after_count",
        "validate": "number",
        "default": "-1",
        "required": false
      },
      {
        "comments": "Events less than this count will be throttled.  Setting this value to -1, the\ndefault, will cause no events to be throttled based on the lower bound.",
        "base": false,
        "name": "before_count",
        "validate": "number",
        "default": "-1",
        "required": false
      },
      {
        "comments": "The maximum age of a timeslot.  Higher values allow better tracking of an asynchronous\nflow of events, but require more memory.  As a rule of thumb you should set this value\nto at least twice the period.  Or set this value to period + maximum time offset\nbetween unordered events with the same key.  Values below the specified period give\nunexpected results if unordered events are processed simultaneously.",
        "base": false,
        "name": "max_age",
        "validate": "number",
        "default": "3600",
        "required": false
      },
      {
        "comments": "The maximum number of counters to store before decreasing the maximum age of a timeslot.\nSetting this value to -1 will prevent an upper bound with no constraint on the\nnumber of counters.  This configuration value should only be used as a memory\ncontrol mechanism and can cause early counter expiration if the value is reached.\nIt is recommended to leave the default value and ensure that your key is selected\nsuch that it limits the number of counters required (i.e. don't use UUID as the key).",
        "base": false,
        "name": "max_counters",
        "validate": "number",
        "default": "100000",
        "required": false
      },
      {
        "comments": "The period in seconds after the first occurrence of an event until a new timeslot\nis created.  This period is tracked per unique key and per timeslot.\nField substitutions are allowed in this value.  This allows you to specify that\ncertain kinds of events throttle for a specific period of time.",
        "base": false,
        "name": "period",
        "validate": "string",
        "default": "60",
        "required": false
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.  It is used by the memory\ncontrol mechanism.  Set to false if you like your VM to go (B)OOM.",
        "base": false,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-cipher/master/lib/logstash/filters/cipher.rb",
    "name": "cipher",
    "type": "filter",
    "params": [
      {
        "comments": "The cipher algorithm\n\nA list of supported algorithms can be obtained by\n[source,ruby]\n    puts OpenSSL::Cipher.ciphers",
        "base": false,
        "name": "algorithm",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Encrypting or decrypting some data\n\nValid values are encrypt or decrypt",
        "base": false,
        "name": "mode",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Do we have to perform a `base64` decode or encode?\n\nIf we are decrypting, `base64` decode will be done before.\nIf we are encrypting, `base64` will be done after.\n",
        "base": false,
        "name": "base64",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Cipher padding to use. Enables or disables padding.\n\nBy default encryption operations are padded using standard block padding\nand the padding is checked and removed when decrypting. If the pad\nparameter is zero then no padding is performed, the total amount of data\nencrypted or decrypted must then be a multiple of the block size or an\nerror will occur.\n\nSee EVP_CIPHER_CTX_set_padding for further information.\n\nWe are using Openssl jRuby which uses default padding to PKCS5Padding\nIf you want to change it, set this parameter. If you want to disable\nit, Set this parameter to 0\n[source,ruby]\n    filter { cipher { cipher_padding => 0 }}",
        "base": false,
        "name": "cipher_padding",
        "validate": "string"
      },
      {
        "comments": "The initialization vector to use (statically hard-coded). For\na random IV see the iv_random_length property\n\nNOTE: If iv_random_length is set, it takes precedence over any value set for \"iv\"\n\nThe cipher modes CBC, CFB, OFB and CTR all need an \"initialization\nvector\", or short, IV. ECB mode is the only mode that does not require\nan IV, but there is almost no legitimate use case for this mode\nbecause of the fact that it does not sufficiently hide plaintext patterns.\n\nFor AES algorithms set this to a 16 byte string.\n[source,ruby]\n    filter { cipher { iv => \"1234567890123456\" }}\n\nDeprecated: Please use `iv_random_length` instead",
        "base": false,
        "name": "iv",
        "validate": "string",
        "deprecated": "\"Please use \"iv_random_length\"\""
      },
      {
        "comments": "Force an random IV to be used per encryption invocation and specify\nthe length of the random IV that will be generated via:\n\n      OpenSSL::Random.random_bytes(int_length)\n\nIf iv_random_length is set, it takes precedence over any value set for \"iv\"\n\nEnabling this will force the plugin to generate a unique\nrandom IV for each encryption call. This random IV will be prepended to the\nencrypted result bytes and then base64 encoded. On decryption \"iv_random_length\" must\nalso be set to utilize this feature. Random IV's are better than statically\nhardcoded IVs\n\nFor AES algorithms you can set this to a 16\n[source,ruby]\n    filter { cipher { iv_random_length => 16 }}",
        "base": false,
        "name": "iv_random_length",
        "validate": "number"
      },
      {
        "comments": "The key to use\n\nNOTE: If you encounter an error message at runtime containing the following:\n\n\"java.security.InvalidKeyException: Illegal key size: possibly you need to install\nJava Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files for your JRE\"\n\nPlease read the following: https://github.com/jruby/jruby/wiki/UnlimitedStrengthCrypto\n",
        "base": false,
        "name": "key",
        "validate": "string"
      },
      {
        "comments": "The character used to pad the key",
        "base": false,
        "name": "key_pad",
        "default": "\"\\0\""
      },
      {
        "comments": "The key size to pad\n\nIt depends of the cipher algorithm. If your key doesn't need\npadding, don't set this parameter\n\nExample, for AES-128, we must have 16 char long key. AES-256 = 32 chars\n[source,ruby]\n    filter { cipher { key_size => 16 }\n",
        "base": false,
        "name": "key_size",
        "validate": "number",
        "default": "16"
      },
      {
        "comments": "If this is set the internal Cipher instance will be\nre-used up to @max_cipher_reuse times before being\nreset() and re-created from scratch. This is an option\nfor efficiency where lots of data is being encrypted\nand decrypted using this filter. This lets the filter\navoid creating new Cipher instances over and over\nfor each encrypt/decrypt operation.\n\nThis is optional, the default is no re-use of the Cipher\ninstance and max_cipher_reuse = 1 by default\n[source,ruby]\n    filter { cipher { max_cipher_reuse => 1000 }}",
        "base": false,
        "name": "max_cipher_reuse",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field to perform filter\n\nExample, to use the @message field (default) :\n[source,ruby]\n    filter { cipher { source => \"message\" } }",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The name of the container to put the result\n\nExample, to place the result into crypt :\n[source,ruby]\n    filter { cipher { target => \"crypt\" } }",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "message"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-syslog_pri/master/lib/logstash/filters/syslog_pri.rb",
    "name": "syslog_pri",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Labels for facility levels. This comes from RFC3164.",
        "base": false,
        "name": "facility_labels",
        "validate": "array",
        "default": "["
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Labels for severity levels. This comes from RFC3164.",
        "base": false,
        "name": "severity_labels",
        "validate": "array",
        "default": "["
      },
      {
        "comments": "Name of field which passes in the extracted PRI part of the syslog message",
        "base": false,
        "name": "syslog_pri_field_name",
        "validate": "string",
        "default": "syslog_pri"
      },
      {
        "comments": "Add human-readable names after parsing severity and facility from PRI",
        "base": false,
        "name": "use_labels",
        "validate": "boolean",
        "default": "true"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-split/master/lib/logstash/filters/split.rb",
    "name": "split",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field which value is split by the terminator.\nCan be a multiline message or the ID of an array.\nNested arrays are referenced like: \"[object_id][array_id]\"",
        "base": false,
        "name": "field",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field within the new event which the value is split into.\nIf not set, the target field defaults to split field name.",
        "base": false,
        "name": "target",
        "validate": "string"
      },
      {
        "comments": "The string to split on. This is usually a line terminator, but can be any\nstring. If you are splitting a JSON array into multiple events, you can ignore this field.",
        "base": false,
        "name": "terminator",
        "validate": "string",
        "default": "\\n"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-sleep/master/lib/logstash/filters/sleep.rb",
    "name": "sleep",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Sleep on every N'th. This option is ignored in replay mode.\n\nExample:\n[source,ruby]\n    filter {\n      sleep {\n        time => \"1\"   # Sleep 1 second\n        every => 10   # on every 10th event\n      }\n    }",
        "base": false,
        "name": "every",
        "validate": "string",
        "default": "1"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Enable replay mode.\n\nReplay mode tries to sleep based on timestamps in each event.\n\nThe amount of time to sleep is computed by subtracting the\nprevious event's timestamp from the current event's timestamp.\nThis helps you replay events in the same timeline as original.\n\nIf you specify a `time` setting as well, this filter will\nuse the `time` value as a speed modifier. For example,\na `time` value of 2 will replay at double speed, while a\nvalue of 0.25 will replay at 1/4th speed.\n\nFor example:\n[source,ruby]\n    filter {\n      sleep {\n        time => 2\n        replay => true\n      }\n    }\n\nThe above will sleep in such a way that it will perform\nreplay 2-times faster than the original time speed.",
        "base": false,
        "name": "replay",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The length of time to sleep, in seconds, for every event.\n\nThis can be a number (eg, 0.5), or a string (eg, `%{foo}`)\nThe second form (string with a field value) is useful if\nyou have an attribute of your event that you want to use\nto indicate the amount of time to sleep.\n\nExample:\n[source,ruby]\n    filter {\n      sleep {\n        # Sleep 1 second for every event.\n        time => \"1\"\n      }\n    }",
        "base": false,
        "name": "time",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-json_encode/master/lib/logstash/filters/json_encode.rb",
    "name": "json_encode",
    "type": "filter",
    "params": [
      {
        "comments": "The field to convert to JSON.",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field to write the JSON into. If not specified, the source\nfield will be overwritten.",
        "base": false,
        "name": "target",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-alter/master/lib/logstash/filters/alter.rb",
    "name": "alter",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Sets the value of field_name to the first nonnull expression among its arguments.\n\nExample:\n[source,ruby]\n    filter {\n      alter {\n        coalesce => [\n             \"field_name\", \"value1\", \"value2\", \"value3\", ...\n        ]\n      }\n    }",
        "base": false,
        "name": "coalesce",
        "validate": "array"
      },
      {
        "comments": "Change the content of the field to the specified value\nif the actual content is equal to the expected one.\n\nExample:\n[source,ruby]\n    filter {\n      alter {\n        condrewrite => [\n             \"field_name\", \"expected_value\", \"new_value\",\n             \"field_name2\", \"expected_value2\", \"new_value2\",\n             ....\n           ]\n      }\n    }",
        "base": false,
        "name": "condrewrite",
        "validate": "array"
      },
      {
        "comments": "Change the content of the field to the specified value\nif the content of another field is equal to the expected one.\n\nExample:\n[source,ruby]\n    filter {\n      alter {\n        condrewriteother => [\n             \"field_name\", \"expected_value\", \"field_name_to_change\", \"value\",\n             \"field_name2\", \"expected_value2\", \"field_name_to_change2\", \"value2\",\n             ....\n        ]\n      }\n    }",
        "base": false,
        "name": "condrewriteother",
        "validate": "array"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-checksum/master/lib/logstash/filters/checksum.rb",
    "name": "checksum",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "",
        "base": false,
        "name": "algorithm",
        "validate": "ALGORITHMS",
        "default": "sha256"
      },
      {
        "comments": "A list of keys to use in creating the string to checksum\nKeys will be sorted before building the string\nkeys and values will then be concatenated with pipe delimeters\nand checksummed",
        "base": false,
        "name": "keys",
        "validate": "array",
        "default": "[\"message\",\"@timestamp\",\"type\"]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-urldecode/master/lib/logstash/filters/urldecode.rb",
    "name": "urldecode",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Urldecode all fields",
        "base": false,
        "name": "all_fields",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Thel character encoding used in this filter. Examples include `UTF-8`\nand `cp1252`\n\nThis setting is useful if your url decoded string are in `Latin-1` (aka `cp1252`)\nor in another character set other than `UTF-8`.",
        "base": false,
        "name": "charset",
        "validate": "Encoding.name_list",
        "default": "UTF-8"
      },
      {
        "comments": "The field which value is urldecoded",
        "base": false,
        "name": "field",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-useragent/master/lib/logstash/filters/useragent.rb",
    "name": "useragent",
    "type": "filter",
    "params": [
      {
        "comments": "The field containing the user agent string. If this field is an\narray, only the first value will be used.",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "UA parsing is surprisingly expensive. This filter uses an LRU cache to take advantage of the fact that\nuser agents are often found adjacent to one another in log files and rarely have a random distribution.\nThe higher you set this the more likely an item is to be in the cache and the faster this filter will run.\nHowever, if you set this too high you can use more memory than desired.\n\nExperiment with different values for this option to find the best performance for your dataset.\n\nThis MUST be set to a value > 0. There is really no reason to not want this behavior, the overhead is minimal\nand the speed gains are large.\n\nIt is important to note that this config value is global. That is to say all instances of the user agent filter\nshare the same cache. The last declared cache size will 'win'. The reason for this is that there would be no benefit\nto having multiple caches for different instances at different points in the pipeline, that would just increase the\nnumber of cache misses and waste memory.",
        "base": false,
        "name": "lru_cache_size",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "A string to prepend to all of the extracted keys",
        "base": false,
        "name": "prefix",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "`regexes.yaml` file to use\n\nIf not specified, this will default to the `regexes.yaml` that ships\nwith logstash.\n\nYou can find the latest version of this here:\n<https://github.com/ua-parser/uap-core/blob/master/regexes.yaml>",
        "base": false,
        "name": "regexes",
        "validate": "string"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The name of the field to assign user agent data into.\n\nIf not specified user agent data will be stored in the root of the event.",
        "base": false,
        "name": "target",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-json/master/lib/logstash/filters/json.rb",
    "name": "json",
    "type": "filter",
    "params": [
      {
        "comments": "The configuration for the JSON filter:\n[source,ruby]\n    source => source_field\n\nFor example, if you have JSON data in the `message` field:\n[source,ruby]\n    filter {\n      json {\n        source => \"message\"\n      }\n    }\n\nThe above would parse the json from the `message` field",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Allow to skip filter on invalid json (allows to handle json and non-json data without warnings)",
        "base": false,
        "name": "skip_on_invalid_json",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Append values to the `tags` field when there has been no\nsuccessful match",
        "base": false,
        "name": "tag_on_failure",
        "validate": "array",
        "default": "[\"_jsonparsefailure\"]"
      },
      {
        "comments": "Define the target field for placing the parsed data. If this setting is\nomitted, the JSON data will be stored at the root (top level) of the event.\n\nFor example, if you want the data to be put in the `doc` field:\n[source,ruby]\n    filter {\n      json {\n        target => \"doc\"\n      }\n    }\n\nJSON in the value of the `source` field will be expanded into a\ndata structure in the `target` field.\n\nNOTE: if the `target` field already exists, it will be overwritten!",
        "base": false,
        "name": "target",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-github/master/lib/logstash/inputs/github.rb",
    "name": "github",
    "type": "input",
    "params": [
      {
        "comments": "The port to listen on",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "If Secret is defined, we drop the events that don't match.\nOtherwise, we'll just add an invalid tag",
        "base": false,
        "name": "drop_invalid",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The ip to listen on",
        "base": false,
        "name": "ip",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "Your GitHub Secret Token for the webhook",
        "base": false,
        "name": "secret_token",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-collate/master/lib/logstash/filters/collate.rb",
    "name": "collate",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "How many logs should be collated.",
        "base": false,
        "name": "count",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "The `interval` is the time window which how long the logs should be collated. (default `1m`)",
        "base": false,
        "name": "interval",
        "validate": "string",
        "default": "1m"
      },
      {
        "comments": "The `order` collated events should appear in.",
        "base": false,
        "name": "order",
        "validate": [
          "ascending",
          "descending"
        ],
        "default": "ascending"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-anonymize/master/lib/logstash/filters/anonymize.rb",
    "name": "anonymize",
    "type": "filter",
    "params": [
      {
        "comments": "digest/hash type",
        "base": false,
        "name": "algorithm",
        "validate": [
          "SHA1",
          "SHA256",
          "SHA384",
          "SHA512",
          "MD5",
          "MURMUR3",
          "IPV4_NETWORK"
        ],
        "required": true,
        "default": "SHA1"
      },
      {
        "comments": "The fields to be anonymized",
        "base": false,
        "name": "fields",
        "validate": "array",
        "required": true
      },
      {
        "comments": "Hashing key\nWhen using MURMUR3 the key is ignored but must still be set.\nWhen using IPV4_NETWORK key is the subnet prefix lentgh",
        "base": false,
        "name": "key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-fingerprint/master/lib/logstash/filters/fingerprint.rb",
    "name": "fingerprint",
    "type": "filter",
    "params": [
      {
        "comments": "The fingerprint method to use.\n\nIf set to `SHA1`, `SHA256`, `SHA384`, `SHA512`, or `MD5` the\ncryptographic keyed-hash function with the same name will be used to\ngenerate the fingerprint. If set to `MURMUR3` the non-cryptographic\nMurmurHash function will be used.\n\nIf set to `IPV4_NETWORK` the input data needs to be a IPv4 address and\nthe hash value will be the masked-out address using the number of bits\nspecified in the `key` option. For example, with \"1.2.3.4\" as the input\nand `key` set to 16, the hash becomes \"1.2.0.0\".\n\nIf set to `PUNCTUATION`, all non-punctuation characters will be removed\nfrom the input string.\n\nIf set to `UUID`, a\nhttps://en.wikipedia.org/wiki/Universally_unique_identifier[UUID] will\nbe generated. The result will be random and thus not a consistent hash.",
        "base": false,
        "name": "method",
        "validate": [
          "SHA1",
          "SHA256",
          "SHA384",
          "SHA512",
          "MD5",
          "MURMUR3",
          "IPV4_NETWORK",
          "UUID",
          "PUNCTUATION"
        ],
        "required": true,
        "default": "SHA1"
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "When set to `true`, the `SHA1`, `SHA256`, `SHA384`, `SHA512` and `MD5` fingerprint methods will produce\nbase64 encoded rather than hex encoded strings.",
        "base": false,
        "name": "base64encode",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "When set to `true` and `method` isn't `UUID` or `PUNCTUATION`, the\nplugin concatenates the names and values of all fields given in the\n`source` option into one string (like the old checksum filter) before\ndoing the fingerprint computation. If `false` and multiple source\nfields are given, the target field will be an array with fingerprints\nof the source fields given.",
        "base": false,
        "name": "concatenate_sources",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "When used with the `IPV4_NETWORK` method fill in the subnet prefix length.\nNot required for `MURMUR3` or `UUID` methods.\nWith other methods fill in the HMAC key.",
        "base": false,
        "name": "key",
        "validate": "string"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The name(s) of the source field(s) whose contents will be used\nto create the fingerprint. If an array is given, see the\n`concatenate_sources` option.",
        "base": false,
        "name": "source",
        "validate": "array",
        "default": "message"
      },
      {
        "comments": "The name of the field where the generated fingerprint will be stored.\nAny current contents of that field will be overwritten.",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "fingerprint"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-elapsed/master/lib/logstash/filters/elapsed.rb",
    "name": "elapsed",
    "type": "filter",
    "params": [
      {
        "comments": "The name of the tag identifying the \"end event\"",
        "base": false,
        "name": "end_tag",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The name of the tag identifying the \"start event\"",
        "base": false,
        "name": "start_tag",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The name of the field containing the task ID.\nThis value must uniquely identify the task in the system, otherwise\nit's impossible to match the couple of events.",
        "base": false,
        "name": "unique_id_field",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "This property manage what to do when an \"end event\" matches a \"start event\".\nIf it's set to `false` (default value), the elapsed information are added\nto the \"end event\"; if it's set to `true` a new \"match event\" is created.",
        "base": false,
        "name": "new_event_on_match",
        "validate": "boolean",
        "required": false,
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The amount of seconds after an \"end event\" can be considered lost.\nThe corresponding \"start event\" is discarded and an \"expired event\"\nis generated. The default value is 30 minutes (1800 seconds).",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "required": false,
        "default": "1800"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-extractnumbers/master/lib/logstash/filters/extractnumbers.rb",
    "name": "extractnumbers",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The source field for the data. By default is message.",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-prune/master/lib/logstash/filters/prune.rb",
    "name": "prune",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Exclude fields whose names match specified regexps, by default exclude unresolved `%{field}` strings.\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        blacklist_names => [ \"method\", \"(referrer|status)\", \"${some}_field\" ]\n      }\n    }",
        "base": false,
        "name": "blacklist_names",
        "validate": "array",
        "default": "[ \"%\\{[^}]+\\}\" ]"
      },
      {
        "comments": "Exclude specified fields if their values match one of the supplied regular expressions.\nIn case field values are arrays, each array item is matched against the regular expressions and matching array items will be excluded.\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        blacklist_values => [ \"uripath\", \"/index.php\",\n                              \"method\", \"(HEAD|OPTIONS)\",\n                              \"status\", \"^[^2]\" ]\n      }\n    }",
        "base": false,
        "name": "blacklist_values",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Trigger whether configuration fields and values should be interpolated for\ndynamic values (when resolving `%{some_field}`).\nProbably adds some performance overhead. Defaults to false.",
        "base": false,
        "name": "interpolate",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Include only fields only if their names match specified regexps, default to empty list which means include everything.\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        whitelist_names => [ \"method\", \"(referrer|status)\", \"${some}_field\" ]\n      }\n    }",
        "base": false,
        "name": "whitelist_names",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Include specified fields only if their values match one of the supplied regular expressions.\nIn case field values are arrays, each array item is matched against the regular expressions and only matching array items will be included.\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        whitelist_values => [ \"uripath\", \"/index.php\",\n                              \"method\", \"(GET|POST)\",\n                              \"status\", \"^[^2]\" ]\n      }\n    }",
        "base": false,
        "name": "whitelist_values",
        "validate": "hash",
        "default": "{}"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-csv/master/lib/logstash/filters/csv.rb",
    "name": "csv",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Define whether column names should be auto-detected from the header column or not.\nDefaults to false.",
        "base": false,
        "name": "autodetect_column_names",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Define whether column names should autogenerated or not.\nDefaults to true. If set to false, columns not having a header specified will not be parsed.",
        "base": false,
        "name": "autogenerate_column_names",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Define a list of column names (in the order they appear in the CSV,\nas if it were a header line). If `columns` is not configured, or there\nare not enough columns specified, the default column names are\n\"column1\", \"column2\", etc. In the case that there are more columns\nin the data than specified in this column list, extra columns will be auto-numbered:\n(e.g. \"user_defined_1\", \"user_defined_2\", \"column3\", \"column4\", etc.)",
        "base": false,
        "name": "columns",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Define a set of datatype conversions to be applied to columns.\nPossible conversions are integer, float, date, date_time, boolean\n\n# Example:\n[source,ruby]\n    filter {\n      csv {\n        convert => {\n          \"column1\" => \"integer\"\n          \"column2\" => \"boolean\"\n        }\n      }\n    }",
        "base": false,
        "name": "convert",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Define the character used to quote CSV fields. If this is not specified\nthe default is a double quote `\"`.\nOptional.",
        "base": false,
        "name": "quote_char",
        "validate": "string",
        "default": "\"\"\""
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Define the column separator value. If this is not specified, the default\nis a comma `,`. If you want to define a tabulation as a separator, you need\nto set the value to the actual tab character and not `\\t`.\nOptional.",
        "base": false,
        "name": "separator",
        "validate": "string",
        "default": ","
      },
      {
        "comments": "Define whether empty columns should be skipped.\nDefaults to false. If set to true, columns containing no value will not get set.",
        "base": false,
        "name": "skip_empty_columns",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The CSV data in the value of the `source` field will be expanded into a\ndata structure.",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "Define target field for placing the data.\nDefaults to writing to the root of the event.",
        "base": false,
        "name": "target",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-drop/master/lib/logstash/filters/drop.rb",
    "name": "drop",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Drop filter.\n\nDrops everything that gets to this filter.\n\nThis is best used in combination with conditionals, for example:\n[source,ruby]\n    filter {\n      if [loglevel] == \"debug\" {\n        drop { }\n      }\n    }\n\nThe above will only pass events to the drop filter if the loglevel field is\n`debug`. This will cause all events matching to be dropped.\nDrop all the events within a pre-configured percentage.\n\nThis is useful if you just need a percentage but not the whole.\n\nExample, to only drop around 40% of the events that have the field loglevel with value \"debug\".\n\n    filter {\n      if [loglevel] == \"debug\" {\n        drop {\n          percentage => 40\n        }\n      }\n    }",
        "base": false,
        "name": "percentage",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-multiline/master/lib/logstash/filters/multiline.rb",
    "name": "multiline",
    "type": "filter",
    "params": [
      {
        "comments": "The expression to match. The same matching engine as the\n<<plugins-filters-grok,grok filter>> is used, so the expression can contain\na plain regular expression or one that also contains grok patterns.",
        "base": false,
        "name": "pattern",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If the pattern matched, does event belong to the next or previous event?",
        "base": false,
        "name": "what",
        "validate": [
          "previous",
          "next"
        ],
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Allow duplcate values on the source field.",
        "base": false,
        "name": "allow_duplicates",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The maximum age an event can be (in seconds) before it is automatically\nflushed.",
        "base": false,
        "name": "max_age",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Negate the regexp pattern ('if not matched')",
        "base": false,
        "name": "negate",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Logstash ships by default with a bunch of patterns, so you don't\nnecessarily need to define this yourself unless you are adding additional\npatterns.\n\nPattern files are plain text with format:\n[source,ruby]\n    NAME PATTERN\n\nFor example:\n[source,ruby]\n    NUMBER \\d+",
        "base": false,
        "name": "patterns_dir",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": false,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field name to execute the pattern match on.",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The stream identity is how the multiline filter determines which stream an\nevent belongs to. This is generally used for differentiating, say, events\ncoming from multiple files in the same file input, or multiple connections\ncoming from a tcp input.\n\nThe default value here is usually what you want, but there are some cases\nwhere you want to change it. One such example is if you are using a tcp\ninput with only one client connecting at any time. If that client\nreconnects (due to error or client restart), then logstash will identify\nthe new connection as a new stream and break any multiline goodness that\nmay have occurred between the old and new connection. To solve this use\ncase, you can use `%{@source_host}.%{@type}` instead.",
        "base": false,
        "name": "stream_identity",
        "validate": "string",
        "default": "%{host}.%{path}.%{type}"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-kafka/master/lib/logstash/outputs/kafka.rb",
    "name": "kafka",
    "type": "output",
    "params": [
      {
        "comments": "The topic to produce messages to",
        "base": false,
        "name": "topic_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The number of acknowledgments the producer requires the leader to have received\nbefore considering a request complete.\n\nacks=0,   the producer will not wait for any acknowledgment from the server at all.\nacks=1,   This will mean the leader will write the record to its local log but\n          will respond without awaiting full acknowledgement from all followers.\nacks=all, This means the leader will wait for the full set of in-sync replicas to acknowledge the record.",
        "base": false,
        "name": "acks",
        "validate": [
          "0",
          "1",
          "all"
        ],
        "default": "1"
      },
      {
        "comments": "The producer will attempt to batch records together into fewer requests whenever multiple\nrecords are being sent to the same partition. This helps performance on both the client\nand the server. This configuration controls the default batch size in bytes.",
        "base": false,
        "name": "batch_size",
        "validate": "number",
        "default": "16384"
      },
      {
        "comments": "When our memory buffer is exhausted we must either stop accepting new\nrecords (block) or throw errors. By default this setting is true and we block,\nhowever in some scenarios blocking is not desirable and it is better to immediately give an error.",
        "base": false,
        "name": "block_on_buffer_full",
        "validate": "boolean",
        "default": true,
        "deprecated": "This config will be removed in a future release"
      },
      {
        "comments": "This is for bootstrapping and the producer will only use it for getting metadata (topics,\npartitions and replicas). The socket connections for sending the actual data will be\nestablished based on the broker information returned in the metadata. The format is\n`host1:port1,host2:port2`, and the list can be a subset of brokers or a VIP pointing to a\nsubset of brokers.",
        "base": false,
        "name": "bootstrap_servers",
        "validate": "string",
        "default": "localhost9092"
      },
      {
        "comments": "The total bytes of memory the producer can use to buffer records waiting to be sent to the server.",
        "base": false,
        "name": "buffer_memory",
        "validate": "number",
        "default": "33554432"
      },
      {
        "comments": "The id string to pass to the server when making requests.\nThe purpose of this is to be able to track the source of requests beyond just\nip/port by allowing a logical application name to be included with the request",
        "base": false,
        "name": "client_id",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The compression type for all data generated by the producer.\nThe default is none (i.e. no compression). Valid values are none, gzip, or snappy.",
        "base": false,
        "name": "compression_type",
        "validate": [
          "none",
          "gzip",
          "snappy",
          "lz4"
        ],
        "default": "none"
      },
      {
        "comments": "The Java Authentication and Authorization Service (JAAS) API supplies user authentication and authorization\nservices for Kafka. This setting provides the path to the JAAS file. Sample JAAS file for Kafka client:\n[source,java]\n----------------------------------\nKafkaClient {\n  com.sun.security.auth.module.Krb5LoginModule required\n  useTicketCache=true\n  renewTicket=true\n  serviceName=\"kafka\";\n  };\n----------------------------------\n\nPlease note that specifying `jaas_path` and `kerberos_config` in the config file will add these\nto the global JVM system properties. This means if you have multiple Kafka inputs, all of them would be sharing the same\n`jaas_path` and `kerberos_config`. If this is not desirable, you would have to run separate instances of Logstash on\ndifferent JVM instances.",
        "base": false,
        "name": "jaas_path",
        "validate": "path"
      },
      {
        "comments": "Optional path to kerberos config file. This is krb5.conf style as detailed in https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html",
        "base": false,
        "name": "kerberos_config",
        "validate": "path"
      },
      {
        "comments": "Serializer class for the key of the message",
        "base": false,
        "name": "key_serializer",
        "validate": "string",
        "default": "org.apache.kafka.common.serialization.StringSerializer"
      },
      {
        "comments": "The producer groups together any records that arrive in between request\ntransmissions into a single batched request. Normally this occurs only under\nload when records arrive faster than they can be sent out. However in some circumstances\nthe client may want to reduce the number of requests even under moderate load.\nThis setting accomplishes this by adding a small amount of artificial delaythat is,\nrather than immediately sending out a record the producer will wait for up to the given delay\nto allow other records to be sent so that the sends can be batched together.",
        "base": false,
        "name": "linger_ms",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The maximum size of a request",
        "base": false,
        "name": "max_request_size",
        "validate": "number",
        "default": "1048576"
      },
      {
        "comments": "The key for the message",
        "base": false,
        "name": "message_key",
        "validate": "string"
      },
      {
        "comments": "the timeout setting for initial metadata request to fetch topic metadata.",
        "base": false,
        "name": "metadata_fetch_timeout_ms",
        "validate": "number",
        "default": "60000"
      },
      {
        "comments": "the max time in milliseconds before a metadata refresh is forced.",
        "base": false,
        "name": "metadata_max_age_ms",
        "validate": "number",
        "default": "300000"
      },
      {
        "comments": "The size of the TCP receive buffer to use when reading data",
        "base": false,
        "name": "receive_buffer_bytes",
        "validate": "number",
        "default": "32768"
      },
      {
        "comments": "The amount of time to wait before attempting to reconnect to a given host when a connection fails.",
        "base": false,
        "name": "reconnect_backoff_ms",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "The configuration controls the maximum amount of time the client will wait\nfor the response of a request. If the response is not received before the timeout\nelapses the client will resend the request if necessary or fail the request if\nretries are exhausted.",
        "base": false,
        "name": "request_timeout_ms",
        "validate": "string"
      },
      {
        "comments": "Setting a value greater than zero will cause the client to\nresend any record whose send fails with a potentially transient error.",
        "base": false,
        "name": "retries",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The amount of time to wait before attempting to retry a failed produce request to a given topic partition.",
        "base": false,
        "name": "retry_backoff_ms",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "The Kerberos principal name that Kafka broker runs as.\nThis can be defined either in Kafka's JAAS config or in Kafka's config.",
        "base": false,
        "name": "sasl_kerberos_service_name",
        "validate": "string"
      },
      {
        "comments": "http://kafka.apache.org/documentation.html#security_sasl[SASL mechanism] used for client connections.\nThis may be any mechanism for which a security provider is available.\nGSSAPI is the default mechanism.",
        "base": false,
        "name": "sasl_mechanism",
        "validate": "string",
        "default": "GSSAPI"
      },
      {
        "comments": "Security protocol to use, which can be either of PLAINTEXT,SSL,SASL_PLAINTEXT,SASL_SSL",
        "base": false,
        "name": "security_protocol",
        "validate": [
          "PLAINTEXT",
          "SSL",
          "SASL_PLAINTEXT",
          "SASL_SSL"
        ],
        "default": "PLAINTEXT"
      },
      {
        "comments": "The size of the TCP send buffer to use when sending data.",
        "base": false,
        "name": "send_buffer_bytes",
        "validate": "number",
        "default": "131072"
      },
      {
        "comments": "Enable SSL/TLS secured communication to Kafka broker.",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": false,
        "deprecated": "\"Use security_protocol => \"ssl\"\""
      },
      {
        "comments": "The password of the private key in the key store file.",
        "base": false,
        "name": "ssl_key_password",
        "validate": "password"
      },
      {
        "comments": "If client authentication is required, this setting stores the keystore path.",
        "base": false,
        "name": "ssl_keystore_location",
        "validate": "path"
      },
      {
        "comments": "If client authentication is required, this setting stores the keystore password",
        "base": false,
        "name": "ssl_keystore_password",
        "validate": "password"
      },
      {
        "comments": "The keystore type.",
        "base": false,
        "name": "ssl_keystore_type",
        "validate": "string"
      },
      {
        "comments": "The JKS truststore path to validate the Kafka broker's certificate.",
        "base": false,
        "name": "ssl_truststore_location",
        "validate": "path"
      },
      {
        "comments": "The truststore password",
        "base": false,
        "name": "ssl_truststore_password",
        "validate": "password"
      },
      {
        "comments": "The truststore type.",
        "base": false,
        "name": "ssl_truststore_type",
        "validate": "string"
      },
      {
        "comments": "The configuration controls the maximum amount of time the server will wait for acknowledgments\nfrom followers to meet the acknowledgment requirements the producer has specified with the\nacks configuration. If the requested number of acknowledgments are not met when the timeout\nelapses an error will be returned. This timeout is measured on the server side and does not\ninclude the network latency of the request.",
        "base": false,
        "name": "timeout_ms",
        "validate": "number",
        "default": 30000,
        "deprecated": "This config will be removed in a future release. Please use request_timeout_ms"
      },
      {
        "comments": "Serializer class for the value of the message",
        "base": false,
        "name": "value_serializer",
        "validate": "string",
        "default": "org.apache.kafka.common.serialization.StringSerializer"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-mutate/master/lib/logstash/filters/mutate.rb",
    "name": "mutate",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Convert a field's value to a different type, like turning a string to an\ninteger. If the field value is an array, all members will be converted.\nIf the field is a hash, no action will be taken.\n\nIf the conversion type is `boolean`, the acceptable values are:\n\n* **True:** `true`, `t`, `yes`, `y`, and `1`\n* **False:** `false`, `f`, `no`, `n`, and `0`\n\nIf a value other than these is provided, it will pass straight through\nand log a warning message.\n\nValid conversion targets are: integer, float, string, and boolean.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        convert => { \"fieldname\" => \"integer\" }\n      }\n    }",
        "base": false,
        "name": "convert",
        "validate": "hash"
      },
      {
        "comments": "Copy an existing field to another field. Existing target field will be overriden.\n==========================\nExample:\n[source,ruby]\n    filter {\n      mutate {\n         copy => { \"source_field\" => \"dest_field\" }\n      }\n    }",
        "base": false,
        "name": "copy",
        "validate": "hash"
      },
      {
        "comments": "Convert a string field by applying a regular expression and a replacement.\nIf the field is not a string, no action will be taken.\n\nThis configuration takes an array consisting of 3 elements per\nfield/substitution.\n\nBe aware of escaping any backslash in the config file.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        gsub => [\n          # replace all forward slashes with underscore\n          \"fieldname\", \"/\", \"_\",\n          # replace backslashes, question marks, hashes, and minuses\n          # with a dot \".\"\n          \"fieldname2\", \"[\\\\?#-]\", \".\"\n        ]\n      }\n    }\n",
        "base": false,
        "name": "gsub",
        "validate": "array"
      },
      {
        "comments": "Join an array with a separator character. Does nothing on non-array fields.\n\nExample:\n[source,ruby]\n   filter {\n     mutate {\n       join => { \"fieldname\" => \",\" }\n     }\n   }",
        "base": false,
        "name": "join",
        "validate": "hash"
      },
      {
        "comments": "Convert a string to its lowercase equivalent.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        lowercase => [ \"fieldname\" ]\n      }\n    }",
        "base": false,
        "name": "lowercase",
        "validate": "array"
      },
      {
        "comments": "Merge two fields of arrays or hashes.\nString fields will be automatically be converted into an array, so:\n==========================\n  `array` + `string` will work\n  `string` + `string` will result in an 2 entry array in `dest_field`\n  `array` and `hash` will not work\n==========================\nExample:\n[source,ruby]\n    filter {\n      mutate {\n         merge => { \"dest_field\" => \"added_field\" }\n      }\n    }",
        "base": false,
        "name": "merge",
        "validate": "hash"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Rename one or more fields.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        # Renames the 'HOSTORIP' field to 'client_ip'\n        rename => { \"HOSTORIP\" => \"client_ip\" }\n      }\n    }",
        "base": false,
        "name": "rename",
        "validate": "hash"
      },
      {
        "comments": "Replace a field with a new value. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        replace => { \"message\" => \"%{source_host}: My new message\" }\n      }\n    }",
        "base": false,
        "name": "replace",
        "validate": "hash"
      },
      {
        "comments": "Split a field to an array using a separator character. Only works on string\nfields.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n         split => { \"fieldname\" => \",\" }\n      }\n    }",
        "base": false,
        "name": "split",
        "validate": "hash"
      },
      {
        "comments": "Strip whitespace from field. NOTE: this only works on leading and trailing whitespace.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n         strip => [\"field1\", \"field2\"]\n      }\n    }",
        "base": false,
        "name": "strip",
        "validate": "array"
      },
      {
        "comments": "Update an existing field with a new value. If the field does not exist,\nthen no action will be taken.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        update => { \"sample\" => \"My new message\" }\n      }\n    }",
        "base": false,
        "name": "update",
        "validate": "hash"
      },
      {
        "comments": "Convert a string to its uppercase equivalent.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        uppercase => [ \"fieldname\" ]\n      }\n    }",
        "base": false,
        "name": "uppercase",
        "validate": "array"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-grok/master/lib/logstash/filters/grok.rb",
    "name": "grok",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Break on first match. The first successful match by grok will result in the\nfilter being finished. If you want grok to try all patterns (maybe you are\nparsing different things), then set this to false.",
        "base": false,
        "name": "break_on_match",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "If `true`, keep empty captures as event fields.",
        "base": false,
        "name": "keep_empty_captures",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "",
        "base": false,
        "name": "match",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If `true`, only store named captures from grok.",
        "base": false,
        "name": "named_captures_only",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The fields to overwrite.\n\nThis allows you to overwrite a value in a field that already exists.\n\nFor example, if you have a syslog line in the `message` field, you can\noverwrite the `message` field with part of the match like so:\n[source,ruby]\n    filter {\n      grok {\n        match => { \"message\" => \"%{SYSLOGBASE} %{DATA:message}\" }\n        overwrite => [ \"message\" ]\n      }\n    }\n\nIn this case, a line like `May 29 16:37:11 sadness logger: hello world`\nwill be parsed and `hello world` will overwrite the original message.",
        "base": false,
        "name": "overwrite",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "A hash of pattern-name and pattern tuples defining custom patterns to be used by\nthe current filter. Patterns matching existing names will override the pre-existing\ndefinition. Think of this as inline patterns available just for this definition of\ngrok",
        "base": false,
        "name": "pattern_definitions",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "\nLogstash ships by default with a bunch of patterns, so you don't\nnecessarily need to define this yourself unless you are adding additional\npatterns. You can point to multiple pattern directories using this setting.\nNote that Grok will read all files in the directory matching the patterns_files_glob\nand assume it's a pattern file (including any tilde backup files).\n[source,ruby]\n    patterns_dir => [\"/opt/logstash/patterns\", \"/opt/logstash/extra_patterns\"]\n\nPattern files are plain text with format:\n[source,ruby]\n    NAME PATTERN\n\nFor example:\n[source,ruby]\n    NUMBER \\d+\n\nThe patterns are loaded when the pipeline is created.",
        "base": false,
        "name": "patterns_dir",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Glob pattern, used to select the pattern files in the directories\nspecified by patterns_dir",
        "base": false,
        "name": "patterns_files_glob",
        "validate": "string",
        "default": "*"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Append values to the `tags` field when there has been no\nsuccessful match",
        "base": false,
        "name": "tag_on_failure",
        "validate": "array",
        "default": "[\"_grokparsefailure\"]"
      },
      {
        "comments": "Tag to apply if a grok regexp times out.",
        "base": false,
        "name": "tag_on_timeout",
        "validate": "string",
        "default": "_groktimeout"
      },
      {
        "comments": "Attempt to terminate regexps after this amount of time.\nThis applies per pattern if multiple patterns are applied\nThis will never timeout early, but may take a little longer to timeout.\nActual timeout is approximate based on a 250ms quantization.\nSet to 0 to disable timeouts",
        "base": false,
        "name": "timeout_millis",
        "validate": "number",
        "default": "30000"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-exec/master/lib/logstash/inputs/exec.rb",
    "name": "exec",
    "type": "input",
    "params": [
      {
        "comments": "Command to run. For example, `uptime`",
        "base": false,
        "name": "command",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Interval to run the command. Value is in seconds.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-environment/master/lib/logstash/filters/environment.rb",
    "name": "environment",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Specify a hash of field names and the environment variable name with the\nvalue you want imported into Logstash. For example:\n\n   add_metadata_from_env => { \"field_name\" => \"ENV_VAR_NAME\" }\n\nor\n\n   add_metadata_from_env => {\n     \"field1\" => \"ENV1\"\n     \"field2\" => \"ENV2\"\n     # \"field_n\" => \"ENV_n\"\n   }",
        "base": false,
        "name": "add_metadata_from_env",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-date/master/lib/logstash/filters/date.rb",
    "name": "date",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Specify a locale to be used for date parsing using either IETF-BCP47 or POSIX language tag.\nSimple examples are `en`,`en-US` for BCP47 or `en_US` for POSIX.\n\nThe locale is mostly necessary to be set for parsing month names (pattern with `MMM`) and\nweekday names (pattern with `EEE`).\n\nIf not specified, the platform default will be used but for non-english platform default\nan english parser will also be used as a fallback mechanism.",
        "base": false,
        "name": "locale",
        "validate": "string"
      },
      {
        "comments": "An array with field name first, and format patterns following, `[ field,\nformats... ]`\n\nIf your time field has multiple possible formats, you can do this:\n[source,ruby]\n    match => [ \"logdate\", \"MMM dd yyyy HH:mm:ss\",\n              \"MMM  d yyyy HH:mm:ss\", \"ISO8601\" ]\n\nThe above will match a syslog (rfc3164) or `iso8601` timestamp.\n\nThere are a few special exceptions. The following format literals exist\nto help you save time and ensure correctness of date parsing.\n\n* `ISO8601` - should parse any valid ISO8601 timestamp, such as\n  `2011-04-19T03:44:01.103Z`\n* `UNIX` - will parse *float or int* value expressing unix time in seconds since epoch like 1326149001.132 as well as 1326149001\n* `UNIX_MS` - will parse **int** value expressing unix time in milliseconds since epoch like 1366125117000\n* `TAI64N` - will parse tai64n time values\n\nFor example, if you have a field `logdate`, with a value that looks like\n`Aug 13 2010 00:03:44`, you would use this configuration:\n[source,ruby]\n    filter {\n      date {\n        match => [ \"logdate\", \"MMM dd yyyy HH:mm:ss\" ]\n      }\n    }\n\nIf your field is nested in your structure, you can use the nested\nsyntax `[foo][bar]` to match its value. For more information, please refer to\n<<logstash-config-field-references>>\n\n*More details on the syntax*\n\nThe syntax used for parsing date and time text uses letters to indicate the\nkind of time value (month, minute, etc), and a repetition of letters to\nindicate the form of that value (2-digit month, full month name, etc).\n\nHere's what you can use to parse dates and times:\n\n[horizontal]\ny:: year\n  yyyy::: full year number. Example: `2015`.\n  yy::: two-digit year. Example: `15` for the year 2015.\n\nM:: month of the year\n  M::: minimal-digit month. Example: `1` for January and `12` for December.\n  MM::: two-digit month. zero-padded if needed. Example: `01` for January  and `12` for December\n  MMM::: abbreviated month text. Example: `Jan` for January. Note: The language used depends on your locale. See the `locale` setting for how to change the language.\n  MMMM::: full month text, Example: `January`. Note: The language used depends on your locale.\n\nd:: day of the month\n  d::: minimal-digit day. Example: `1` for the 1st of the month.\n  dd::: two-digit day, zero-padded if needed. Example: `01` for the 1st of the month.\n\nH:: hour of the day (24-hour clock)\n  H::: minimal-digit hour. Example: `0` for midnight.\n  HH::: two-digit hour, zero-padded if needed. Example: `00` for midnight.\n\nm:: minutes of the hour (60 minutes per hour)\n  m::: minimal-digit minutes. Example: `0`.\n  mm::: two-digit minutes, zero-padded if needed. Example: `00`.\n\ns:: seconds of the minute (60 seconds per minute)\n  s::: minimal-digit seconds. Example: `0`.\n  ss::: two-digit seconds, zero-padded if needed. Example: `00`.\n\nS:: fraction of a second\n  *Maximum precision is milliseconds (`SSS`). Beyond that, zeroes are appended.*\n  S::: tenths of a second. Example:  `0` for a subsecond value `012`\n  SS::: hundredths of a second. Example:  `01` for a subsecond value `01`\n  SSS::: thousandths of a second. Example:  `012` for a subsecond value `012`\n\nZ:: time zone offset or identity\n  Z::: Timezone offset structured as HHmm (hour and minutes offset from Zulu/UTC). Example: `-0700`.\n  ZZ::: Timezone offset structured as HH:mm (colon in between hour and minute offsets). Example: `-07:00`.\n  ZZZ::: Timezone identity. Example: `America/Los_Angeles`. Note: Valid IDs are listed on the http://joda-time.sourceforge.net/timezones.html[Joda.org available time zones page].\n\nz:: time zone names. *Time zone names ('z') cannot be parsed.*\n\nw:: week of the year\n  w::: minimal-digit week. Example: `1`.\n  ww::: two-digit week, zero-padded if needed. Example: `01`.\n\nD:: day of the year\n\ne:: day of the week (number)\n\nE:: day of the week (text)\n  E, EE, EEE::: Abbreviated day of the week. Example:  `Mon`, `Tue`, `Wed`, `Thu`, `Fri`, `Sat`, `Sun`. Note: The actual language of this will depend on your locale.\n  EEEE::: The full text day of the week. Example: `Monday`, `Tuesday`, ... Note: The actual language of this will depend on your locale.\n\nFor non-formatting syntax, you'll need to put single-quote characters around the value. For example, if you were parsing ISO8601 time, \"2015-01-01T01:12:23\" that little \"T\" isn't a valid time format, and you want to say \"literally, a T\", your format would be this: \"yyyy-MM-dd'T'HH:mm:ss\"\n\nOther less common date units, such as era (G), century \\(C), am/pm (a), and # more, can be learned about on the\nhttp://www.joda.org/joda-time/key_format.html[joda-time documentation].",
        "base": false,
        "name": "match",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Append values to the `tags` field when there has been no\nsuccessful match",
        "base": false,
        "name": "tag_on_failure",
        "validate": "array",
        "default": "[\"_dateparsefailure\"]"
      },
      {
        "comments": "Store the matching timestamp into the given target field.  If not provided,\ndefault to updating the `@timestamp` field of the event.",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "LogStashEventTIMESTAMP"
      },
      {
        "comments": "Specify a time zone canonical ID to be used for date parsing.\nThe valid IDs are listed on the http://joda-time.sourceforge.net/timezones.html[Joda.org available time zones page].\nThis is useful in case the time zone cannot be extracted from the value,\nand is not the platform default.\nIf this is not specified the platform default will be used.\nCanonical ID is good as it takes care of daylight saving time for you\nFor example, `America/Los_Angeles` or `Europe/Paris` are valid IDs.\nThis field can be dynamic and include parts of the event using the `%{field}` syntax",
        "base": false,
        "name": "timezone",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-dns/master/lib/logstash/filters/dns.rb",
    "name": "dns",
    "type": "filter",
    "params": [
      {
        "comments": "Determine what action to do: append or replace the values in the fields\nspecified under `reverse` and `resolve`.",
        "base": false,
        "name": "action",
        "validate": [
          "append",
          "replace"
        ],
        "default": "append"
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "cache size for failed requests",
        "base": false,
        "name": "failed_cache_size",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "how long to cache failed requests (in seconds)",
        "base": false,
        "name": "failed_cache_ttl",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "set the size of cache for successful requests",
        "base": false,
        "name": "hit_cache_size",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "how long to cache successful requests (in seconds)",
        "base": false,
        "name": "hit_cache_ttl",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "Use custom hosts file(s). For example: `[\"/var/db/my_custom_hosts\"]`",
        "base": false,
        "name": "hostsfile",
        "validate": "array"
      },
      {
        "comments": "number of times to retry a failed resolve/reverse",
        "base": false,
        "name": "max_retries",
        "validate": "number",
        "default": "2"
      },
      {
        "comments": "Use custom nameserver(s). For example: `[\"8.8.8.8\", \"8.8.4.4\"]`",
        "base": false,
        "name": "nameserver",
        "validate": "array"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Forward resolve one or more fields.",
        "base": false,
        "name": "resolve",
        "validate": "array"
      },
      {
        "comments": "Reverse resolve one or more fields.",
        "base": false,
        "name": "reverse",
        "validate": "array"
      },
      {
        "comments": "`resolv` calls will be wrapped in a timeout instance",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "0.5"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-zeromq/master/lib/logstash/outputs/zeromq.rb",
    "name": "zeromq",
    "type": "output",
    "params": [
      {
        "comments": "The default logstash topologies work as follows:\n\n* pushpull - inputs are pull, outputs are push\n* pubsub - inputs are subscribers, outputs are publishers\n* pair - inputs are clients, outputs are servers\n\nIf the predefined topology flows don't work for you,\nyou can change the 'mode' setting",
        "base": false,
        "name": "topology",
        "validate": [
          "pushpull",
          "pubsub",
          "pair"
        ],
        "required": true
      },
      {
        "comments": "0mq socket address to connect or bind.\nPlease note that `inproc://` will not work with logstashi.\nFor each we use a context per thread.\nBy default, inputs bind/listen and outputs connect.",
        "base": false,
        "name": "address",
        "validate": "array",
        "default": "[\"tcp//127.0.0.12120\"]"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Server mode binds/listens. Client mode connects.",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "client"
      },
      {
        "comments": "This exposes zmq_setsockopt for advanced tuning.\nSee http://api.zeromq.org/2-1:zmq-setsockopt for details.\n\nThis is where you would set values like:\n\n* ZMQ::HWM - high water mark\n* ZMQ::IDENTITY - named queues\n* ZMQ::SWAP_SIZE - space for disk overflow\n\nExample:\n[source,ruby]\n    sockopt => {\n       \"ZMQ::HWM\" => 50\n       \"ZMQ::IDENTITY\"  => \"my_named_queue\"\n    }",
        "base": false,
        "name": "sockopt",
        "validate": "hash"
      },
      {
        "comments": "This is used for the 'pubsub' topology only.\nOn inputs, this allows you to filter messages by topic.\nOn outputs, this allows you to tag a message for routing.\nNOTE: ZeroMQ does subscriber-side filtering\nNOTE: Topic is evaluated with `event.sprintf` so macros are valid here.",
        "base": false,
        "name": "topic",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-librato/master/lib/logstash/outputs/librato.rb",
    "name": "librato",
    "type": "output",
    "params": [
      {
        "comments": "Your Librato account\nusually an email address",
        "base": false,
        "name": "account_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your Librato API Token",
        "base": false,
        "name": "api_token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Annotations\nRegisters an annotation with Librato\nThe only required field is `title` and `name`.\n`start_time` and `end_time` will be set to `event.get(\"@timestamp\").to_i`\nYou can add any other optional annotation values as well.\nAll values will be passed through `event.sprintf`\n\nExample:\n[source,ruby]\n  {\n      \"title\" => \"Logstash event on %{host}\"\n      \"name\" => \"logstash_stream\"\n  }\nor\n[source,ruby]\n   {\n      \"title\" => \"Logstash event\"\n      \"description\" => \"%{message}\"\n      \"name\" => \"logstash_stream\"\n   }",
        "base": false,
        "name": "annotation",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Batch size\nNumber of events to batch up before sending to Librato.\n",
        "base": false,
        "name": "batch_size",
        "validate": "string",
        "default": "10"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Counters\nSend data to Librato as a counter\n\nExample:\n[source,ruby]\n    {\n        \"value\" => \"1\"\n        \"source\" => \"%{host}\"\n        \"name\" => \"messages_received\"\n    }\n\nAdditionally, you can override the `measure_time` for the event. Must be a unix timestamp:\n[source,ruby]\n    {\n        \"value\" => \"1\"\n        \"source\" => \"%{host}\"\n        \"name\" => \"messages_received\"\n        \"measure_time\" => \"%{my_unixtime_field}\"\n    }\nDefault is to use the event's timestamp",
        "base": false,
        "name": "counter",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Gauges\nSend data to Librato as a gauge\n\nExample:\n[source,ruby]\n    {\n        \"value\" => \"%{bytes_received}\"\n        \"source\" => \"%{host}\"\n        \"name\" => \"apache_bytes\"\n    }\nAdditionally, you can override the `measure_time` for the event. Must be a unix timestamp:\n[source,ruby]\n    {\n        \"value\" => \"%{bytes_received}\"\n        \"source\" => \"%{host}\"\n        \"name\" => \"apache_bytes\"\n        \"measure_time\" => \"%{my_unixtime_field}\n    }\nDefault is to use the event's timestamp",
        "base": false,
        "name": "gauge",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-loggly/master/lib/logstash/outputs/loggly.rb",
    "name": "loggly",
    "type": "output",
    "params": [
      {
        "comments": "The loggly http input key to send to.\nThis is usually visible in the Loggly 'Inputs' page as something like this:\n....\n    https://logs-01.loggly.net/inputs/abcdef12-3456-7890-abcd-ef0123456789\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                          \\---------->   key   <-------------/\n....\nYou can use `%{foo}` field lookups here if you need to pull the api key from\nthe event. This is mainly aimed at multitenant hosting providers who want\nto offer shipping a customer's logs to that customer's loggly account.",
        "base": false,
        "name": "key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Can Retry.\nSetting this value true helps user to send multiple retry attempts if the first request fails",
        "base": false,
        "name": "can_retry",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The hostname to send logs to. This should target the loggly http input\nserver which is usually \"logs-01.loggly.com\" (Gen2 account).\nSee Loggly HTTP endpoint documentation at\nhttps://www.loggly.com/docs/http-endpoint/",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "logs-01.loggly.com"
      },
      {
        "comments": "Should the log action be sent over https instead of plain http",
        "base": false,
        "name": "proto",
        "validate": "string",
        "default": "http"
      },
      {
        "comments": "Proxy Host",
        "base": false,
        "name": "proxy_host",
        "validate": "string"
      },
      {
        "comments": "Proxy Password",
        "base": false,
        "name": "proxy_password",
        "validate": "password",
        "default": ""
      },
      {
        "comments": "Proxy Port",
        "base": false,
        "name": "proxy_port",
        "validate": "number"
      },
      {
        "comments": "Proxy Username",
        "base": false,
        "name": "proxy_user",
        "validate": "string"
      },
      {
        "comments": "Retry count.\nIt may be possible that the request may timeout due to slow Internet connection\nif such condition appears, retry_count helps in retrying request for multiple times\nIt will try to submit request until retry_count and then halt",
        "base": false,
        "name": "retry_count",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Loggly Tag\nTag helps you to find your logs in the Loggly dashboard easily\nYou can make a search in Loggly using tag as \"tag:logstash-contrib\"\nor the tag set by you in the config file.\n\nYou can use %{somefield} to allow for custom tag values.\nHelpful for leveraging Loggly source groups.\nhttps://www.loggly.com/docs/source-groups/",
        "base": false,
        "name": "tag",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-graphtastic/master/lib/logstash/outputs/graphtastic.rb",
    "name": "graphtastic",
    "type": "output",
    "params": [
      {
        "comments": "the number of metrics to send to GraphTastic at one time. 60 seems to be the perfect\namount for UDP, with default packet size.",
        "base": false,
        "name": "batch_number",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "if using rest as your end point you need to also provide the application url\nit defaults to localhost/graphtastic.  You can customize the application url\nby changing the name of the .war file.  There are other ways to change the\napplication context, but they vary depending on the Application Server in use.\nPlease consult your application server documentation for more on application\ncontexts.",
        "base": false,
        "name": "context",
        "validate": "string",
        "default": "graphtastic"
      },
      {
        "comments": "setting allows you to specify where we save errored transactions\nthis makes the most sense at this point - will need to decide\non how we reintegrate these error metrics\nNOT IMPLEMENTED!",
        "base": false,
        "name": "error_file",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "host for the graphtastic server - defaults to 127.0.0.1",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "127.0.0.1"
      },
      {
        "comments": "options are udp(fastest - default) - rmi(faster) - rest(fast) - tcp(don't use TCP yet - some problems - errors out on linux)",
        "base": false,
        "name": "integration",
        "validate": [
          "udp",
          "tcp",
          "rmi",
          "rest"
        ],
        "default": "udp"
      },
      {
        "comments": "metrics hash - you will provide a name for your metric and the metric\ndata as key value pairs.  so for example:\n\n[source,ruby]\nmetrics => { \"Response\" => \"%{response}\" }\n\nexample for the logstash config\n\n[source,ruby]\nmetrics => [ \"Response\", \"%{response}\" ]\n\nNOTE: you can also use the dynamic fields for the key value as well as the actual value",
        "base": false,
        "name": "metrics",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "port for the graphtastic instance - defaults to 1199 for RMI, 1299 for TCP, 1399 for UDP, and 8080 for REST",
        "base": false,
        "name": "port",
        "validate": "number"
      },
      {
        "comments": "number of attempted retry after send error - currently only way to integrate\nerrored transactions - should try and save to a file or later consumption\neither by graphtastic utility or by this program after connectivity is\nensured to be established.",
        "base": false,
        "name": "retries",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-xmpp/master/lib/logstash/outputs/xmpp.rb",
    "name": "xmpp",
    "type": "output",
    "params": [
      {
        "comments": "The message to send. This supports dynamic strings like `%{host}`",
        "base": false,
        "name": "message",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The xmpp password for the user/identity.",
        "base": false,
        "name": "password",
        "validate": "password",
        "required": true
      },
      {
        "comments": "The user or resource ID, like foo@example.com.",
        "base": false,
        "name": "user",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The xmpp server to connect to. This is optional. If you omit this setting,\nthe host on the user/identity is used. (foo.com for user@foo.com)",
        "base": false,
        "name": "host",
        "validate": "string"
      },
      {
        "comments": "if muc/multi-user-chat required, give the name of the room that\nyou want to join: room@conference.domain/nick",
        "base": false,
        "name": "rooms",
        "validate": "array"
      },
      {
        "comments": "The users to send messages to",
        "base": false,
        "name": "users",
        "validate": "array"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-jira/master/lib/logstash/outputs/jira.rb",
    "name": "jira",
    "type": "output",
    "params": [
      {
        "comments": "JIRA Issuetype number",
        "base": false,
        "name": "issuetypeid",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "password",
        "validate": "string",
        "required": true
      },
      {
        "comments": "JIRA Priority",
        "base": false,
        "name": "priority",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Javalicious has no proxy support\n#\nJIRA Project number",
        "base": false,
        "name": "projectid",
        "validate": "string",
        "required": true
      },
      {
        "comments": "JIRA Summary\n\nTruncated and appended with '...' if longer than 255 characters.",
        "base": false,
        "name": "summary",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "username",
        "validate": "string",
        "required": true
      },
      {
        "comments": "JIRA Reporter",
        "base": false,
        "name": "assignee",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The hostname to send logs to. This should target your JIRA server\nand has to have the REST interface enabled.",
        "base": false,
        "name": "host",
        "validate": "string"
      },
      {
        "comments": "JIRA Reporter",
        "base": false,
        "name": "reporter",
        "validate": "string"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-datadog/master/lib/logstash/outputs/datadog.rb",
    "name": "datadog",
    "type": "output",
    "params": [
      {
        "comments": "Your DatadogHQ API key",
        "base": false,
        "name": "api_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Alert type",
        "base": false,
        "name": "alert_type",
        "validate": [
          "info",
          "error",
          "warning",
          "success"
        ]
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Date Happened",
        "base": false,
        "name": "date_happened",
        "validate": "string"
      },
      {
        "comments": "Tags\nSet any custom tags for this event\nDefault are the Logstash tags if any",
        "base": false,
        "name": "dd_tags",
        "validate": "array"
      },
      {
        "comments": "Priority",
        "base": false,
        "name": "priority",
        "validate": [
          "normal",
          "low"
        ]
      },
      {
        "comments": "Source type name",
        "base": false,
        "name": "source_type_name",
        "validate": [
          "nagios",
          "hudson",
          "jenkins",
          "user",
          "my apps",
          "feed",
          "chef",
          "puppet",
          "git",
          "bitbucket",
          "fabric",
          "capistrano"
        ],
        "default": "my apps"
      },
      {
        "comments": "Text",
        "base": false,
        "name": "text",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "Title",
        "base": false,
        "name": "title",
        "validate": "string",
        "default": "Logstash event for %{host}"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-gemfire/master/lib/logstash/outputs/gemfire.rb",
    "name": "gemfire",
    "type": "output",
    "params": [
      {
        "comments": "Your client cache name",
        "base": false,
        "name": "cache_name",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The path to a GemFire client cache XML file.\n\nExample:\n[source,xml]\n     <client-cache>\n       <pool name=\"client-pool\">\n           <locator host=\"localhost\" port=\"31331\"/>\n       </pool>\n       <region name=\"Logstash\">\n           <region-attributes refid=\"CACHING_PROXY\" pool-name=\"client-pool\" >\n           </region-attributes>\n       </region>\n     </client-cache>\n",
        "base": false,
        "name": "cache_xml_file",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "A sprintf format to use when building keys",
        "base": false,
        "name": "key_format",
        "validate": "string",
        "default": "%{host}-%{@timestamp}"
      },
      {
        "comments": "The region name",
        "base": false,
        "name": "region_name",
        "validate": "string",
        "default": "Logstash"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-journald/master/lib/logstash/inputs/journald.rb",
    "name": "journald",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Filter on events. Not heavily tested.\n",
        "base": false,
        "name": "filter",
        "validate": "hash",
        "required": false,
        "default": "{}"
      },
      {
        "comments": "System journal flags\n0 = all avalable\n1 = local only\n2 = runtime only\n4 = system only\n",
        "base": false,
        "name": "flags",
        "validate": [
          0,
          1,
          2,
          4
        ],
        "default": "0"
      },
      {
        "comments": "Lowercase annoying UPPERCASE fieldnames. (May clobber existing fields)\n",
        "base": false,
        "name": "lowercase",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Path to read journal files from\n",
        "base": false,
        "name": "path",
        "validate": "string",
        "default": "/var/log/journal"
      },
      {
        "comments": "Where in the journal to start capturing logs\nOptions: head, tail",
        "base": false,
        "name": "seekto",
        "validate": [
          "head",
          "tail"
        ],
        "default": "tail"
      },
      {
        "comments": "Where to write the sincedb database (keeps track of the current\nposition of the journal). The default will write\nthe sincedb file to matching `$HOME/.sincedb_journal`\n",
        "base": false,
        "name": "sincedb_path",
        "validate": "string"
      },
      {
        "comments": "How often (in seconds) to write a since database with the current position of\nthe journal.\n",
        "base": false,
        "name": "sincedb_write_interval",
        "validate": "number",
        "default": "15"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Filter logs since the system booted (only relevant with seekto => \"head\")\n",
        "base": false,
        "name": "thisboot",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "The max timeout in microsends to wait for new events from the journal.\nSet to -1 to wait indefinitely. Setting this to a large value will\nresult in delayed shutdown of the plugin.",
        "base": false,
        "name": "wait_timeout",
        "validate": "number",
        "default": "3000000"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-mongodb/master/lib/logstash/outputs/mongodb.rb",
    "name": "mongodb",
    "type": "output",
    "params": [
      {
        "comments": "The collection to use. This value can use `%{foo}` values to dynamically\nselect a collection based on data in the event.",
        "base": false,
        "name": "collection",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The database to use.",
        "base": false,
        "name": "database",
        "validate": "string",
        "required": true
      },
      {
        "comments": "A MongoDB URI to connect to.\nSee http://docs.mongodb.org/manual/reference/connection-string/.",
        "base": false,
        "name": "uri",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "If true, an \"_id\" field will be added to the document before insertion.\nThe \"_id\" field will use the timestamp of the event and overwrite an existing\n\"_id\" field in the event.",
        "base": false,
        "name": "generateId",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If true, store the @timestamp field in MongoDB as an ISODate type instead\nof an ISO8601 string.  For more information about this, see\nhttp://www.mongodb.org/display/DOCS/Dates.",
        "base": false,
        "name": "isodate",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The number of seconds to wait after failure before retrying.",
        "base": false,
        "name": "retry_delay",
        "validate": "number",
        "default": "3",
        "required": false
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-websocket/master/lib/logstash/outputs/websocket.rb",
    "name": "websocket",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The address to serve websocket data from",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The port to serve websocket data from",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "3232"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-couchdb_changes/master/lib/logstash/inputs/couchdb_changes.rb",
    "name": "couchdb_changes",
    "type": "input",
    "params": [
      {
        "comments": "The CouchDB db to connect to.\nRequired parameter.",
        "base": false,
        "name": "db",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Reconnect flag.  When true, always try to reconnect after a failure",
        "base": false,
        "name": "always_reconnect",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Path to a CA certificate file, used to validate certificates",
        "base": false,
        "name": "ca_file",
        "validate": "path"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Logstash connects to CouchDB's _changes with feed=continuous\nThe heartbeat is how often (in milliseconds) Logstash will ping\nCouchDB to ensure the connection is maintained.  Changing this\nsetting is not recommended unless you know what you are doing.",
        "base": false,
        "name": "heartbeat",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "IP or hostname of your CouchDB instance",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "Future feature! Until implemented, changing this from the default\nwill not do anything.\n\nIgnore attachments associated with CouchDB documents.",
        "base": false,
        "name": "ignore_attachments",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "If unspecified, Logstash will attempt to read the last sequence number\nfrom the `sequence_path` file.  If that is empty or non-existent, it will\nbegin with 0 (the beginning).\n\nIf you specify this value, it is anticipated that you will\nonly be doing so for an initial read under special circumstances\nand that you will unset this value afterwards.",
        "base": false,
        "name": "initial_sequence",
        "validate": "number"
      },
      {
        "comments": "Preserve the CouchDB document id \"_id\" value in the\noutput.",
        "base": false,
        "name": "keep_id",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Preserve the CouchDB document revision \"_rev\" value in the\noutput.",
        "base": false,
        "name": "keep_revision",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Password, if authentication is needed to connect to\nCouchDB",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": "nil"
      },
      {
        "comments": "Port of your CouchDB instance.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "5984"
      },
      {
        "comments": "Reconnect delay: time between reconnect attempts, in seconds.",
        "base": false,
        "name": "reconnect_delay",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "Connect to CouchDB's _changes feed securely (via https)\nDefault: false (via http)",
        "base": false,
        "name": "secure",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "File path where the last sequence number in the _changes\nstream is stored. If unset it will write to `$HOME/.couchdb_seq`",
        "base": false,
        "name": "sequence_path",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Timeout: Number of milliseconds to wait for new data before\nterminating the connection.  If a timeout is set it will disable\nthe heartbeat configuration option.",
        "base": false,
        "name": "timeout",
        "validate": "number"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Username, if authentication is needed to connect to\nCouchDB",
        "base": false,
        "name": "username",
        "validate": "string",
        "default": "nil"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-circonus/master/lib/logstash/outputs/circonus.rb",
    "name": "circonus",
    "type": "output",
    "params": [
      {
        "comments": "Annotations\nRegisters an annotation with Circonus\nThe only required field is `title` and `description`.\n`start` and `stop` will be set to the event timestamp.\nYou can add any other optional annotation values as well.\nAll values will be passed through `event.sprintf`\n\nExample:\n[source,ruby]\n  [\"title\":\"Logstash event\", \"description\":\"Logstash event for %{host}\"]\nor\n[source,ruby]\n  [\"title\":\"Logstash event\", \"description\":\"Logstash event for %{host}\", \"parent_id\", \"1\"]",
        "base": false,
        "name": "annotation",
        "validate": "hash",
        "required": true,
        "default": "{}"
      },
      {
        "comments": "Your Circonus API Token",
        "base": false,
        "name": "api_token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your Circonus App name\nThis will be passed through `event.sprintf`\nso variables are allowed here:\n\nExample:\n `app_name => \"%{myappname}\"`",
        "base": false,
        "name": "app_name",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-riak/master/lib/logstash/outputs/riak.rb",
    "name": "riak",
    "type": "output",
    "params": [
      {
        "comments": "The bucket name to write events to\nExpansion is supported here as values are\npassed through event.sprintf\nMultiple buckets can be specified here\nbut any bucket-specific settings defined\napply to ALL the buckets.",
        "base": false,
        "name": "bucket",
        "validate": "array",
        "default": "[\"logstash-%{+YYYY.MM.dd}\"]"
      },
      {
        "comments": "Bucket properties (NYI)\nLogstash hash of properties for the bucket\ni.e.\n[source,ruby]\n    bucket_props => {\n        \"r\" => \"one\"\n        \"w\" => \"one\"\n        \"dw\", \"one\n     }\nor\n[source,ruby]\n    bucket_props => { \"n_val\" => \"3\" }\nProperties will be passed as-is",
        "base": false,
        "name": "bucket_props",
        "validate": "hash"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Search\nEnable search on the bucket defined above",
        "base": false,
        "name": "enable_search",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "SSL\nEnable SSL",
        "base": false,
        "name": "enable_ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Indices\nArray of fields to add 2i on\ne.g.\n[source,ruby]\n    `indices => [\"source_host\", \"type\"]\nOff by default as not everyone runs eleveldb",
        "base": false,
        "name": "indices",
        "validate": "array"
      },
      {
        "comments": "The event key name\nvariables are valid here.\n\nChoose this carefully. Best to let riak decide.",
        "base": false,
        "name": "key_name",
        "validate": "string"
      },
      {
        "comments": "The nodes of your Riak cluster\nThis can be a single host or\na Logstash hash of node/port pairs\ne.g\n[source,ruby]\n    {\n        \"node1\" => \"8098\"\n        \"node2\" => \"8098\"\n    }",
        "base": false,
        "name": "nodes",
        "validate": "hash",
        "default": "{\"localhost\" =>  \"8098\"}"
      },
      {
        "comments": "The protocol to use\nHTTP or ProtoBuf\nApplies to ALL backends listed above\nNo mix and match",
        "base": false,
        "name": "proto",
        "validate": [
          "http",
          "pb"
        ],
        "default": "http"
      },
      {
        "comments": "SSL Options\nOptions for SSL connections\nOnly applied if SSL is enabled\nLogstash hash that maps to the riak-client options\nhere: https://github.com/basho/riak-ruby-client/wiki/Connecting-to-Riak\nYou'll likely want something like this:\n\n[source, ruby]\n    ssl_opts => {\n       \"pem\" => \"/etc/riak.pem\"\n       \"ca_path\" => \"/usr/share/certificates\"\n    }\n\nPer the riak client docs, the above sample options\nwill turn on SSL `VERIFY_PEER`",
        "base": false,
        "name": "ssl_opts",
        "validate": "hash"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-boundary/master/lib/logstash/outputs/boundary.rb",
    "name": "boundary",
    "type": "output",
    "params": [
      {
        "comments": "Your Boundary API key",
        "base": false,
        "name": "api_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your Boundary Org ID",
        "base": false,
        "name": "org_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Auto\nIf set to true, logstash will try to pull boundary fields out\nof the event. Any field explicitly set by config options will\noverride these.\n`['type', 'subtype', 'creation_time', 'end_time', 'links', 'tags', 'loc']`",
        "base": false,
        "name": "auto",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Sub-Type",
        "base": false,
        "name": "bsubtype",
        "validate": "string"
      },
      {
        "comments": "Tags\nSet any custom tags for this event\nDefault are the Logstash tags if any",
        "base": false,
        "name": "btags",
        "validate": "array"
      },
      {
        "comments": "Type",
        "base": false,
        "name": "btype",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "End time\nOverride the stop time\nNote that Boundary requires this to be seconds since epoch\nIf overriding, it is your responsibility to type this correctly\nBy default this is set to `event.get(\"@timestamp\").to_i`",
        "base": false,
        "name": "end_time",
        "validate": "string"
      },
      {
        "comments": "Start time\nOverride the start time\nNote that Boundary requires this to be seconds since epoch\nIf overriding, it is your responsibility to type this correctly\nBy default this is set to `event.get(\"@timestamp\").to_i`",
        "base": false,
        "name": "start_time",
        "validate": "string"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-zabbix/master/lib/logstash/outputs/zabbix.rb",
    "name": "zabbix",
    "type": "output",
    "params": [
      {
        "comments": "The field name which holds the Zabbix host name. This can be a sub-field of\nthe @metadata field.",
        "base": false,
        "name": "zabbix_host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Use the `multi_value` directive to send multiple key/value pairs.\nThis can be thought of as an array, like:\n\n`[ zabbix_key1, zabbix_value1, zabbix_key2, zabbix_value2, ... zabbix_keyN, zabbix_valueN ]`\n\n...where `zabbix_key1` is an instance of `zabbix_key`, and `zabbix_value1`\nis an instance of `zabbix_value`.  If the field referenced by any\n`zabbix_key` or `zabbix_value` does not exist, that entry will be ignored.\n\nThis directive cannot be used in conjunction with the single-value directives\n`zabbix_key` and `zabbix_value`.",
        "base": false,
        "name": "multi_value",
        "validate": "array"
      },
      {
        "comments": "The number of seconds to wait before giving up on a connection to the Zabbix\nserver. This number should be very small, otherwise delays in delivery of\nother outputs could result.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      },
      {
        "comments": "A single field name which holds the value you intend to use as the Zabbix\nitem key. This can be a sub-field of the @metadata field.\nThis directive will be ignored if using `multi_value`\n\nIMPORTANT: `zabbix_key` is required if not using `multi_value`.\n",
        "base": false,
        "name": "zabbix_key",
        "validate": "string"
      },
      {
        "comments": "The IP or resolvable hostname where the Zabbix server is running",
        "base": false,
        "name": "zabbix_server_host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The port on which the Zabbix server is running",
        "base": false,
        "name": "zabbix_server_port",
        "validate": "number",
        "default": "10051"
      },
      {
        "comments": "The field name which holds the value you want to send.\nThis directive will be ignored if using `multi_value`",
        "base": false,
        "name": "zabbix_value",
        "validate": "string",
        "default": "message"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-redmine/master/lib/logstash/outputs/redmine.rb",
    "name": "redmine",
    "type": "output",
    "params": [
      {
        "comments": "redmine issue priority_id\nrequired",
        "base": false,
        "name": "priority_id",
        "validate": "number",
        "required": true
      },
      {
        "comments": "redmine issue projet_id\nrequired",
        "base": false,
        "name": "project_id",
        "validate": "number",
        "required": true
      },
      {
        "comments": "redmine issue status_id\nrequired",
        "base": false,
        "name": "status_id",
        "validate": "number",
        "required": true
      },
      {
        "comments": "redmine token user used for authentication\nrequired",
        "base": false,
        "name": "token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "redmine issue tracker_id\nrequired",
        "base": false,
        "name": "tracker_id",
        "validate": "number",
        "required": true
      },
      {
        "comments": "host of redmine app\nrequired\nvalue format : 'http://urlofredmine.tld' - Not add '/issues' at end",
        "base": false,
        "name": "url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "redmine issue assigned_to\nnot required for post_issue",
        "base": false,
        "name": "assigned_to_id",
        "validate": "number",
        "default": "nil"
      },
      {
        "comments": "redmine issue categorie_id\nnot required for post_issue",
        "base": false,
        "name": "categorie_id",
        "validate": "number",
        "default": "nil"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "redmine issue description\nrequired",
        "base": false,
        "name": "description",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "redmine issue fixed_version_id\nnot required for post_issue",
        "base": false,
        "name": "fixed_version_id",
        "validate": "number",
        "default": "nil"
      },
      {
        "comments": "redmine issue parent_issue_id\nnot required for post_issue",
        "base": false,
        "name": "parent_issue_id",
        "validate": "number",
        "default": "nil"
      },
      {
        "comments": "http request ssl trigger\nnot required",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "redmine issue subject\nrequired",
        "base": false,
        "name": "subject",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-metriccatcher/master/lib/logstash/outputs/metriccatcher.rb",
    "name": "metriccatcher",
    "type": "output",
    "params": [
      {
        "comments": "The metrics to send. This supports dynamic strings like `%{host}`\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value.\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "biased",
        "validate": "hash"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The metrics to send. This supports dynamic strings like `%{host}`\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value. Example:\n[source,ruby]\n  counter => { \"%{host}.apache.hits.%{response} => \"1\" }\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "counter",
        "validate": "hash"
      },
      {
        "comments": "The metrics to send. This supports dynamic strings like `%{host}`\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value.\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "gauge",
        "validate": "hash"
      },
      {
        "comments": "The address of the MetricCatcher",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The metrics to send. This supports dynamic strings like `%{host}`\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value.\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "meter",
        "validate": "hash"
      },
      {
        "comments": "The port to connect on your MetricCatcher",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "1420"
      },
      {
        "comments": "The metrics to send. This supports dynamic strings like %{host}\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value. Example:\n[source,ruby]\n  timer => { \"%{host}.apache.response_time => \"%{response_time}\" }\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "timer",
        "validate": "hash"
      },
      {
        "comments": "The metrics to send. This supports dynamic strings like `%{host}`\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value.\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "uniform",
        "validate": "hash"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-influxdb/master/lib/logstash/outputs/influxdb.rb",
    "name": "influxdb",
    "type": "output",
    "params": [
      {
        "comments": "Hash of key/value pairs representing data points to send to the named database\nExample: `{'column1' => 'value1', 'column2' => 'value2'}`\n\nEvents for the same measurement will be batched together where possible\nBoth keys and values support sprintf formatting",
        "base": false,
        "name": "data_points",
        "validate": "hash",
        "default": "{}",
        "required": true
      },
      {
        "comments": "The hostname or IP address to reach your InfluxDB instance",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Allow the override of the `time` column in the event?\n\nBy default any column with a name of `time` will be ignored and the time will\nbe determined by the value of `@timestamp`.\n\nSetting this to `true` allows you to explicitly set the `time` column yourself\n\nNote: **`time` must be an epoch value in either seconds, milliseconds or microseconds**",
        "base": false,
        "name": "allow_time_override",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Allow value coercion\n\nthis will attempt to convert data point values to the appropriate type before posting\notherwise sprintf-filtered numeric values could get sent as strings\nformat is `{'column_name' => 'datatype'}`\n\ncurrently supported datatypes are `integer` and `float`\n",
        "base": false,
        "name": "coerce_values",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The database to write - supports sprintf formatting",
        "base": false,
        "name": "db",
        "validate": "string",
        "default": "statistics"
      },
      {
        "comments": "An array containing the names of fields from the event to exclude from the\ndata points\n\nEvents, in general, contain keys \"@version\" and \"@timestamp\". Other plugins\nmay add others that you'll want to exclude (such as \"command\" from the\nexec plugin).\n\nThis only applies when use_event_fields_for_data_points is true.",
        "base": false,
        "name": "exclude_fields",
        "validate": "array",
        "default": "[\"@timestamp\",\"@version\",\"sequence\",\"message\",\"type\"]"
      },
      {
        "comments": "This setting controls how many events will be buffered before sending a batch\nof events. Note that these are only batched for the same measurement",
        "base": false,
        "name": "flush_size",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "The amount of time since last flush before a flush is forced.\n\nThis setting helps ensure slow event rates don't get stuck in Logstash.\nFor example, if your `flush_size` is 100, and you have received 10 events,\nand it has been more than `idle_flush_time` seconds since the last flush,\nlogstash will flush those 10 events automatically.\n\nThis helps keep both fast and slow log streams moving along in\nnear-real-time.",
        "base": false,
        "name": "idle_flush_time",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Measurement name - supports sprintf formatting",
        "base": false,
        "name": "measurement",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The password for the user who access to the named database",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": "nil"
      },
      {
        "comments": "The port for InfluxDB",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8086"
      },
      {
        "comments": "The retention policy to use",
        "base": false,
        "name": "retention_policy",
        "validate": "string",
        "default": "default"
      },
      {
        "comments": "An array containing the names of fields to send to Influxdb as tags instead\nof fields. Influxdb 0.9 convention is that values that do not change every\nrequest should be considered metadata and given as tags.",
        "base": false,
        "name": "send_as_tags",
        "validate": "array",
        "default": "[\"host\"]"
      },
      {
        "comments": "Enable SSL/TLS secured communication to InfluxDB",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Set the level of precision of `time`\n\nonly useful when overriding the time value",
        "base": false,
        "name": "time_precision",
        "validate": [
          "n",
          "u",
          "ms",
          "s",
          "m",
          "h"
        ],
        "default": "ms"
      },
      {
        "comments": "Automatically use fields from the event as the data points sent to Influxdb",
        "base": false,
        "name": "use_event_fields_for_data_points",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The user who has access to the named database",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-riemann/master/lib/logstash/outputs/riemann.rb",
    "name": "riemann",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "\nEnable debugging output?",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The address of the Riemann server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "If set to true automatically map all logstash defined fields to riemann event fields.\nAll nested logstash fields will be mapped to riemann fields containing all parent keys\nseparated by dots and the deepest value.\n\nAs an example, the logstash event:\n[source,ruby]\n   {\n     \"@timestamp\":\"2013-12-10T14:36:26.151+0000\",\n     \"@version\": 1,\n     \"message\":\"log message\",\n     \"host\": \"host.domain.com\",\n     \"nested_field\": {\n                       \"key\": \"value\"\n                     }\n   }\nIs mapped to this riemann event:\n[source,ruby]\n  {\n    :time 1386686186,\n    :host host.domain.com,\n    :message log message,\n    :nested_field.key value\n  }\n\nIt can be used in conjunction with or independent of the riemann_event option.\nWhen used with the riemann_event any duplicate keys receive their value from\nriemann_event instead of the logstash event itself.",
        "base": false,
        "name": "map_fields",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The port to connect to on your Riemann server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "5555"
      },
      {
        "comments": "The protocol to use\nUDP is non-blocking\nTCP is blocking\n\nLogstash's default output behaviour\nis to never lose events\nAs such, we use tcp as default here",
        "base": false,
        "name": "protocol",
        "validate": [
          "tcp",
          "udp"
        ],
        "default": "tcp"
      },
      {
        "comments": "A Hash to set Riemann event fields\n(http://riemann.io/concepts.html).\n\nThe following event fields are supported:\n`description`, `state`, `metric`, `ttl`, `service`\n\nTags found on the Logstash event will automatically be added to the\nRiemann event.\n\nAny other field set here will be passed to Riemann as an event attribute.\n\nExample:\n[source,ruby]\n    riemann {\n        riemann_event => {\n            \"metric\"  => \"%{metric}\"\n            \"service\" => \"%{service}\"\n        }\n    }\n\n`metric` and `ttl` values will be coerced to a floating point value.\nValues which cannot be coerced will zero (0.0).\n\n`description`, by default, will be set to the event message\nbut can be overridden here.",
        "base": false,
        "name": "riemann_event",
        "validate": "hash"
      },
      {
        "comments": "The name of the sender.\nThis sets the `host` value\nin the Riemann event",
        "base": false,
        "name": "sender",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-yaml/master/lib/logstash/filters/yaml.rb",
    "name": "yaml",
    "type": "filter",
    "params": [
      {
        "comments": "The configuration for the YAML filter:\n[source,ruby]\n    source => source_field\n\nFor example, if you have YAML data in the @message field:\n[source,ruby]\n    filter {\n      yaml {\n        source => \"message\"\n      }\n    }\n\nThe above would parse the yaml from the @message field",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Define the target field for placing the parsed data. If this setting is\nomitted, the YAML data will be stored at the root (top level) of the event.\n\nFor example, if you want the data to be put in the `doc` field:\n[source,ruby]\n    filter {\n      yaml {\n        target => \"doc\"\n      }\n    }\n\nYAML in the value of the `source` field will be expanded into a\ndata structure in the `target` field.\n\nNOTE: if the `target` field already exists, it will be overwritten!",
        "base": false,
        "name": "target",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-solr_http/master/lib/logstash/outputs/solr_http.rb",
    "name": "solr_http",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Solr document ID for events. You'd typically have a variable here, like\n'%{foo}' so you can assign your own IDs",
        "base": false,
        "name": "document_id",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Number of events to queue up before writing to Solr",
        "base": false,
        "name": "flush_size",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "Amount of time since the last flush before a flush is done even if\nthe number of buffered events is smaller than flush_size",
        "base": false,
        "name": "idle_flush_time",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "URL used to connect to Solr",
        "base": false,
        "name": "solr_url",
        "validate": "string",
        "default": "http//localhost8983/solr"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-datadog_metrics/master/lib/logstash/outputs/datadog_metrics.rb",
    "name": "datadog_metrics",
    "type": "output",
    "params": [
      {
        "comments": "Your DatadogHQ API key. https://app.datadoghq.com/account/settings#api",
        "base": false,
        "name": "api_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Set any custom tags for this event,\ndefault are the Logstash tags if any.",
        "base": false,
        "name": "dd_tags",
        "validate": "array"
      },
      {
        "comments": "The name of the device that produced the metric.",
        "base": false,
        "name": "device",
        "validate": "string",
        "default": "%{metric_device}"
      },
      {
        "comments": "The name of the host that produced the metric.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "The name of the time series.",
        "base": false,
        "name": "metric_name",
        "validate": "string",
        "default": "%{metric_name}"
      },
      {
        "comments": "The type of the metric.",
        "base": false,
        "name": "metric_type",
        "validate": [
          "gauge",
          "counter",
          "%{metric_type}"
        ],
        "default": "%{metric_type}"
      },
      {
        "comments": "The value.",
        "base": false,
        "name": "metric_value",
        "default": "%{metric_value}"
      },
      {
        "comments": "How many events to queue before flushing to Datadog\nprior to schedule set in `@timeframe`",
        "base": false,
        "name": "queue_size",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "How often (in seconds) to flush queued events to Datadog",
        "base": false,
        "name": "timeframe",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-kv/master/lib/logstash/filters/kv.rb",
    "name": "kv",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "A bool option for removing duplicate key/value pairs. When set to false, only\none unique key/value pair will be preserved.\n\nFor example, consider a source like `from=me from=me`. `[from]` will map to\nan Array with two elements: `[\"me\", \"me\"]`. To only keep unique key/value pairs,\nyou could use this configuration:\n[source,ruby]\n    filter {\n      kv {\n        allow_duplicate_values => false\n      }\n    }",
        "base": false,
        "name": "allow_duplicate_values",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "A hash specifying the default keys and their values which should be added to the event\nin case these keys do not exist in the source field being parsed.\n[source,ruby]\n    filter {\n      kv {\n        default_keys => [ \"from\", \"logstash@example.com\",\n                         \"to\", \"default@dev.null\" ]\n      }\n    }",
        "base": false,
        "name": "default_keys",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "An array specifying the parsed keys which should not be added to the event.\nBy default no keys will be excluded.\n\nFor example, consider a source like `Hey, from=<abc>, to=def foo=bar`.\nTo exclude `from` and `to`, but retain the `foo` key, you could use this configuration:\n[source,ruby]\n    filter {\n      kv {\n        exclude_keys => [ \"from\", \"to\" ]\n      }\n    }",
        "base": false,
        "name": "exclude_keys",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "A string of characters to use as delimiters for parsing out key-value pairs.\n\nThese characters form a regex character class and thus you must escape special regex\ncharacters like `[` or `]` using `\\`.\n\n#### Example with URL Query Strings\n\nFor example, to split out the args from a url query string such as\n`?pin=12345~0&d=123&e=foo@bar.com&oq=bobo&ss=12345`:\n[source,ruby]\n    filter {\n      kv {\n        field_split => \"&?\"\n      }\n    }\n\nThe above splits on both `&` and `?` characters, giving you the following\nfields:\n\n* `pin: 12345~0`\n* `d: 123`\n* `e: foo@bar.com`\n* `oq: bobo`\n* `ss: 12345`",
        "base": false,
        "name": "field_split",
        "validate": "string",
        "default": " "
      },
      {
        "comments": "A boolean specifying whether to treat square brackets, angle brackets,\nand parentheses as value \"wrappers\" that should be removed from the value.\n[source,ruby]\n    filter {\n      kv {\n        include_brackets => true\n      }\n    }\n\nFor example, the result of this line:\n`bracketsone=(hello world) bracketstwo=[hello world] bracketsthree=<hello world>`\n\nwill be:\n\n* bracketsone: hello world\n* bracketstwo: hello world\n* bracketsthree: hello world\n\ninstead of:\n\n* bracketsone: (hello\n* bracketstwo: [hello\n* bracketsthree: <hello\n",
        "base": false,
        "name": "include_brackets",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "An array specifying the parsed keys which should be added to the event.\nBy default all keys will be added.\n\nFor example, consider a source like `Hey, from=<abc>, to=def foo=bar`.\nTo include `from` and `to`, but exclude the `foo` key, you could use this configuration:\n[source,ruby]\n    filter {\n      kv {\n        include_keys => [ \"from\", \"to\" ]\n      }\n    }",
        "base": false,
        "name": "include_keys",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "A string to prepend to all of the extracted keys.\n\nFor example, to prepend arg_ to all keys:\n[source,ruby]\n    filter { kv { prefix => \"arg_\" } }",
        "base": false,
        "name": "prefix",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "A boolean specifying whether to drill down into values\nand recursively get more key-value pairs from it.\nThe extra key-value pairs will be stored as subkeys of the root key.\n\nDefault is not to recursive values.\n[source,ruby]\n    filter {\n      kv {\n        recursive => \"true\"\n      }\n    }\n",
        "base": false,
        "name": "recursive",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "A string of characters to remove from the key.\n\nThese characters form a regex character class and thus you must escape special regex\ncharacters like `[` or `]` using `\\`.\n\nContrary to trim option, all characters are removed from the key, whatever their position.\n\nFor example, to remove `<` `>` `[` `]` and `,` characters from keys:\n[source,ruby]\n    filter {\n      kv {\n        remove_char_key => \"<>\\[\\],\"\n      }\n    }",
        "base": false,
        "name": "remove_char_key",
        "validate": "string"
      },
      {
        "comments": "A string of characters to remove from the value.\n\nThese characters form a regex character class and thus you must escape special regex\ncharacters like `[` or `]` using `\\`.\n\nContrary to trim option, all characters are removed from the value, whatever their position.\n\nFor example, to remove `<`, `>`, `[`, `]` and `,` characters from values:\n[source,ruby]\n    filter {\n      kv {\n        remove_char_value => \"<>\\[\\],\"\n      }\n    }",
        "base": false,
        "name": "remove_char_value",
        "validate": "string"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field to perform `key=value` searching on\n\nFor example, to process the `not_the_message` field:\n[source,ruby]\n    filter { kv { source => \"not_the_message\" } }",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The name of the container to put all of the key-value pairs into.\n\nIf this setting is omitted, fields will be written to the root of the\nevent, as individual fields.\n\nFor example, to place all keys into the event field kv:\n[source,ruby]\n    filter { kv { target => \"kv\" } }",
        "base": false,
        "name": "target",
        "validate": "string"
      },
      {
        "comments": "Transform keys to lower case, upper case or capitals.\n\nFor example, to lowercase all keys:\n[source,ruby]\n    filter {\n      kv {\n        transform_key => \"lowercase\"\n      }\n    }",
        "base": false,
        "name": "transform_key",
        "validate": "[TRANSFORM_LOWERCASE_KEY, TRANSFORM_UPPERCASE_KEY, TRANSFORM_CAPITALIZE_KEY]"
      },
      {
        "comments": "Transform values to lower case, upper case or capitals.\n\nFor example, to capitalize all values:\n[source,ruby]\n    filter {\n      kv {\n        transform_value => \"capitalize\"\n      }\n    }",
        "base": false,
        "name": "transform_value",
        "validate": "[TRANSFORM_LOWERCASE_KEY, TRANSFORM_UPPERCASE_KEY, TRANSFORM_CAPITALIZE_KEY]"
      },
      {
        "comments": "A string of characters to trim from the key. This is useful if your\nkeys are wrapped in brackets or start with space.\n\nThese characters form a regex character class and thus you must escape special regex\ncharacters like `[` or `]` using `\\`.\n\nOnly leading and trailing characters are trimed from the key.\n\nFor example, to trim `<` `>` `[` `]` and `,` characters from keys:\n[source,ruby]\n    filter {\n      kv {\n        trim_key => \"<>\\[\\],\"\n      }\n    }",
        "base": false,
        "name": "trim_key",
        "validate": "string"
      },
      {
        "comments": "A string of characters to trim from the value. This is useful if your\nvalues are wrapped in brackets or are terminated with commas (like postfix\nlogs).\n\nThese characters form a regex character class and thus you must escape special regex\ncharacters like `[` or `]` using `\\`.\n\nOnly leading and trailing characters are trimed from the value.\n\nFor example, to trim `<`, `>`, `[`, `]` and `,` characters from values:\n[source,ruby]\n    filter {\n      kv {\n        trim_value => \"<>\\[\\],\"\n      }\n    }",
        "base": false,
        "name": "trim_value",
        "validate": "string"
      },
      {
        "comments": "A non-empty string of characters to use as delimiters for identifying key-value relations.\n\nThese characters form a regex character class and thus you must escape special regex\ncharacters like `[` or `]` using `\\`.\n\nFor example, to identify key-values such as\n`key1:value1 key2:value2`:\n[source,ruby]\n    filter { kv { value_split => \":\" } }",
        "base": false,
        "name": "value_split",
        "validate": "string",
        "default": "="
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-google_bigquery/master/lib/logstash/outputs/google_bigquery.rb",
    "name": "google_bigquery",
    "type": "output",
    "params": [
      {
        "comments": "BigQuery dataset to which these events will be added to.",
        "base": false,
        "name": "dataset",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Path to private key file for Google Service Account.",
        "base": false,
        "name": "key_path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Google Cloud Project ID (number, not Project Name!).",
        "base": false,
        "name": "project_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Service account to access Google APIs.",
        "base": false,
        "name": "service_account",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Schema for log data. It must follow this format:\n<field1-name>:<field1-type>,<field2-name>:<field2-type>,...\nExample: path:STRING,status:INTEGER,score:FLOAT",
        "base": false,
        "name": "csv_schema",
        "validate": "string",
        "required": false,
        "default": "nil"
      },
      {
        "comments": "Time pattern for BigQuery table, defaults to hourly tables.\nMust Time.strftime patterns: www.ruby-doc.org/core-2.0/Time.html#method-i-strftime",
        "base": false,
        "name": "date_pattern",
        "validate": "string",
        "default": "%Y-%m-%dT%H00"
      },
      {
        "comments": "Deleter interval when checking if upload jobs are done for file deletion.\nThis only affects how long files are on the hard disk after the job is done.",
        "base": false,
        "name": "deleter_interval_secs",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "Flush interval in seconds for flushing writes to log files. 0 will flush\non every message.",
        "base": false,
        "name": "flush_interval_secs",
        "validate": "number",
        "default": "2"
      },
      {
        "comments": "Indicates if BigQuery should allow extra values that are not represented in the table schema.\nIf true, the extra values are ignored. If false, records with extra columns are treated as bad records, and if there are too many bad records, an invalid error is returned in the job result. The default value is false.",
        "base": false,
        "name": "ignore_unknown_values",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Schema for log data, as a hash. Example:\njson_schema => {\n    fields => [{\n        name => \"timestamp\"\n        type => \"TIMESTAMP\"\n    }, {\n        name => \"host\"\n        type => \"STRING\"\n    }, {\n        name => \"message\"\n        type => \"STRING\"\n    }]\n}",
        "base": false,
        "name": "json_schema",
        "validate": "hash",
        "required": false,
        "default": "nil"
      },
      {
        "comments": "Private key password for service account private key.",
        "base": false,
        "name": "key_password",
        "validate": "string",
        "default": "notasecret"
      },
      {
        "comments": "BigQuery table ID prefix to be used when creating new tables for log data.\nTable name will be <table_prefix><table_separator><date>",
        "base": false,
        "name": "table_prefix",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "BigQuery table separator to be added between the table_prefix and the\ndate suffix.",
        "base": false,
        "name": "table_separator",
        "validate": "string",
        "default": "_"
      },
      {
        "comments": "Directory where temporary files are stored.\nDefaults to /tmp/logstash-bq-<random-suffix>",
        "base": false,
        "name": "temp_directory",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Temporary local file prefix. Log file will follow the format:\n<prefix>_hostname_date.part?.log",
        "base": false,
        "name": "temp_file_prefix",
        "validate": "string",
        "default": "logstash_bq"
      },
      {
        "comments": "Uploader interval when uploading new files to BigQuery. Adjust time based\non your time pattern (for example, for hourly files, this interval can be\naround one hour).",
        "base": false,
        "name": "uploader_interval_secs",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-kafka/master/lib/logstash/inputs/kafka.rb",
    "name": "kafka",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The frequency in milliseconds that the consumer offsets are committed to Kafka.",
        "base": false,
        "name": "auto_commit_interval_ms",
        "validate": "string",
        "default": "5000"
      },
      {
        "comments": "What to do when there is no initial offset in Kafka or if an offset is out of range:\n\n* earliest: automatically reset the offset to the earliest offset\n* latest: automatically reset the offset to the latest offset\n* none: throw exception to the consumer if no previous offset is found for the consumer's group\n* anything else: throw exception to the consumer.",
        "base": false,
        "name": "auto_offset_reset",
        "validate": "string"
      },
      {
        "comments": "A list of URLs to use for establishing the initial connection to the cluster.\nThis list should be in the form of `host1:port1,host2:port2` These urls are just used\nfor the initial connection to discover the full cluster membership (which may change dynamically)\nso this list need not contain the full set of servers (you may want more than one, though, in\ncase a server is down).",
        "base": false,
        "name": "bootstrap_servers",
        "validate": "string",
        "default": "localhost9092"
      },
      {
        "comments": "Automatically check the CRC32 of the records consumed. This ensures no on-the-wire or on-disk\ncorruption to the messages occurred. This check adds some overhead, so it may be\ndisabled in cases seeking extreme performance.",
        "base": false,
        "name": "check_crcs",
        "validate": "string"
      },
      {
        "comments": "The id string to pass to the server when making requests. The purpose of this\nis to be able to track the source of requests beyond just ip/port by allowing\na logical application name to be included.",
        "base": false,
        "name": "client_id",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Close idle connections after the number of milliseconds specified by this config.",
        "base": false,
        "name": "connections_max_idle_ms",
        "validate": "string"
      },
      {
        "comments": "Ideally you should have as many threads as the number of partitions for a perfect\nbalancemore threads than partitions means that some threads will be idle",
        "base": false,
        "name": "consumer_threads",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Option to add Kafka metadata like topic, message size to the event.\nThis will add a field named `kafka` to the logstash event containing the following attributes:\n  `topic`: The topic this message is associated with\n  `consumer_group`: The consumer group used to read in this event\n  `partition`: The partition this message is associated with\n  `offset`: The offset from the partition this message is associated with\n  `key`: A ByteBuffer containing the message key",
        "base": false,
        "name": "decorate_events",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If true, periodically commit to Kafka the offsets of messages already returned by the consumer.\nThis committed offset will be used when the process fails as the position from\nwhich the consumption will begin.",
        "base": false,
        "name": "enable_auto_commit",
        "validate": "string",
        "default": "true"
      },
      {
        "comments": "Whether records from internal topics (such as offsets) should be exposed to the consumer.\nIf set to true the only way to receive records from an internal topic is subscribing to it.",
        "base": false,
        "name": "exclude_internal_topics",
        "validate": "string"
      },
      {
        "comments": "The maximum amount of data the server should return for a fetch request. This is not an\nabsolute maximum, if the first message in the first non-empty partition of the fetch is larger\nthan this value, the message will still be returned to ensure that the consumer can make progress.",
        "base": false,
        "name": "fetch_max_bytes",
        "validate": "string"
      },
      {
        "comments": "The maximum amount of time the server will block before answering the fetch request if\nthere isn't sufficient data to immediately satisfy `fetch_min_bytes`. This\nshould be less than or equal to the timeout used in `poll_timeout_ms`",
        "base": false,
        "name": "fetch_max_wait_ms",
        "validate": "string"
      },
      {
        "comments": "The minimum amount of data the server should return for a fetch request. If insufficient\ndata is available the request will wait for that much data to accumulate\nbefore answering the request.",
        "base": false,
        "name": "fetch_min_bytes",
        "validate": "string"
      },
      {
        "comments": "The identifier of the group this consumer belongs to. Consumer group is a single logical subscriber\nthat happens to be made up of multiple processors. Messages in a topic will be distributed to all\nLogstash instances with the same `group_id`",
        "base": false,
        "name": "group_id",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The expected time between heartbeats to the consumer coordinator. Heartbeats are used to ensure\nthat the consumer's session stays active and to facilitate rebalancing when new\nconsumers join or leave the group. The value must be set lower than\n`session.timeout.ms`, but typically should be set no higher than 1/3 of that value.\nIt can be adjusted even lower to control the expected time for normal rebalances.",
        "base": false,
        "name": "heartbeat_interval_ms",
        "validate": "string"
      },
      {
        "comments": "The Java Authentication and Authorization Service (JAAS) API supplies user authentication and authorization\nservices for Kafka. This setting provides the path to the JAAS file. Sample JAAS file for Kafka client:\n[source,java]\n----------------------------------\nKafkaClient {\n  com.sun.security.auth.module.Krb5LoginModule required\n  useTicketCache=true\n  renewTicket=true\n  serviceName=\"kafka\";\n  };\n----------------------------------\n\nPlease note that specifying `jaas_path` and `kerberos_config` in the config file will add these\nto the global JVM system properties. This means if you have multiple Kafka inputs, all of them would be sharing the same\n`jaas_path` and `kerberos_config`. If this is not desirable, you would have to run separate instances of Logstash on\ndifferent JVM instances.",
        "base": false,
        "name": "jaas_path",
        "validate": "path"
      },
      {
        "comments": "Optional path to kerberos config file. This is krb5.conf style as detailed in https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html",
        "base": false,
        "name": "kerberos_config",
        "validate": "path"
      },
      {
        "comments": "Java Class used to deserialize the record's key",
        "base": false,
        "name": "key_deserializer_class",
        "validate": "string",
        "default": "org.apache.kafka.common.serialization.StringDeserializer"
      },
      {
        "comments": "The maximum amount of data per-partition the server will return. The maximum total memory used for a\nrequest will be <code>#partitions * max.partition.fetch.bytes</code>. This size must be at least\nas large as the maximum message size the server allows or else it is possible for the producer to\nsend messages larger than the consumer can fetch. If that happens, the consumer can get stuck trying\nto fetch a large message on a certain partition.",
        "base": false,
        "name": "max_partition_fetch_bytes",
        "validate": "string"
      },
      {
        "comments": "The maximum delay between invocations of poll() when using consumer group management. This places\nan upper bound on the amount of time that the consumer can be idle before fetching more records.\nIf poll() is not called before expiration of this timeout, then the consumer is considered failed and\nthe group will rebalance in order to reassign the partitions to another member.\nThe value of the configuration `request_timeout_ms` must always be larger than max_poll_interval_ms",
        "base": false,
        "name": "max_poll_interval_ms",
        "validate": "string"
      },
      {
        "comments": "The maximum number of records returned in a single call to poll().",
        "base": false,
        "name": "max_poll_records",
        "validate": "string"
      },
      {
        "comments": "The period of time in milliseconds after which we force a refresh of metadata even if\nwe haven't seen any partition leadership changes to proactively discover any new brokers or partitions",
        "base": false,
        "name": "metadata_max_age_ms",
        "validate": "string"
      },
      {
        "comments": "The class name of the partition assignment strategy that the client will use to distribute\npartition ownership amongst consumer instances",
        "base": false,
        "name": "partition_assignment_strategy",
        "validate": "string"
      },
      {
        "comments": "Time kafka consumer will wait to receive new messages from topics",
        "base": false,
        "name": "poll_timeout_ms",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "The size of the TCP receive buffer (SO_RCVBUF) to use when reading data.",
        "base": false,
        "name": "receive_buffer_bytes",
        "validate": "string"
      },
      {
        "comments": "The amount of time to wait before attempting to reconnect to a given host.\nThis avoids repeatedly connecting to a host in a tight loop.\nThis backoff applies to all requests sent by the consumer to the broker.",
        "base": false,
        "name": "reconnect_backoff_ms",
        "validate": "string"
      },
      {
        "comments": "The configuration controls the maximum amount of time the client will wait\nfor the response of a request. If the response is not received before the timeout\nelapses the client will resend the request if necessary or fail the request if\nretries are exhausted.",
        "base": false,
        "name": "request_timeout_ms",
        "validate": "string"
      },
      {
        "comments": "The amount of time to wait before attempting to retry a failed fetch request\nto a given topic partition. This avoids repeated fetching-and-failing in a tight loop.",
        "base": false,
        "name": "retry_backoff_ms",
        "validate": "string"
      },
      {
        "comments": "The Kerberos principal name that Kafka broker runs as.\nThis can be defined either in Kafka's JAAS config or in Kafka's config.",
        "base": false,
        "name": "sasl_kerberos_service_name",
        "validate": "string"
      },
      {
        "comments": "http://kafka.apache.org/documentation.html#security_sasl[SASL mechanism] used for client connections.\nThis may be any mechanism for which a security provider is available.\nGSSAPI is the default mechanism.",
        "base": false,
        "name": "sasl_mechanism",
        "validate": "string",
        "default": "GSSAPI"
      },
      {
        "comments": "Security protocol to use, which can be either of PLAINTEXT,SSL,SASL_PLAINTEXT,SASL_SSL",
        "base": false,
        "name": "security_protocol",
        "validate": [
          "PLAINTEXT",
          "SSL",
          "SASL_PLAINTEXT",
          "SASL_SSL"
        ],
        "default": "PLAINTEXT"
      },
      {
        "comments": "The size of the TCP send buffer (SO_SNDBUF) to use when sending data",
        "base": false,
        "name": "send_buffer_bytes",
        "validate": "string"
      },
      {
        "comments": "The timeout after which, if the `poll_timeout_ms` is not invoked, the consumer is marked dead\nand a rebalance operation is triggered for the group identified by `group_id`",
        "base": false,
        "name": "session_timeout_ms",
        "validate": "string"
      },
      {
        "comments": "Enable SSL/TLS secured communication to Kafka broker.",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": false,
        "deprecated": "\"Use security_protocol => \"ssl\"\""
      },
      {
        "comments": "The password of the private key in the key store file.",
        "base": false,
        "name": "ssl_key_password",
        "validate": "password"
      },
      {
        "comments": "If client authentication is required, this setting stores the keystore path.",
        "base": false,
        "name": "ssl_keystore_location",
        "validate": "path"
      },
      {
        "comments": "If client authentication is required, this setting stores the keystore password",
        "base": false,
        "name": "ssl_keystore_password",
        "validate": "password"
      },
      {
        "comments": "The keystore type.",
        "base": false,
        "name": "ssl_keystore_type",
        "validate": "string"
      },
      {
        "comments": "The JKS truststore path to validate the Kafka broker's certificate.",
        "base": false,
        "name": "ssl_truststore_location",
        "validate": "path"
      },
      {
        "comments": "The truststore password",
        "base": false,
        "name": "ssl_truststore_password",
        "validate": "password"
      },
      {
        "comments": "The truststore type.",
        "base": false,
        "name": "ssl_truststore_type",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "A list of topics to subscribe to, defaults to [\"logstash\"].",
        "base": false,
        "name": "topics",
        "validate": "array",
        "default": "[\"logstash\"]"
      },
      {
        "comments": "A topic regex pattern to subscribe to.\nThe topics configuration will be ignored when using this configuration.",
        "base": false,
        "name": "topics_pattern",
        "validate": "string"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Java Class used to deserialize the record's value",
        "base": false,
        "name": "value_deserializer_class",
        "validate": "string",
        "default": "org.apache.kafka.common.serialization.StringDeserializer"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-syslog/master/lib/logstash/outputs/syslog.rb",
    "name": "syslog",
    "type": "output",
    "params": [
      {
        "comments": "syslog server address to connect to",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "syslog server port to connect to",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "application name for syslog message. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "appname",
        "validate": "string",
        "default": "LOGSTASH"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "facility label for syslog message\ndefault fallback to user-level as in rfc3164\nThe new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "facility",
        "validate": "string",
        "default": "user-level"
      },
      {
        "comments": "message text to log. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "message",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "message id for syslog message. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "msgid",
        "validate": "string",
        "default": "-"
      },
      {
        "comments": "syslog priority\nThe new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "priority",
        "validate": "string",
        "default": "%{syslog_pri}"
      },
      {
        "comments": "process id for syslog message. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "procid",
        "validate": "string",
        "default": "-"
      },
      {
        "comments": "syslog server protocol. you can choose between udp, tcp and ssl/tls over tcp",
        "base": false,
        "name": "protocol",
        "validate": [
          "tcp",
          "udp",
          "ssl-tcp"
        ],
        "default": "udp"
      },
      {
        "comments": "when connection fails, retry interval in sec.",
        "base": false,
        "name": "reconnect_interval",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "syslog message format: you can choose between rfc3164 or rfc5424",
        "base": false,
        "name": "rfc",
        "validate": [
          "rfc3164",
          "rfc5424"
        ],
        "default": "rfc3164"
      },
      {
        "comments": "severity label for syslog message\ndefault fallback to notice as in rfc3164\nThe new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "severity",
        "validate": "string",
        "default": "notice"
      },
      {
        "comments": "source host for syslog message. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "sourcehost",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "The SSL CA certificate, chainfile or CA path. The system CA path is automatically included.",
        "base": false,
        "name": "ssl_cacert",
        "validate": "path"
      },
      {
        "comments": "SSL certificate path",
        "base": false,
        "name": "ssl_cert",
        "validate": "path"
      },
      {
        "comments": "SSL key path",
        "base": false,
        "name": "ssl_key",
        "validate": "path"
      },
      {
        "comments": "SSL key passphrase",
        "base": false,
        "name": "ssl_key_passphrase",
        "validate": "password",
        "default": "nil"
      },
      {
        "comments": "Verify the identity of the other end of the SSL connection against the CA.",
        "base": false,
        "name": "ssl_verify",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "use label parsing for severity and facility levels\nuse priority field if set to false",
        "base": false,
        "name": "use_labels",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "TODO remove this in Logstash 6.0\nwhen we no longer support the :legacy type\nThis is hacky, but it can only be herne",
        "base": true,
        "name": "workers",
        "type": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-metaevent/master/lib/logstash/filters/metaevent.rb",
    "name": "metaevent",
    "type": "filter",
    "params": [
      {
        "comments": "syntax: `followed_by_tags => [ \"tag\", \"tag\" ]`",
        "base": false,
        "name": "followed_by_tags",
        "validate": "array",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "syntax: `period => 60`",
        "base": false,
        "name": "period",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  }
]